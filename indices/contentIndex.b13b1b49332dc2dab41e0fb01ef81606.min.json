{"/":{"title":"🎁首页","content":"[[obsidian]] + [[Quartz]] Publish Web HomePage\n\nHello👋，本页面提供了一个网络访问我的笔记的途径，使用 [[Hugo]] 搭建（Quartz），笔记使用 Obsidian 编写。\n\n## 双链的阅读建议\n- 通过鼠标悬浮预览进行上下文不中断的阅读。\n- 通过底部图和双向链接找到更多感兴趣的。\n- 想查找直接使用搜索按钮\n\n也可以查看这个导航：[标签](/tags/)\n\n\n目前的写作工作流：\n### 写\n在任意地方都可以写，打开一个 [[obsidian]] 目标文件夹即可。\n\n### 发布到网页 notes.abser.top\n1. 通过 abserari/quartz 这个 repo，直接使用 obsidian-git 插件，push 到 quartz 库中。\n2. 最后通过 git 提交到 github 触发 action 自动构建\n3. 构建使用 [[hugo-extended]]  [[hugo-obsidian]] 工具\n\n### 同步功能\n然后使用 [remotely save](https://github.com/remotely-save/remotely-save) 插件同步，\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/%E4%B8%8D%E8%A6%81%E7%89%87%E9%9D%A2%E7%9A%84%E6%A0%B9%E6%8D%AE%E4%B8%80%E9%83%A8%E5%88%86%E6%83%B3%E6%B3%95%E4%B8%8B%E5%86%B3%E5%AE%9A":{"title":"不要片面的根据一部分想法下决定","content":"这个事例是在日常的生活中，会想到自己如果做什么事就好了。以写日记和记录照片为例，在自己翻看的时候就会想看到自己的记录，但实际上为了这个目的养成一个记录日记，照片，整理数据库的习惯是性价比不高的。以单满足一个回忆的需求来说，在年轻的时候是价值不高的，所以不推荐做。\r\n那么一个简短总结就是标题。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E4%B8%A4%E9%98%B6%E6%AE%B5%E9%94%81":{"title":"两阶段锁","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E4%B8%A5%E6%A0%BC%E7%9A%84%E9%A1%BA%E5%BA%8F":{"title":"严格的顺序","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E5%9C%A8RocksDB%E4%B8%8A%E5%88%9B%E5%BB%BACockroachDB%E9%A1%B9%E7%9B%AE":{"title":"为什么我们在RocksDB上创建CockroachDB项目？","content":"https://www.cockroachlabs.com/blog/cockroachdb-on-rocksd/","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E4%BA%91%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F":{"title":"","content":"\r\n### Notes\r\n- 和 [[分布式]] 有关\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E4%BD%BF%E7%94%A8-quartz-%E6%89%98%E7%AE%A1-obsidian-%E5%88%B0%E7%BD%91%E7%BB%9C%E4%B8%8A":{"title":"使用 quartz 托管 obsidian 到网络上","content":"## Refer\r\nhttps://quartz.jzhao.xyz/notes/setup/\r\nhttps://quartz.jzhao.xyz/notes/obsidian/","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%81%9A%E6%80%BB%E7%BB%93%E5%92%8C%E6%84%9F%E6%83%B3%E7%9A%84%E6%97%B6%E5%80%99%E9%9C%80%E8%A6%81%E5%85%B7%E4%BD%93%E7%9A%84%E4%BA%8B%E4%BE%8B":{"title":"做总结和感想的时候需要具体的事例","content":"\r\n由前文想到，不同情况下人会有不同的感想，但是事例不会变，如果只记录当时的感悟和道理，并不能和后续的想法做对比。没有事例也没有说服力。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%85%AC%E6%96%87%E5%86%99%E4%BD%9C":{"title":"公文写作","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n- [[公文写作助记词]]\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%85%AC%E6%96%87%E5%86%99%E4%BD%9C%E5%8A%A9%E8%AE%B0%E8%AF%8D":{"title":"公文写作助记词","content":"- 统一描述：正确的、直接的、中肯的、雅致的、客观的、完整的、立体的、全面的、辩证的、形而上学的、雅俗共赏的、一针见血的、直击要害的\r\n- 错误的、间接的、虚假的、庸俗的、主观的、残缺的、平面的、片面的、孤立的、辩证法的、雅俗之分的、离题万里的、不痛不痒的、违背祖宗的、不合常理的、乱七八糟的、七上八下的、龙飞凤舞的、不知所谓的、无语至极的、不可理喻的、无法接受的、一派胡言的、错上加错的、钉在耻辱柱上的错误行径。\r\n- 针对性的、创造性的、发散性、\r\n- 以全局性、战略性、前瞻性的眼光把握时代性、坚持实践性、特别要有针对性，面对战略性、长期性、可讲性\r\n- 复杂性与艰巨性，调动积极性与创造性，有计划性、敏锐性的干好事情。在\r\n- 规范化、程序化与制度化的重要程度，只有在有序化、科学化、知识化、法制化、集约化、正常化、有序化、智能化、优质化、常态化、科学化、集约化、年轻化、专业化的条件下，我们才能让结果变得正常化。\r\n- 找准出发点、把握切入点、明确落脚点、找准落脚点、抓住切入点、把握着重点、找准切入点、把握着力点、抓好落脚点，注意着眼点、结合点、关键点、重视着重点、着力点、关键点，这些是做事情的支撑点。\r\n- 始终要秉承着：责任感、紧迫感、危机感、认同感、荣誉感、成就感。\r\n- 多层次、多方面、多途径、多渠道、多措施、多力量、多元素。\r\n- 历史的必然、现实的选择、未来的方向。要：立足当前，着眼长远，自觉按规律办事。要：抓住机遇，应对挑战，勇敢顺潮流而为。要：突出重点，分步实施，找准切入点实施。要：全面推进，统筹兼顾，综合治理，融入其中，贯穿始终，切实抓好，扎实推进，加快发展，持续增收，积极稳妥，狠抓落实，从严控制， 严格执行，坚决制止，明确职责，高举旗帜，坚定不移，牢牢把握，积极争取，深入开展，注重强化，规范程序，改进作风，积极发展，努力建设，依法实行，良性互动，优势互补，率先发展，互惠互利，做深、做细、做实、全面分析，全面贯彻，持续推进，全面落实、全面实施，逐步扭转，基本形成，普遍增加，基本建立， 更加完备，逐步完善，明显提高，逐渐好转，逐步形成，不断加强，持续增效，巩固深化，大幅提高，显著改善，不断增强，日趋完善，比较圆满的完成任务！","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%85%AC%E6%9C%89%E4%BA%91%E5%92%8C%E7%A7%81%E6%9C%89%E4%BA%91%E5%8D%A0%E6%AF%94":{"title":"","content":"[[AWS]] CEO Andy Jessy在AWS 2019 re:Invent大会上分享了如下消息：[[公有云]]的总支出只占到总IT支出的3% 。IT支出中，[[私有云]]仍占绝大多数","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%88%86%E5%B8%83%E5%BC%8F":{"title":"分布式","content":"\r\n[[巨石应用]] 其实也能满足老旧小公司的需求，只是技术含量跟不上时代罢了。\r\n\r\n其实真正需要的是低成本，灵活性，随取随用，轻松连接，这也是云时代的需求。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F":{"title":"分布式系统","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  分布式系统理论：[[CAP]]\r\n- [[FLP 不可能性|FLP Impossibility]]\r\n- [[BASE 理论]]\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%88%A9%E5%9F%BA%E5%B8%82%E5%9C%BA":{"title":"","content":"**利基市场**（英语：niche market）也称**利益市场**、**小众市场**，是指由已有[市场占有率](https://zh.m.wikipedia.org/wiki/%E5%B8%82%E5%A0%B4%E4%BD%94%E6%9C%89%E7%8E%87 \"市场占有率\")绝对优势的[企业](https://zh.m.wikipedia.org/wiki/%E4%BC%81%E4%B8%9A \"企业\")所忽略之某些细分[市场](https://zh.m.wikipedia.org/wiki/%E5%B8%82%E5%A0%B4 \"市场\")，并且在此市场尚未完善供应服务。一般由较小的产品市场并具有持续发展的潜力中，一些需要但尚未被满足的族群消费者所组成。为了满足特定的市场需求，价格区间与产品质量，针对细分后的产品进入这个小型市场且有盈利的基础。企业根据自身所特有的资源优势，经由专业化的经营将品牌意识灌输到该特定消费者族群中，逐渐形成该族群的领导品牌，来占领这些市场，从而最大限度的获取收益所采取之竞争战略。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%89%8D%E6%B2%BF%E7%9F%A5%E8%AF%86":{"title":"","content":"- 梳理一下我学习的过去的前沿知识和未来要学习的范畴\r\n\t- [[SaaS]]、[[IaaS]]\r\n\t- [[GitOps]]：真没用\r\n\t- [[AIops]]：算法真是锦上添花的东西\r\n\t- [[DevSecOps]]：融入安全理念，希望结合出新东西吧\r\n\t- [[Workflow as Code|Temporal]]\r\n\t- [[PriceOps]]","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%8C%BA%E5%9D%97%E9%93%BE%E9%92%B1%E5%8C%85":{"title":"区块链钱包","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%9C%A8-Pod-%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8%E4%B9%8B%E9%97%B4%E5%85%B1%E4%BA%AB%E8%BF%9B%E7%A8%8B%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4":{"title":"","content":"[在 Pod 中的容器之间共享进程命名空间 | Kubernetes](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/share-process-namespace/#%E9%85%8D%E7%BD%AE-pod)\r\n\r\n```bash\r\nps ax\r\n```\r\n\r\n输出类似于：\r\n\r\n```bash\r\nPID   USER     TIME  COMMAND\r\n    1 root      0:00 /pause\r\n    8 root      0:00 nginx: master process nginx -g daemon off;\r\n   15 root      0:00 sh\r\n   22 101       0:00 nginx: worker process\r\n   23 root      0:00 ps ax\r\n```","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E7%9A%84%E5%9F%B9%E8%82%B2":{"title":"基础设施的培育","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  [[Hashicorp]] 现在就像是 [[IaaS]] 行业的标准了，但是现在我们需要的是各个行业的细分标准的确立\r\n- 像 [[altogic]] 这样的软件服务，只要[[IaaS|基础设施即服务]]能够做好，上层这些应用构建都不太难。\r\n- [[行业的细分标准-基础设施层|如何定义当前行业的细分标准]]？\r\n\t- 不清楚，现在要做的是尝试去做这方面的工作。以快速提供一个 [[firestore]] 类似服务的框架为载体探究一下如何完成这件事。\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%A4%A7%E5%9E%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B":{"title":"大型深度学习模型","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n- 为什么大型深度学习模型需要极大的内存 #query \r\n\r\n## 主要笔记\r\n- 需要大内存储存==**中间层**==的==**激活函数输出**==和==**权重**==等\r\n- 模型训练限制\r\n\t- 只能在单个 GPU 上训练，批大小（batch size）设置得极小\r\n\t- 太大的模型，单个 GPU 又放不下\r\n- 大型模型训练方法：\r\n\t- [[数据并行]]\r\n\t- [[模型并行]]\r\n- [[2022-10-18]] 当前流行的九大深度学习库\r\n\t1. **[[Megatron-LM]]**\r\n\t2. **[[DeepSpeed]]**\r\n\t3. **[[FairScale]]**\r\n\t4. **[[ParallelFormers]]**\r\n\t5. **[[ColossalAI]]**\r\n\t6. [[Alpa]]\r\n\t7. [[Hivemind]]\r\n\t8. **[[OneFlow]]**\r\n\t9. [[Mesh-Tensorflow]]\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86":{"title":"","content":"存储管理关注的不是高性能，而是高扩展，管理面的能力以存储为例，其实就是贯彻整个存储生命周期，通过管理的手段提高存储价值。\r\n\r\n因此，数据的生产、[[备份]]、[[故障隔离]]、[[容灾]]、恢复、[[持续数据保护]]、安全能力接入、数据 AI 接入这些属于管理面的能力。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%AD%98%E5%82%A8%E9%A2%86%E5%9F%9F%E7%9A%84%E4%BA%8B%E4%BB%B6":{"title":"存储领域的事件","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  全球存储发展的几大趋势：**分布式**、**云化**、**[[闪存化]]**、**[[存储领域的智能化|智能]]**\r\n\r\n\r\n## 主要笔记\r\n### 事件\r\n2010年：EMC收购Isilon，22.5亿美元 ;\r\n\r\n2010年：惠普收购3Par，23.5亿美元 ;\r\n\r\n2011年：戴尔收购Compellent，9.6亿美元 ;\r\n\r\n2011年：[[希捷]]收购[[三星]]硬盘，13.75亿美元 ;\r\n\r\n2011年：[[西部数据]]收购日立GST，48亿美元 ;\r\n\r\n**2012**年：[[VMware]]提出[[SDDC（软件定义数据中心）]]和[[SDS（软件定义存储）]]的概念，之后推出SDS产品 – Virtual SAN的预览版 \r\n\r\n2013年：Avago收购LSI，66亿美元 ;\r\n\r\n2014年：闪迪收购Fusion-io，11亿美元 ;\r\n\r\n**2015**年：戴尔收购EMC，630亿美元 ;\r\n\r\n2015年：[[西部数据]]收购闪迪，160亿美元 ;\r\n\r\n2016年：IBM收购对象存储，约13亿美元 ;\r\n\r\n**2016**年：[[HCI]]（超融合）概念提出者[[Nutanix]]上市；\r\n\r\n2016年：博通收购博科，59亿美元 ;\r\n\r\n2016年：OpenText收购Dell EMC企业内容部门(包括Documentum等)，16.2亿美元 ;\r\n\r\n2017年：HPE收购Nimble Storage，10.9亿美元 ;\r\n\r\n**2018**年：[[微软]]收购混合云数据存储公司Avere Systems ;\r\n\r\n**2019**年:  [[AWS]]**收购E8 Storage**，估计在5000万美元至6000万美元之间 ;\r\n\r\n2019年: [[谷歌]]收购存储企业Elastifile, 2亿美元 ;\r\n\r\n2019年:  [[IBM]]收购 RedHat, 340亿美元，[[RedHat]]有两款开源存储产品：[[Ceph]]和[[Gluster]]。\r\n### Refer\r\n- [[未来人才的学习目标#^eda002]]\r\n- [中国存储网存储历史时间线](https://www.chinastor.com/history/)","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%AD%98%E5%82%A8%E9%A2%86%E5%9F%9F%E7%9A%84%E6%99%BA%E8%83%BD%E5%8C%96":{"title":"","content":"![[Pasted image 20221013145353.png]]","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%AD%A6%E4%B9%A0%E7%A8%80%E7%BC%BA%E7%9A%84%E6%8A%80%E8%83%BD":{"title":"学习稀缺的技能","content":"\r\n\u003e _应用异常值算法的困难正是这样做的原因！更困难意味着更稀有。_\r\n\r\n为了帮助您开始应对这些挑战，以下是我一遍又一遍地回顾的四个经验法则。虽然只有四个，但它们的冲击力很大。它们是数百小时思考如何更好地找到稀有和有价值的技能的简化：\r\n\r\n1.  **成为第一个学习有价值的技能的人。** 跟踪新兴科学、技术、应用程序、工具、行业和领域。当它们呈指数增长时，花几个小时探索它们，看看是否值得投入更多时间。它可以帮助您尝试新事物，而不会将时间浪费在变成无用的事物上。\r\n2.  **学习很难的有价值的技能。** 愿意投资于禁忌、不是超级性感的、耗时、费力、看起来有风险或超级技术或学术的领域。\r\n3.  **学习具有隐藏好处的宝贵技能。** 人类有价值盲点。他们低估了具有抽象、长期回报的技能；微技能；其他学科的技能；和被遗忘的经典思想。\r\n4.  **重新定义价值比共识更好。** 在我们的职业生涯中，我们利用我们的技能为他人服务。可能是客户、老板、我们的粉丝、招聘人员或其他人。如果您能够比其他人更好地了解您所服务的人的未满足需求，您将能够更好地满足这些需求。\r\n\r\n或者，简而言之，请记住：\r\n\r\n-   成为第一\r\n-   面对困难\r\n-   寻找隐藏的好处\r\n-   重新定义价值\r\n\r\n以上是 Michael Simomons 的学习稀有技能的理论，其中总结的几点我们应该尝试的规则，在他的规则下，他指导有如下学习资源（随个人的情景可以有变种，这是当然的）\r\n![[2022-09-29#^cff4f1]] \r\n\r\n## 第一步，解释：我们现在从规则开始解释，从而找到我们自己情景下的学习\r\n1. 首先，所有的技能都很有价值，然后我们要去寻找稀缺性的技能。这个前提是我们先找到有价值的技能，通过一个指数增长的模型判断一个事物发展是否超出常规，这通常都代表这件事有价值。那我们以程序员的视角举例， Web3 正好是急需技能之一（因为需求而有价值），物联网，边缘方面的技能也并驾齐驱。\r\n2. 然后我们去寻找技能的**稀缺性**！（因为稀缺性决定价格）这也有一个模型来描述他，即被禁止的，未被大部分人发现的，可能枯燥的，耗时，费力的，或者看起来非常有风险的，以及非常学术的，非常技术专业的领域。（毕竟你不能指望一蹴而就的领域有多稀缺，信息差是很难一直保持的）\r\n3. 同时不要忽视长期的**被动技能**，复利主义，以及一些经典思想，他们由于长期存在于我们的身边，会被我们的价值评估器忽视。以抽象的，长期回报的技能为例：写笔记，坚持总结输出；微技能，一些工作上的小技巧，比如电脑使用领域上的一些工具快捷键；**其他领域的技能！** 往往会产生巨大的**化学反应**，比如生物对计算机领域的影响，当然我还是建议学数学，比如金融割韭菜融入区块链；以及一些经典思想：分治和中间层的思想贯彻整个计算机科学，经常发现计算机的一些新领域应用了老的经典的思想从而大放异彩，比如深度学习啦！\r\n4. 重新定义价值而非共识，这句类比对一个行业需求的深挖。技能可以为每一个使用者提供相同影响的结果，但精准的需求分析和挖掘，能让某项技能发掘出更多的价值，他提升了价值的同时也提升了稀缺性。\r\n\r\n当然，以上规则其实主要是个人视角出发，如何发挥个人最大的影响力的思考，从团队等方向还能有更多，比如规模效应，然而我们希望将讨论范围限制在个人视角上以期提供一些切实的指导。\r\n\r\n\t道理的扩散成本是非常低的，但是让一个人相信这个道理的成本是极高的。\r\n\r\n## 第二步 分析学习资源的新变化\r\n| 考虑稀缺性前                                 | 后                                                           |\r\n| -------------------------------------------- | ------------------------------------------------------------ |\r\n| 阅读最新的畅销书                             | 学术论文                                                     |\r\n| 时刻检查社交媒体查看最具影响力的人的发言     | 领域之外的学科（加密学、经济学）                             |\r\n| 时刻保持关注行业最新的新闻（所有人都在关注） | 获取一些专有数据的授权并研究                                 |\r\n|                                              | 和领域内部人士建立深厚的关系（并且他们很少公开分享自己的观点 |\r\n|                                              | 一些抽象的心理模型                                                             |\r\n\r\n可以看到其中明显的差距，同样的例子还有很多，就不一一列举了。笔者这里最想强调的还是稀缺代表远离共识，意味着你的选择几乎总是错的（即使你是个顶尖聪明的人），这是需要谦虚的态度，避免一次投入过多，需要有足够多的证据、验证。所以保持谦虚，离群才能尽量保证个人的稳定。\r\n\r\n以程序员的视角来讲，可以先有这几条经验：\r\n1. 不需要看书，书几乎总是过时的，不稀缺的，有需求的技能学习途径或者基础，尽量看学术论文。\r\n2. 避免关注颇具影响力的人的一言一行，避免一直关注，要么深挖，要么略过。\r\n3. 多接触一些其他领域的学识，当前行业的其他领域：安全、底层、抽象论证，其他行业的各个领域：哲学（游戏理论）、数学、经济学、等等。\r\n4. 专有数据并不一定需要授权，也需要自己收集，当你的一些独特视角需要验证时，可以轻松通过编程（最好合法）爬取需要的信息分析验证（如果有一些微技能比如数据分析，会让这个过程更顺理成章）\r\n5. 不用关注网红，而是转而建立深厚的个人关系，这其实在个人认知（资源等）不足的情况下的最好选择，请寻找自己身边的这种人，如果接触不到，或许你应该换一个环境。\r\n6. 保持写笔记的习惯，持续锻炼自己的心理模型，这个过于抽象，不过当你阅读这篇笔记的时候，其实就在做这样的事。延续它！\r\n\r\n## 下一步，选择\r\n\r\n现在，实践一下这个稀缺性的模型，找到适合你的技能，至少不盲目的学习了是吗？","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%AE%B9%E5%99%A8%E5%AE%B9%E7%81%BE%E5%A4%87%E4%BB%BD":{"title":"容器容灾备份","content":"\r\n## 主要笔记\r\n-  [[velero]] 是现行的 [[Kubernetes]] 集群[[异地灾备]]的主要选择，由 [[VMware]] 开发。\r\n- [[容器]]备份容灾的功能的工作原理是\r\n\t- 用于存储[[容器镜像]]的数据（即[[PV]]）、[[Kubernetes]] 资源对象（即一系列 yaml 文件）的服务\r\n\t\t- 服务可以是 [[Minio]] 提供[[对象存储服务]]\r\n\t\t\t- 举例：[[本地磁盘]]由 [[carina]] 纳管提供的。\r\n\t\t- 也可以不上传，直接调 [[SC]] 对 [[PV]] 进行 [[Snapshot]]\r\n- 在管理上提供了更多的工作机制，从 [[velero]] 可以梳理这些\r\n\t- 按需备份恢复\r\n\t\t- 例如，您可能需要告诉数据库在拍摄快照之前将其内存缓冲区刷新到磁盘。来保证[[应用一致性]]，三个一致性中（[[不一致性]]，[[崩溃一致性]]和[[应用一致性]]）\r\n\t\t\t- 在备份前对数据库进行 quiesce 操作，备份完 unquiesce。保证应用一致性，该点属于\r\n\t- 自动定期执行\r\n\t- 磁盘读写限速（避免影响业务）\r\n\t- 增量备份\r\n\t- 差异定制化选择\r\n\t\t- 解释一下：不同集群毕竟还是有不一样的地方，比如 NodePort 类型的 Service，本身就有随机性，是否要在恢复的时候恢复到新集群呢？提供一个参数来选择  `--preserve-nodeports`\r\n- [[Kubernetes 容灾解决方案]]梳理以下要点\r\n\t- 备份粒度要到[[容器]]\r\n\t- 可以备份数据（即 [[PV]]）和配置（即一系列资源的 yaml）\r\n\t- 可以感知 [[Kubernetes]] 的[[命名空间]]\r\n\t- 适合[[多云]]和[[混合云]]环境\r\n\t- 保持[[应用一致性]]\r\n\t\t- 换句话说也要有应用这个层面的抽象，这可能需要结合 [[Kubevela]] 和 [[OAM]] 这样的工具\r\n\t- [[RTO]] 和 [[RPO]] 需求对应不同方案\r\n\t\t- 同步（例如：[[CDP]] ）与异步的方案影响\r\n\t\t- 网络状态（数据中心的地理位置）\r\n\t\t- 数据大小","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%AE%B9%E5%99%A8%E7%9A%84%E4%B8%80%E5%8F%B7%E8%BF%9B%E7%A8%8B":{"title":"容器的一号进程","content":"\r\n## 摘要\r\n\r\n[[容器]]有自己的 Pid Namespace，从 1 开始计数。容器里 init 进程也叫 1 号进程，同时也是容器执行的程序本身。\r\n换句话说，kill 1 号进程，容器也就被 kill 了\r\n\r\n## 主要笔记\r\n\r\n- [[pause 容器]]的作用[深入理解容器的单进程模型和 k8s 中的 pause 容器 - kkbill - 博客园](https://www.cnblogs.com/kkbill/p/12952815.html)\r\n  1.  作为每个 pod 中共享 Linux Namespace 的基础\r\n  2.  [[在 Pod 中的容器之间共享进程命名空间||启用命名空间共享功能]]共享 PID namespace 之后，作为每个 pod 中 PID 为 1 的进程，负责回收僵尸进程。\r\n- from: [02 理解进程（1）：为什么我在容器中不能 kill 1 号进程？.md](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/02%20%E7%90%86%E8%A7%A3%E8%BF%9B%E7%A8%8B%EF%BC%881%EF%BC%89%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%9C%A8%E5%AE%B9%E5%99%A8%E4%B8%AD%E4%B8%8D%E8%83%BDkill%201%E5%8F%B7%E8%BF%9B%E7%A8%8B%EF%BC%9F.md)\r\n  1.  在容器中，1 号进程永远不会响应 SIGKILL 和 SIGSTOP 这两个特权[[信号]]；\r\n  2.  对于其他的信号，如果用户自己注册了 handler，1 号进程可以响应。\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%B4%A9%E6%BA%83%E4%B8%80%E8%87%B4%E6%80%A7":{"title":"崩溃一致性","content":"## 摘要\n崩溃一致性备份会捕获[[备份]]时[[磁盘]]上已存在的数据，文件/磁盘数据在同一时间点，但不会备份内存数据并且[[静默应用系统]]，不保证应用系统备份一致性。尽管并未保证应用一致性，但通常情况下，操作系统重启后会进行 chkdsk 等磁盘检查过程来修复各种损坏错误，数据库会进行日志回滚操作保证一致性。\n\n## 问题、提示\n-  \n\n## 主要笔记\n-  \n\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%BA%94%E7%94%A8%E4%B8%80%E8%87%B4%E6%80%A7":{"title":"应用一致性","content":"## 摘要\r\n文件/磁盘数据在同一时间点，并备份内存数据，保证应用系统一致性。\r\n\r\n需要应用自身支持，比如数据库软件通常都支持。\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6":{"title":"微服务二次开发框架","content":"## 摘要\r\n核心是一套可定制的平台，以 [[Go]] Binary 程序和[[配置文件]]的方式安装基础设施。是二次开发平台，不是最终产品。\r\n功能核心是 [[SQL]] 扩容，[[文档型数据库]]描述模型和 [[MInio]] 扩容。\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98":{"title":"性能调优","content":"在非分布式时代，单机性能调优还有些作用。在云上时代，除非没钱需要[[降本增效]]，不然加机器加配置就行。相对于商业价值来说，一般的硬件成本占比较低。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E6%97%85%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8":{"title":"旅行的本质","content":"用异化思想去思考：\r\n旅行的本质是\r\n-   个人远离他们文化的真实道路\r\n-   深入未知的深处重塑自我\r\n-   然后将这种学习带回他们的文化，这样它就可以发展\r\n\r\n旅行就是人类的一种[[离群算法]]应用，另一种应用是学习具有稀缺性的技能 [[学习稀缺的技能]]\r\n\r\n为什么小说里的英雄冒险那么吸引人 -\u003e [[英雄之旅]] ","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E6%9C%AA%E6%9D%A5%E4%BA%BA%E6%89%8D%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87":{"title":"","content":"### Summary\r\n做 ==AI 的基础设施==的学习\r\n\r\n### Questions/Cues\r\n- 2022-10-12 [[学习稀缺的技能]]对[[程序员]]来说是哪些？ #query\r\n\r\n### Notes\r\n做 [[AI 的基础设施]]，再逐渐过渡到数据，后续还有数据交易这些。我的观点是不做ai，毕竟优势不在这。\r\n\r\n最优选择还是和金融相关、数据交易相关，但是这些目前都还是起步，风险大，慢慢看就行。\r\n\r\n主要是对于这个行业的理解，必须随着持续做下去，才能看到清晰的方向。这样有机会的话，可以带着技术换个更牛的企业，比如银行。\r\n\r\n做复合型人才，主要还是要做，做的过程中体会，不要着急，我觉得都没必要在三年内锁死自己发展方向。\r\n\r\n存储、网络如果发展的好，本地计算机意义都不大，问题是在什么时候才能达到预期，那么[[云操作系统]]，又应该是什么形态，这些作为主攻方向都是不错的。 ^eda002\r\n\r\n反正我觉得与其在性能上下功夫，不如在管理上，硬件成本又不是大问题，对不对。\r\n\r\n即使在云端考虑，也是[[管理高于性能]]的，做管控，不做技术细节的提升，本身优势就很大，单项技术深入研究难度极高，但是管理控制是综合技术应用，空间也大。\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E6%B5%8B%E8%AF%95":{"title":"测试","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  在写[[集成测试]]","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E6%B8%85%E7%90%86-GIt-%E4%B8%AD%E7%9A%84%E5%8E%86%E5%8F%B2%E6%96%87%E4%BB%B6":{"title":"清理 GIt 中的历史文件","content":"```\r\ngit filter-branch --force --index-filter 'git rm --cached --ignore-unmatch path-to-your-remove-file' --prune-empty --tag-name-filter cat -- --all\r\n```\r\n\r\n其中, path-to-your-remove-file 就是你要删除的文件的相对路径(相对于git仓库的跟目录), 替换成你要删除的文件即可. 注意一点，这里的文件或文件夹，都不能以 '/' 开头，否则文件或文件夹会被认为是从 git 的安装目录开始。\r\n\r\n如果你要删除的目标不是文件，而是文件夹，那么请在 `git rm --cached` 命令后面添加 -r 命令，表示递归的删除（子）文件夹和文件夹下的文件，类似于 `rm -rf` 命令。\r\n\r\n更多请参考：[ https://help.github.com/articles/remove-sensitive-data](https://help.github.com/articles/remove-sensitive-data)","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E7%89%B9%E6%9D%83%E5%AE%B9%E5%99%A8":{"title":"特权容器","content":"## 摘要\r\n\r\n## 问题、提示\r\n- 容器中为什么无法运行 systemd 进程？ #query \r\n\t- [如何在Docker里面使用systemctl - Ehds](https://ehds.github.io/2021/01/21/docker_systemctl/)\r\n## 主要笔记\r\n- docker 通过 `--privileged=true` 和 `/usr/sbin/init` 共同启动特权容器，可以不用后面的命令，区别是：\r\n    - 前者只是切换了 root 身份，但 Shell 环境仍然是普通用户的 Shell；而后者连用户和 Shell 环境一起切换成 root 身份了。\r\n- 通常是为了使用 systemctl 而开启特权容器，原因是：\r\n\t- \u003e That’s because “systemctl” talks to the systemd daemon by using the d-bus. In a container there is no systemd-daemon. Asking for a start will probably not quite do what you expect - the dev-mapping need to be a bit longer.\r\n\t\t- 容器里面是没有systemd进程的，所以不能正常开启systemctl。\r\n\t- 因为 docker只是提供了进程隔离，不是操作系统的虚拟。\r\n\t\t- This is by design. Docker should be running a process in the foreground in your container and it will be spawned as PID 1 within the container’s pid namespace. Docker is designed for process isolation, not for OS virtualization, so there are no other OS processes and daemons running inside the container (like systemd, cron, syslog, etc), only your entrypoint or command you run.  \r\n\t\t- If they included systemd commands, you’d find a lot of things not working since your entrypoint replaces init. Systemd also makes use to cgroups which docker restricts inside of containers since the ability to change cgroups could allow a process to escape the container’s isolation. Without systemd running as init inside your container, there’s no daemon to process your start and stop commands.\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E7%94%9F%E6%B4%BB%E6%B2%A1%E6%9C%89%E6%84%8F%E4%B9%89":{"title":"生活没有意义？","content":"\r\n参见我的 [[Focus and Different]] 原则","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E7%A7%81%E6%9C%89%E4%BA%91":{"title":"私有云","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  [[HCI|超融合]] 可以视为私有云的一种部署形态\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E7%A7%8B":{"title":"","content":"秋桜　aki sakura\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E9%97%AE%E9%A2%98":{"title":"端口占用问题","content":"## 摘要\r\n- `ps -ef | grep` then `netstat -anp | grep pid`\r\n- `lsof -i:port_num`\r\n\r\n## 主要笔记\r\n### netstat\r\nnetstat -tunlp 用于显示 tcp，udp 的端口和进程等相关情况。\r\nnetstat 查看端口占用语法格式：\r\nnetstat -tunlp | grep 端口号\r\n-   -t (tcp) 仅显示tcp相关选项\r\n-   -u (udp)仅显示udp相关选项\r\n-   -n 拒绝显示别名，能显示数字的全部转化为数字\r\n-   -l 仅列出在Listen(监听)的服务状态\r\n-   -p 显示建立相关链接的程序名","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E7%AE%A1%E7%90%86%E9%AB%98%E4%BA%8E%E6%80%A7%E8%83%BD":{"title":"","content":"### 摘要\r\n\r\n\r\n### 问题、提示\r\n- \r\n\r\n### 主要笔记\r\n-  [[分布式]] 和 [[云操作系统]] 带来的变化\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E8%81%8C%E4%B8%9A%E6%9C%AC%E7%A7%91":{"title":"","content":"高等职业本科，是全日制本科[学历教育](https://baike.baidu.com/item/%E5%AD%A6%E5%8E%86%E6%95%99%E8%82%B2/10592032?fromModule=lemma_inlink)的一种，学位为[学士学位](https://baike.baidu.com/item/%E5%AD%A6%E5%A3%AB%E5%AD%A6%E4%BD%8D/1737875?fromModule=lemma_inlink)。[高职本科](https://baike.baidu.com/item/%E9%AB%98%E8%81%8C%E6%9C%AC%E7%A7%91/7222073?fromModule=lemma_inlink)与[普通本科](https://baike.baidu.com/item/%E6%99%AE%E9%80%9A%E6%9C%AC%E7%A7%91/4342303?fromModule=lemma_inlink)共同构成我国高等教育体系的全日制[本科层次](https://baike.baidu.com/item/%E6%9C%AC%E7%A7%91%E5%B1%82%E6%AC%A1/23566221?fromModule=lemma_inlink)。\r\n\r\n肩负着培养面向生产、建设、服务和管理第一线需要的高素质的应用技术型和职业技能型高等专业人才的使命。\r\n\r\n其中最重要的是高素质和高技能，注意他们工作的领域，生产建设，服务管理。如果没有高素质，社会很难安定，如果没有高技能，就很容易被科技淘汰。随着时代发展，低级的生产，建设，会降低占比，这代表提高职高人素质势在必行，国家的相关文案也是如此：为了适应[国家经济](https://baike.baidu.com/item/%E5%9B%BD%E5%AE%B6%E7%BB%8F%E6%B5%8E/4147005?fromModule=lemma_inlink)转型和新的[经济增长方式转变](https://baike.baidu.com/item/%E7%BB%8F%E6%B5%8E%E5%A2%9E%E9%95%BF%E6%96%B9%E5%BC%8F%E8%BD%AC%E5%8F%98/3385368?fromModule=lemma_inlink)对各类高级技能人才的需求，构建[现代职业教育](https://baike.baidu.com/item/%E7%8E%B0%E4%BB%A3%E8%81%8C%E4%B8%9A%E6%95%99%E8%82%B2/14890026?fromModule=lemma_inlink)的“立交桥”，促进现代职业教育的发展。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E8%87%AA%E5%8A%A8%E5%86%85%E5%A4%96%E7%BD%91%E8%B7%AF%E7%94%B1-bat-%E8%84%9A%E6%9C%AC":{"title":"自动内外网路由 bat 脚本","content":"```batch\r\n@:loop\r\n@time /T\r\n@REM REM 是注释. '@'是关闭回显.\r\n@REM 1.1.1.1 是网关, 对应修改.\r\n@REM For Chengdu\r\n@set gateway=1.1.1.1\r\n@REM For Shenzhen\r\n@REM @set gateway=1.1.1.1\r\n\r\n@REM 内网网络每间隔几秒钟会添加自己的网关为默认网关, 导致不能连通wifi外网\r\n@REM 删除内网网络添加的默认网关后, wifi网络才能连通.\r\n@route delete 0.0.0.0 %gateway% 2\u003enul\r\n\r\n@REM 根据自己的访问需求, 添加内网网段的白名单, 走内网路由.\r\n@route add 10.0.0.0  mask 255.0.0.0 %gateway% 2\u003enul\r\n@route add 200.200.0.0 mask 255.255.0.0 %gateway% 2\u003enul\r\n@route add 200.201.0.0 mask 255.255.0.0 %gateway% 2\u003enul\r\n@route add 192.200.0.0 mask 255.255.0.0 %gateway% 2\u003enul\r\n\r\n@timeout /T 1 /NOBREAK\r\n@goto :loop\r\n```\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E8%8B%B1%E9%9B%84%E4%B9%8B%E6%97%85":{"title":"英雄之旅","content":"\r\n英雄之旅就是人类社会中的最常见的原型神话。\r\n\r\n![[Pasted image 20220929203142.png]]![[Pasted image 20220929203529.png]]\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E8%A1%8C%E4%B8%9A%E7%9A%84%E7%BB%86%E5%88%86%E6%A0%87%E5%87%86-%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E5%B1%82":{"title":"","content":"","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E9%80%90%E6%AD%A5%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%AA%E6%96%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%BC%8F":{"title":"逐步描述一个新模型的方式","content":"通过描述一个东西，能提供什么，能做什么来感知他。\r\n\r\n不要急着下定义，这会让自己丧失求知欲。\r\n\r\n有点像编程领域的[[鸭子类型]]的延伸。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6":{"title":"配置文件","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  [[云原生的十二因素]]-可配置化\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E9%97%AA%E5%AD%98%E5%8C%96":{"title":"","content":"根据IDC 在2019年12月31日的报道：“2019前三个季度，中国企业级全闪存存储阵列市场同比增长超过60%。IDC预测， 2019年全闪存存储市场仍将达到近50%的增长”。\r\n\r\n据报道，[[闪存颗粒]]价格逐年下降（按照30%～40%的降幅）\r\n\r\n闪存化的过程中，必然催生[[全闪存阵列]]的发展，其中的相关技术[[NVMe Over Fabric]]（简称NVMe-oF）借助[[RDMA]]等超低延时的传输协议，可远程访问[[SSD]]，解决了下一代数据中心在横向扩展 (Scale-out) 时所遭遇的性能、功能、容量三者之间难以取舍的权衡问题","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E9%99%8D%E6%9C%AC%E5%A2%9E%E6%95%88":{"title":"降本增效","content":"新词，顾名思义\r\n\r\n和大环境也有关，[[diary/2022-10-12]] 记录：由于[[疫情]]原因，全球[[经济]]都有萎缩的倾向，大部分企业寻求降本增效来活下去。\r\n\r\n[[Kubernetes 集群的降本增效]]","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E9%9B%86%E6%88%90%E6%B5%8B%E8%AF%95":{"title":"集成测试","content":"## 摘要\r\n集成测试的对象是软件中的不同模块，即==不同模块==之间的交互（数据通信）是该类型测试关注点。\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  [[Kubernetes 的集成测试]]\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B":{"title":"鸭子类型","content":"同 Duck-Type\r\n\r\n原理是：只要一个东西会“嘎嘎”叫，就认为它是一只鸭子。\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/AI-%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD":{"title":"AI 的基础设施","content":"AI Infra","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/CTO-%E7%A1%AE%E5%AE%9E%E5%90%8C%E6%97%B6%E9%9C%80%E8%A6%81%E6%89%AE%E6%BC%94%E4%BC%98%E7%A7%80%E7%9A%84-engineer":{"title":"CTO 确实同时需要扮演优秀的 engineer","content":"\r\n### CTO 确实同时需要扮演优秀的 engineer\r\n需要对该角色设置一些评判满足标准，保证 CTO 满足达到标准[https://blog.southparkcommons.com/your-cto-should-actually-be-technical/](https://blog.southparkcommons.com/your-cto-should-actually-be-technical/)。该文章也在 HackerNews 上引起热议。","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/CUDA-Windows-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA":{"title":"CUDA Windows 环境搭建","content":"引用[[anaconda 安装]]\r\n\r\n## Windows 环境搭建\r\n\r\n### CUDA\r\n\r\n[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads?target_os=Windows\u0026target_arch=x86_64\u0026target_version=11\u0026target_type=exe_local)\r\n\r\n下载了 windows 下的 CUDA 11.7 的 local 版本，2.5g，下载安装即可\r\n\r\n安装完之后重启，可以运行`nvidia-smi.exe`查看一下自己的显卡。\r\n\r\n### Conda （python 环境）\r\n\r\n下载 miniconda\r\n\r\n[https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html) 安装\r\n\r\n通过 `code $PROFILE` 启动 vscode 添加该环境到 powershell 自动启动脚本中\r\n\r\n```powershell\r\n# 启动 miniconda 环境\r\nD:\\miniconda\\shell\\condabin\\conda-hook.ps1 ;conda activate 'D:\\miniconda'\r\n```\r\n\r\n  \r\n### Pytorch \u0026 Jupyter\r\n\r\n因为有梯子，速度挺快，就不用镜像源了\r\n\r\n[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) 找到对应 CUDA 的版本，用的 pip 下载，快一些比 conda。\r\n\r\n`pip3 install torch torchvision torchaudio --extra-index-url [https://download.pytorch.org/whl/cu116](https://download.pytorch.org/whl/cu116)`\r\n\r\n  \r\n\r\n`pip install matplotlib numpy jupyterlab`\r\n\r\n## oneflow\r\n\r\n[https://docs.oneflow.org/master/index.html](https://docs.oneflow.org/master/index.html)\r\n\r\n`python3 -m pip install -f https://release.oneflow.info oneflow==0.8.0+cu112`\r\n\r\n### 动手学深度学习\r\n\r\n可以从这里下载大量的 nodebook 供学习  \r\n\r\n[https://github.com/d2l-ai/d2l-zh](https://github.com/d2l-ai/d2l-zh)","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/ConfigMap":{"title":"ConfigMap","content":"Kubernetes 中的一个对象。可以理解为一个 yaml 文件专门用作 APP 的配置文件，提供了相应的\r\n- 挂载\r\n- 监控更新\r\n等功能。\r\n\r\n限制：只有 1m 大小","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/DeepSpeed":{"title":"DeepSpeed","content":"#github\r\nIntro::: DeepSpeed是Microsoft推出的深度学习库，用于训练Megatron-Turing NLG 530B和BLOOM等大型模型，在训练、推理和压缩三方面都有所创新。\r\n\r\nLink::: https://github.com/microsoft/DeepSpeed\r\n\r\nDeepSpeed具有如下优点：\r\n\r\n-   可进行十亿乃至万亿级参数的稀疏/密集模型的训练/推理\r\n-   可实现超高系统吞吐量，可高效扩展至数千个GPU\r\n-   可在资源有限的GPU系统上进行训练/推理\r\n-   可实现前所未有的低延时与高推理吞吐量\r\n-   可用低成本实现压缩，从而大幅降低推理延迟并缩减模型大小","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Dockerfile-%E5%A4%9A%E8%A1%8C%E8%84%9A%E6%9C%AC":{"title":"Dockerfile 多行脚本","content":"```\r\n# syntax = docker/dockerfile:1.4\r\nFROM debian \r\n-RUN apt-get \u0026\u0026 \\ \r\n- apt-get install -y vim \r\n +RUN \u003c\u003ceot bash \r\n+ apt-get update \r\n+ apt-get install -y vim \r\neot\r\n```","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/FairScale":{"title":"FairScale","content":"#github\r\nIntro::: FairScale是由Facebook Research开发的PyTorch扩展库，具备高性能，可用于大型训练。\r\nLink::: https://github.com/facebookresearch/fairscale\r\n\r\n#### FairScale的愿景如下：\r\n-  易用性，开发简单易懂的FairScale API，使用户方便上手。\r\n-  模块化，实现模块化，使用户可将多个FairScale API无缝加入其训练循环。\r\n-  性能，实现 FairScale API的高可扩展性和高效率。\r\nFairScale支持完全分片数据并行（FullyShardedDataParallel，FSDP），FSDP是扩展大型神经网络训练的推荐方法。","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Focus-and-Different":{"title":"Focus and Different","content":"## 摘要\r\n最近总结的 #原则 和 #观念 ： 意义这种东西是通过聚集一个层面产生的。\r\n\r\n## 问题、提示\r\n-  为什么会觉得[[生活没有意义？]] #query \r\n\t- 其实更应该聚焦自己的当下生活，就会发现任何以前评估没有太大影响的事件，都对自己至关重要。 \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Go":{"title":"Go","content":"[[Golang]] 的简写，一门计算机语言，[[我]]比较擅长。","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/GoJS":{"title":"GoJS","content":"#github\r\nLink:::https://github.com/NorthwoodsSoftware/GoJS\r\nIntro:::JavaScript diagramming library for interactive flowcharts, org charts, design tools, planning tools, visual languages.\r\n\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/IaaS":{"title":"IaaS","content":"## 摘要\r\n\r\n\r\n## 针对笔记问的问题\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/KDE-%E7%AA%97%E5%8F%A3%E9%BB%91%E8%BE%B9%E7%AA%97%E5%8F%A3%E7%AE%A1%E7%90%86%E5%99%A8-Kwin-%E9%9C%80%E8%A6%81%E9%87%8D%E5%90%AF":{"title":"KDE 窗口黑边，窗口管理器 Kwin 需要重启","content":"## 摘要\r\n输入命令可以通过 `alt+f2` 或者 `alt+space` 快捷键唤出快捷烂直接输入运行。\r\n窗口卡死，没法切换了，通过命令\r\n`kwin_x11 --place` 重启 kwin 解决\r\n如果是 plasmashell 问题可以用 \r\n`kquitapp5 plasmashell \u0026\u0026 kstart5 plasmashell`\r\n或者 `plasmashell --place` 。\r\n\r\n如果还没法解决，直接重启整个图形界面吧。\r\n`sudo systemctl restart display-manager`\r\n\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Kubernetes-%E7%9A%84%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88":{"title":"Kubernetes 的性能瓶颈","content":"[[Kubernetes]] 有很多[[性能调优]]的参数。\r\n\r\n2022年10月12日：前年看到的限制是 5000 pod，印象里这个值没有太大提升。\r\n\r\n单 Node Pod 承载数：一般在 100 以下，也有强力[[Kubernetes 集群的降本增效]]到 140 的","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Kubernetes-%E7%9A%84%E9%9B%86%E6%88%90%E6%B5%8B%E8%AF%95":{"title":"Kubernetes 的集成测试","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n- Kubernetes 的[[集成测试]]怎么做的？\r\n- 如何让自己的应用在 Kubernetes 中进行集成测试？ #query \r\n\t- [[Kind]] 可能就是答案。Refer [Running KIND Inside A Kubernetes Cluster For Continuous Integration | D2iQ](https://d2iq.com/blog/running-kind-inside-a-kubernetes-cluster-for-continuous-integration)\r\n\r\n## 主要笔记\r\n- 模块依赖：最依赖的两个模块分别是 ETCD 和 APIServer \r\n\t- ETCD：Kubernetes 集成测试需要安装 [[etcd]]（只要安装即可，不需要启动），测试的时候通过 [[Golang]] 调用 Command 执行二进制程序的方式启动。\r\n\t\t- [来自源码](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/test/integration/framework/etcd.go?L129\u0026subtree=true#tab=def)\r\n\t- [[APIServer]] 是集成测试中必要且复杂的模块，[[StartTestServer]] 函数初始化了一些本地测试需要的配置，然后通过 [/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[cmd /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd)[kube-apiserver /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kube-apiserver)[app /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kube-apiserver/app)[server.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kube-apiserver/app/server.go) 目录的[app.CreateKubeAPIServer](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/test/integration/framework/test_server.go?L168) 创建了一个本地测试服务器，这样也就启动了一个集成测试环境啦。\r\n\t\t- [启动来源代码](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/test/integration/framework/test_server.go)\r\n- 项目结构组织：[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[test /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/test)[integration /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/test/integration) 在这个目录下存放集成测试的代码，以测试的对象分类（如 [[Deployment]], [[ConfigMap]]，[[Kubernetes Client|Client]]），用于复用的测试框架在文件夹 [framework](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/test/integration/framework) 内，如 [ETCD](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/test/integration/framework/etcd.go?L68) 和 [APIServer](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/test/integration/framework/test_server.go) 的启动都是该框架的函数调用。\r\n\t- 这样的集成测试框架很有用，比如开发 [[Kubernetes]] 的 [[Operator]] 的时候，[`controller-runtime`](http://sigs.k8s.io/controller-runtime) 提供 `envtest` ([godoc](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/envtest?tab=doc)) 帮助设置并启动的 [[controllers]] 实例来写集成测试，不需要 [[kubelet]]，[[controller-manager]] 或者其他组件。\r\n- 触发方式：通过 `makefile` 的脚本触发，具体执行操作在：[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[hack /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/hack)[make-rules /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/hack/make-rules)[test-integration.sh](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/hack/make-rules/test-integration.sh)\r\n- [[etcd 的集成测试]]","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Kubernetes-%E9%9B%86%E7%BE%A4%E7%9A%84%E9%99%8D%E6%9C%AC%E5%A2%9E%E6%95%88":{"title":"Kubernetes 集群的降本增效","content":"通过 [[HPA]] 和 [[ClusterAutoscaler]] [[降本增效]]吧\r\n\r\n[[离线混部]]也是业界主要探索的，有不少[[开源项目]]\r\n\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/LSMs":{"title":"LSMs","content":"基于日志结构的合并树\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/LVM-%E5%B1%9E%E6%80%A7%E5%90%AB%E4%B9%89":{"title":"","content":"http://blog.itpub.net/611609/viewspace-692589/\r\n\r\n摘抄部分 lvs  命令显示字段\r\n\u003e lv_attr      Attr         逻辑卷状态。逻辑卷属性字节如下：  \r\n\t\t   字节 1：卷类型：镜像（m）、不带初始同步的镜像（M ）、源  \r\n\t\t\t（o）、pvmove（p）、快照（s）、不可用快照（S）、虚拟(v）  \r\n\t\t   字节 2：权限：可写入（w），只读（r）  \r\n\t\t   字节 3：分配策略：持续（c）、正常（n）、任意（a）、继承  \r\n\t\t\t（i）。如果在修改分配时锁定了卷，就会显示成大写，比如在执行  \r\n\t\t\tpvmove 命令的时候。  \r\n\t\t   字节 4：固定的副号码（m）  \r\n\t\t   字节 5：激活（a）、暂停的（s）、不可用快照（I）、不可用暂停 快照（S）、不带表格的映射设备（d）、带未激活表格的映射设备（i）  \r\n\t\t   字节 6：设备开放（o）","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/LVM-%E5%BC%BA%E5%88%B6%E4%BF%AE%E6%94%B9%E5%85%83%E4%BF%A1%E6%81%AF":{"title":"","content":"Refer： https://listman.redhat.com/archives/linux-lvm/2012-October/msg00030.html\r\n\r\n```bash\r\nvgcfgbackup -f  vg.bak   vgname\r\n\r\nedit vg.bak and remove all thinp related volumes\r\n\r\nvgcfgrestore -f vg.bak  vgname\r\n```\r\n三步中，第二步要酌情删除目标项。","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/LVM-%E6%8F%90%E7%A4%BA%E4%BF%A1%E6%81%AF%E9%97%AE%E9%A2%98":{"title":"LVM 提示信息问题","content":"## 摘要\r\n`export LVM_SUPPRESS_FD_WARNINGS=1` 解决\r\n\r\n## 主要笔记\r\n- 每次执行 [[lvm]] 相关命令时都会弹出大量的报错信息，包含文件描述符泄露的信息：File descriptor 20 leaked on lvs invocation. Parent PID 1239361\r\n-  https://unix.stackexchange.com/questions/4931/leaking-file-descriptors","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/LVM-%E9%87%8D%E5%90%AF%E9%9C%80%E8%A6%81%E9%87%8D%E6%96%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E5%90%A6%E5%88%99%E4%B8%A2%E5%A4%B1-vg-%E8%AE%BE%E5%A4%87%E7%9A%84%E9%97%AE%E9%A2%98":{"title":"LVM 重启需要重新初始化否则丢失 vg 设备的问题","content":"https://www.aboutyun.com/thread-16065-1-1.html\r\n\r\n设置  开机挂载 ，ok了  \r\n  \r\ncat /etc/rc.d/rc.local |grep cinder-volumes || echo 'losetup -f /var/lib/cinder/cinder-volumes \u0026\u0026 vgchange -a y cinder-volumes ' \u003e\u003e /etc/rc.d/rc.local\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Megatron-LM":{"title":"Megatron-LM","content":"Intro::: Megatron是NVIDIA应用深度学习研究团队研发的大规模Transformer语言模型训练框架，支持模型并行（张量并行、序列并行与流水并行）与多节点预训练（multi-node pre-training），目前已支持BERT、GPT和T5模型。\r\n#github\r\nLink::: https://github.com/NVIDIA/Megatron-LM","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/NVMe":{"title":"","content":"Non-Volatile Memory Host Controller Interface Specification，非易失性[[存储]]主机控制器接口标准\r\n\r\n- 为什么 NVMe 更快 #query \r\n\t- [[PCIe]]：总线带来更大带宽和更低延迟\r\n\t- 并行：传统的[[SATA]]设备只能支持一个队列，一次只能接收32条数据；而NVMe存储则支持最多64000个队列，每个队列有64000个条目。\r\n\t- [[SATA]] 协议理论最大传输速度 6.0Gbps","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/OLTP":{"title":"OLTP","content":"## OLTP 的定义\r\n\r\nOLTP 或联机事务处理是一种数据处理类型，包括执行多个并发的事务，例如网上银行、购物、订单输入或发送文本消息。这些事务传统上被称为经济或财务事务，会被记录并加以保护，帮助企业随时访问这些信息，以用于会计或报告目的。\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n- [[OLTP 和 OLAP 的比较]] \r\n\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/OLTP-%E5%92%8C-OLAP-%E7%9A%84%E6%AF%94%E8%BE%83":{"title":"OLTP 和 OLAP 的比较","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n- **[[ACID]] 合规：** OLTP 系统必须确保正确记录整个事务处理。事务处理通常涉及执行多个步骤或操作的程序执行。它可能在所有相关方确认事务、交付产品/服务，或对数据库中的特定表进行一定数量的更新时完成。仅当执行并记录了涉及的所有步骤时，事务处理才算是获得了正确的记录。如果任何一个步骤有任何错误，则必须中止整个事务处理，并且从系统中删除所有步骤。因此，OLTP 系统必须符合[[原子性]]、一致性、隔离性和持久性 (ACID) ，以确保系统中数据的准确性。\r\n    -   [[原子性]]：原子性控制确保事务处理中的所有步骤作为一个组成功完成。也就是说，如果事务处理之间的任何步骤失败，则所有其他步骤也必须失败或回滚。事务处理的成功完成称为提交。事务处理的失败称为中止。\r\n    -   一致性：事务处理保留数据库的内部一致性。如果您在最初一致的数据库中单独执行事务处理，则当事务处理完成执行时，数据库再次保持一致。\r\n    -   隔离性：事务处理的执行方式就像它单独运行一样，没有涉及其他事务处理。也就是说，运行一组事务处理的效果与一次运行一项事务处理的效果相同。这种行为称为可串行化，通常通过锁定表中的特定行来实现。\r\n    -   持久：事务处理的结果不会因失败而丢失。\r\n-   **并发性：** OLTP 系统的用户群可以非常庞大，许多用户尝试同时访问相同的数据。系统必须确保尝试读取或写入系统的所有用户都可以同时执行此操作。并发控制确保两个用户同时访问数据库系统中的相同数据将无法更改该数据，或者一个用户必须等到另一个用户完成处理后才能更改数据。\r\n-   **扩展性：** OLTP 系统必须能够即时扩展和缩小，以实时管理事务处理量并同时执行事务处理，而与尝试访问系统的用户数量无关。\r\n-   **可用性：** OLTP 系统必须始终可用并随时准备接受事务处理。事务处理失败可能导致收入损失或产生法律影响。事务处理可以在全球任何地方随时执行，因此系统必须全天候可用。\r\n-   **高吞吐量并缩短响应时间：** OLTP 系统需要纳秒甚至更短的响应时间，来提高企业用户的工作效率并满足客户不断增长的期望。\r\n-   **可靠性：** OLTP 系统通常读取和操作高度选择性的少量数据。更重要的是，在任何指定时间点，数据库中的数据对于访问该数据的用户和应用程序来说都是可靠且值得信赖的。\r\n-   **安全性：** 由于这些系统存储高度敏感的客户事务处理数据，因此数据安全性至关重要。任何违规行为对公司来说都需要付出非常昂贵的代价。\r\n-   **可恢复性：** OLTP 系统必须在发生任何硬件或软件故障时具有恢复能力。\r\n\r\n| [[OLTP]] 系统                              | [[OLAP]] 系统                                                    |\r\n| -------------------------------------- | ------------------------------------------------------------ |\r\n| 需要闪电般的快速回应时间               | 要求的响应时间比 [[OLTP]] 慢几个数量级                           |\r\n| 频繁修改少量数据，通常涉及读写平衡     | 完全不修改数据；工作负载通常是读取密集型的                   |\r\n| 使用[[索引]]数据来缩短响应时间             | 以[[列格式存储]]数据，以便轻松访问大量记录                       |\r\n| 需要频繁或并发的数据库[[备份]]             | 不需要太频繁的数据库备份                                     |\r\n| 需要相对较少的存储空间                 | 通常有大量存储空间，因为它们存储了大量历史数据               |\r\n| 通常运行仅涉及一个或多个记录的简单查询 | 运行涉及大量记录的复杂查询                                   |\r\n| 支持大量人员实时执行大量[[数据库事务]]     | 通常涉及查询数据库中的许多记录，甚至所有记录，以用于分析目的 |\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/OR-Tools":{"title":"OR-Tools","content":"## 摘要\r\n[[Google]] 贡献的， [[OR-Tools]]是用于组合优化的开源软件，该软件旨在从一系列可能的解决方案中找到解决问题的最佳解决方案。这里有一些问题解决的示例：\r\n- 车辆路线：寻找限制的车辆舰队的最佳路线（例如，“这辆卡车不能容纳超过20,000磅”或“所有交货必须在两个小时的窗口中进行”）。\r\n- 调度：找到一组复杂任务的最佳时间表，其中一些需要在其他机器或其他资源面前执行。\r\n- 垃圾箱包装：将尽可能多的各种尺寸的物体包装成具有最大能力的固定数量的垃圾箱。\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  [About OR-Tools  |  Google Developers](https://developers.google.com/optimization/introduction/overview)\r\n\r\n\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/OSS-Fuzz":{"title":"OSS-Fuzz","content":"## 摘要\r\nOSS-Fuzz 是谷歌为关键的开源项目提供的一项免费服务，用于持续运行它们的 fuzzing 并报告任何崩溃。\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/OneFlow":{"title":"OneFlow","content":"#github\r\nIntro:::\r\n-   兼容PyTorch的API对模型进行编程\r\n-   运用全局视角（Global View）API将模型扩展至n维并行执行或分布式执行\r\n-   用静态图编译器（Static Graph Compiler）进行模型加速/部署\r\nLink::: https://github.com/Oneflow-Inc/oneflow （论文：\r\nhttps://arxiv.org/abs/2110.15032 ）","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Opensergo":{"title":"Opensergo","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  这可能就是行业的细分标准之微服务治理层！[[行业的细分标准-基础设施层]]","lastmodified":"2022-12-15T12:39:01.344929431Z","tags":null},"/Paxos":{"title":"Paxos","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  [[paxos 抽象理解]]\r\n- [Is Raft more modular than MultiPaxos? | mahesh’s blog](https://maheshba.bitbucket.io/blog/2021/12/14/Modularity.html)\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/PriceOps":{"title":"PriceOps","content":"什么是 PriceOps ?\r\n解释网址： https://priceops.org/\r\n\r\n做一下笔记：\r\n\r\n\tPriceOps 部分定义\r\n\t\r\n\tPriceOps 是一种实现迭代和灵活性的方法。它描述了一组通过有效管理固有复杂性来促进定价模型探索的实现属性。将此视为一组架构蓝图和最佳实践，可以帮助您持续开发和完善您的定价基础架构。\r\n\t\r\n\tPriceOps_不是_关于任何特定产品应如何定价或如何确定此类价格的规定性指南。相反，它是如何实施定价模型以最大限度地提高灵活性和稳定性的指南。\r\n\r\n我理解如下：\r\n\r\n将定价模型通过代码描述出来，现在是 json，因为有 code 极强的表达能力，所以可能可以从中看出定价对于产品的影响有哪些。从而做出调整。\r\n\r\n列出了支撑模型的五个支柱（英文：[[pillar]])\r\n\r\n1. 定价模型的定义：例如版本化代表不会影响之前的用户\r\n2. 用户的时间表：这样一个定价计划就可以规定在哪些时间里面可以使用哪些功能，并能有多少使用量\r\n3. 计量系统：用于收集所有用户使用信息，一个数据中心中存储，来帮助定价模型的更新\r\n4. 权限检查：这样应用程序代码只需要提供功能，不需要知道功能在哪些计划中。\r\n5. PriceOps 工具：为以上行为提供操作的工具\r\n\r\n我很喜欢这种 [[逐步描述一个新模型的方式]]，它还能持续迭代一个东西定义，如果一开始就下结论，这个模型就老死了。","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/Pricing-as-Code":{"title":"Pricing as Code","content":"一种很新的东西.\r\n概念模型: [[PriceOps]]\r\n示例产品：[[Tier]]  https://www.tier.run/","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/Quorum-%E6%9C%BA%E5%88%B6":{"title":"Quorum 机制","content":"## 摘要\r\n假设有 N 个副本，更新操作 w 在 W 个副本中更新成功之后，才认为此次更新操作w 成功。称成功提交的更新操作对应的数据为：“成功提交的数据”。对于读操作而言，至少需要读R个副本才能读到此次更新的数据。**其中，W+R\u003eN ，即 W 和 R 有重叠。** 一般而言，W+R=N+1\r\n比如 [[Raft]] 常见的三节点，W 和 R 都等于 2， W+R=4 \u003e N=3\r\n## 问题、提示\r\n-  如何避免 [[Split Brain]] 问题\r\n\r\n## 主要笔记\r\n-  Quorum 机制是“[[抽屉原理]]”的一个应用。\r\n- \r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/Request-For-Comments":{"title":"Request For Comments","content":"## 摘要\r\nRFC 不能修改，有可能是错的，他从相当意义上构成了互联网的历史。标准并不总是对的。\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n- RFC由一系列草案组成，起始于1969年（第一个RFC文档发布于1969年4月7日，参见“RFC30年”，RFC2555”）\r\n- “RFC编辑者”：约翰•普斯特尔（Jon Postel）（前 30 年）现在是一个小组\r\n- 从哪里开始阅读 RFC #query \r\n\t- 查找RFC的规范位置是[RFC编辑器网站](https://rfc-editor.org/)。但是，正如我们将在下面看到的那样，RFC编辑器缺少一些关键信息，因此大多数人都使用 [tools.ietf.org](https://tools.ietf.org/)\r\n- [IETF介绍及RFC Draft撰写 | Louie's Blog](http://ylong.net.cn/How_to_write_RFC_draft.html)","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/SaaS":{"title":"","content":"2018 年吧，我还记得当时讲解的现场，经过一系列的推演和被教学，当时决定 SaaS 作为未来 5 年的研究方向， 现在回首，确实通过 SaaS 进入了云计算的领域，做到了 [[IaaS]] 层去啦。也对基础设施有更深的领悟。对于 IT 和互联网企业和物联网企业，一个扎实的基础设施，灵活扩展的[[规范标准]]，行业的深耕领域知识的结合，这些基础设施的构建将会是未来一个阶段的主旋律。当[[行业的细分标准-基础设施层|行业细分标准]]制定完成后，类似 FireStore 的 SaaS 将会较容易的做出来了。说不定这时候，反而阴差阳错的实现了当初的目标。","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/Sophisticated-zh":{"title":"","content":"\r\n# Sophisticated\r\n\r\n## 发音\r\n\r\n- səˈfɪstɪkeɪtɪd\r\n- səˈfɪstɪˌketɪd\r\n\r\n## 词义\r\n\r\n### 能被表示程度的副词或介词词组修饰的形容词\r\n\r\n(机器、装置等)高级的，精密的;(方法)复杂的\r\n\r\n\u003e 蜜蜂之间所用的交流方式是昆虫中最为复杂的方式之一。\r\n\r\n### 能被表示程度的副词或介词词组修饰的形容词\r\n\r\n善于社交的;高雅时髦的;见过世面的\r\n\r\n\u003e 克劳德是一个很有魅力、见多识广的伙伴。\r\n\r\n### 能被表示程度的副词或介词词组修饰的形容词\r\n\r\n精明老练的;老于世故的\r\n\r\n\u003e 这些人是观察外交政策领域动向的行家里手。\r\n\r\n\r\n\r\n## \r\n\r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/Split-Brain":{"title":"Split Brain","content":"## 摘要\r\nSplit Brain 是指在同一时刻有两个认为自己处于 Active 状态的 NameNode。\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/StartTestServer":{"title":"StartTestServer","content":"\r\n```go\r\n// StartTestServer runs a kube-apiserver, optionally calling out to the setup.ModifyServerRunOptions and setup.ModifyServerConfig functions\r\n\r\nfunc StartTestServer(t testing.TB, setup TestServerSetup) (client.Interface, *rest.Config, TearDownFunc) {\r\n\r\ncertDir, err := os.MkdirTemp(\"\", \"test-integration-\"+strings.ReplaceAll(t.Name(), \"/\", \"_\"))\r\n\r\nif err != nil {\r\n\r\nt.Fatalf(\"Couldn't create temp dir: %v\", err)\r\n\r\n}\r\n\r\nstopCh := make(chan struct{})\r\n\r\nvar errCh chan error\r\n\r\ntearDownFn := func() {\r\n\r\n// Closing stopCh is stopping apiserver and cleaning up\r\n\r\n// after itself, including shutting down its storage layer.\r\n\r\nclose(stopCh)\r\n\r\n// If the apiserver was started, let's wait for it to\r\n\r\n// shutdown clearly.\r\n\r\nif errCh != nil {\r\n\r\nerr, ok := \u003c-errCh\r\n\r\nif ok \u0026\u0026 err != nil {\r\n\r\nt.Error(err)\r\n\r\n}\r\n\r\n}\r\n\r\nif err := os.RemoveAll(certDir); err != nil {\r\n\r\nt.Log(err)\r\n\r\n}\r\n\r\n}\r\n\r\n_, defaultServiceClusterIPRange, _ := netutils.ParseCIDRSloppy(\"10.0.0.0/24\")\r\n\r\nproxySigningKey, err := utils.NewPrivateKey()\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nproxySigningCert, err := cert.NewSelfSignedCACert(cert.Config{CommonName: \"front-proxy-ca\"}, proxySigningKey)\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nproxyCACertFile, _ := os.CreateTemp(certDir, \"proxy-ca.crt\")\r\n\r\nif err := os.WriteFile(proxyCACertFile.Name(), utils.EncodeCertPEM(proxySigningCert), 0644); err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nclientSigningKey, err := utils.NewPrivateKey()\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nclientSigningCert, err := cert.NewSelfSignedCACert(cert.Config{CommonName: \"client-ca\"}, clientSigningKey)\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nclientCACertFile, _ := os.CreateTemp(certDir, \"client-ca.crt\")\r\n\r\nif err := os.WriteFile(clientCACertFile.Name(), utils.EncodeCertPEM(clientSigningCert), 0644); err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nlistener, _, err := genericapiserveroptions.CreateListener(\"tcp\", \"127.0.0.1:0\", net.ListenConfig{})\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nsaSigningKeyFile, err := os.CreateTemp(\"/tmp\", \"insecure_test_key\")\r\n\r\nif err != nil {\r\n\r\nt.Fatalf(\"create temp file failed: %v\", err)\r\n\r\n}\r\n\r\ndefer os.RemoveAll(saSigningKeyFile.Name())\r\n\r\nif err = os.WriteFile(saSigningKeyFile.Name(), []byte(ecdsaPrivateKey), 0666); err != nil {\r\n\r\nt.Fatalf(\"write file %s failed: %v\", saSigningKeyFile.Name(), err)\r\n\r\n}\r\n\r\nkubeAPIServerOptions := options.NewServerRunOptions()\r\n\r\nkubeAPIServerOptions.SecureServing.Listener = listener\r\n\r\nkubeAPIServerOptions.SecureServing.BindAddress = netutils.ParseIPSloppy(\"127.0.0.1\")\r\n\r\nkubeAPIServerOptions.SecureServing.ServerCert.CertDirectory = certDir\r\n\r\nkubeAPIServerOptions.ServiceAccountSigningKeyFile = saSigningKeyFile.Name()\r\n\r\nkubeAPIServerOptions.Etcd.StorageConfig.Prefix = path.Join(\"/\", uuid.New().String(), \"registry\")\r\n\r\nkubeAPIServerOptions.Etcd.StorageConfig.Transport.ServerList = []string{GetEtcdURL()}\r\n\r\nkubeAPIServerOptions.ServiceClusterIPRanges = defaultServiceClusterIPRange.String()\r\n\r\nkubeAPIServerOptions.Authentication.RequestHeader.UsernameHeaders = []string{\"X-Remote-User\"}\r\n\r\nkubeAPIServerOptions.Authentication.RequestHeader.GroupHeaders = []string{\"X-Remote-Group\"}\r\n\r\nkubeAPIServerOptions.Authentication.RequestHeader.ExtraHeaderPrefixes = []string{\"X-Remote-Extra-\"}\r\n\r\nkubeAPIServerOptions.Authentication.RequestHeader.AllowedNames = []string{\"kube-aggregator\"}\r\n\r\nkubeAPIServerOptions.Authentication.RequestHeader.ClientCAFile = proxyCACertFile.Name()\r\n\r\nkubeAPIServerOptions.Authentication.APIAudiences = []string{\"https://foo.bar.example.com\"}\r\n\r\nkubeAPIServerOptions.Authentication.ServiceAccounts.Issuers = []string{\"https://foo.bar.example.com\"}\r\n\r\nkubeAPIServerOptions.Authentication.ServiceAccounts.KeyFiles = []string{saSigningKeyFile.Name()}\r\n\r\nkubeAPIServerOptions.Authentication.ClientCert.ClientCA = clientCACertFile.Name()\r\n\r\nkubeAPIServerOptions.Authorization.Modes = []string{\"Node\", \"RBAC\"}\r\n\r\nif setup.ModifyServerRunOptions != nil {\r\n\r\nsetup.ModifyServerRunOptions(kubeAPIServerOptions)\r\n\r\n}\r\n\r\ncompletedOptions, err := app.Complete(kubeAPIServerOptions)\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nif errs := completedOptions.Validate(); len(errs) != 0 {\r\n\r\nt.Fatalf(\"failed to validate ServerRunOptions: %v\", utilerrors.NewAggregate(errs))\r\n\r\n}\r\n\r\nkubeAPIServerConfig, _, _, err := app.CreateKubeAPIServerConfig(completedOptions)\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nif setup.ModifyServerConfig != nil {\r\n\r\nsetup.ModifyServerConfig(kubeAPIServerConfig)\r\n\r\n}\r\n\r\nkubeAPIServer, err := app.CreateKubeAPIServer(kubeAPIServerConfig, genericapiserver.NewEmptyDelegate())\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nerrCh = make(chan error)\r\n\r\ngo func() {\r\n\r\ndefer close(errCh)\r\n\r\nif err := kubeAPIServer.GenericAPIServer.PrepareRun().Run(stopCh); err != nil {\r\n\r\nerrCh \u003c- err\r\n\r\n}\r\n\r\n}()\r\n\r\n// Adjust the loopback config for external use (external server name and CA)\r\n\r\nkubeAPIServerClientConfig := rest.CopyConfig(kubeAPIServerConfig.GenericConfig.LoopbackClientConfig)\r\n\r\nkubeAPIServerClientConfig.CAFile = path.Join(certDir, \"apiserver.crt\")\r\n\r\nkubeAPIServerClientConfig.CAData = nil\r\n\r\nkubeAPIServerClientConfig.ServerName = \"\"\r\n\r\n// wait for health\r\n\r\nerr = wait.PollImmediate(100*time.Millisecond, 10*time.Second, func() (done bool, err error) {\r\n\r\nselect {\r\n\r\ncase err := \u003c-errCh:\r\n\r\nreturn false, err\r\n\r\ndefault:\r\n\r\n}\r\n\r\nhealthzConfig := rest.CopyConfig(kubeAPIServerClientConfig)\r\n\r\nhealthzConfig.ContentType = \"\"\r\n\r\nhealthzConfig.AcceptContentTypes = \"\"\r\n\r\nkubeClient, err := client.NewForConfig(healthzConfig)\r\n\r\nif err != nil {\r\n\r\n// this happens because we race the API server start\r\n\r\nt.Log(err)\r\n\r\nreturn false, nil\r\n\r\n}\r\n\r\nhealthStatus := 0\r\n\r\nkubeClient.Discovery().RESTClient().Get().AbsPath(\"/healthz\").Do(context.TODO()).StatusCode(\u0026healthStatus)\r\n\r\nif healthStatus != http.StatusOK {\r\n\r\nreturn false, nil\r\n\r\n}\r\n\r\nif _, err := kubeClient.CoreV1().Namespaces().Get(context.TODO(), \"default\", metav1.GetOptions{}); err != nil {\r\n\r\nreturn false, nil\r\n\r\n}\r\n\r\nif _, err := kubeClient.CoreV1().Namespaces().Get(context.TODO(), \"kube-system\", metav1.GetOptions{}); err != nil {\r\n\r\nreturn false, nil\r\n\r\n}\r\n\r\nreturn true, nil\r\n\r\n})\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nkubeAPIServerClient, err := client.NewForConfig(kubeAPIServerClientConfig)\r\n\r\nif err != nil {\r\n\r\nt.Fatal(err)\r\n\r\n}\r\n\r\nreturn kubeAPIServerClient, kubeAPIServerClientConfig, tearDownFn\r\n\r\n}\r\n```","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/Terraform-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%A6%BB%E7%BA%BF%E6%BA%90":{"title":"","content":"\r\n### 1. 创建配置文件\r\n\r\n`.terraformrc`是 [[Terraform]] CLI的配置文件\r\n\r\n```\r\nplugin_cache_dir  = \"/root/.terraform.d/terraform-plugin-cache\" \r\ndisable_checkpoint = true\r\nprovider_installation {\r\n  filesystem_mirror {\r\n    path    = \"/root/.terraform.d/terraform-plugin-cache\"\r\n    include = [\"registry.terraform.io/*/*\"]\r\n  }\r\n}\r\n```\r\n\r\n-   plugin_cache_dir 是插件的缓存目录（此目录需要提前创建不然init报错）\r\n-   disable_checkpoint 禁用 需要连接HashiCorp 提供的网络服务的升级和安全公告检查\r\n\r\n\r\n### 2. 进行初始化\r\n\r\n插件下载方式有两种：\r\n\r\n1.  通过 `terraform init` 自动下载 provider 插件；\r\n2.  登入`registry.terraform.io`手动到 `GitHub`下载，并按照目录结构存放到`plugin_cache_dir`;\r\n\r\n```bash\r\n❯ tree /root/.terraform.d\r\n/root/.terraform.d\r\n├── checkpoint_signature\r\n└── terraform-plugin-cache\r\n    └── registry.terraform.io\r\n        ├── coder\r\n        │   └── coder\r\n        │       └── 0.5.0\r\n        │           └── linux_amd64\r\n        │               └── terraform-provider-coder_v0.5.0\r\n        └── kreuzwerker\r\n            └── docker\r\n                └── 2.20.2\r\n                    └── linux_amd64\r\n                        └── terraform-provider-docker_v2.20.2\r\n```\r\n\r\n然后运行 `terraform init` 即可\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/Tier":{"title":"Tier","content":"https://www.tier.run/\r\n理念： [[Pricing as Code]] 、[[PriceOps]] ","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/WARNING-PV-in-VG-vgXX-is-using-an-old-PV-header-modify-the-VG-to-update":{"title":"WARNING PV in VG vgXX is using an old PV header, modify the VG to update","content":"## 摘要\r\n运行一下命令即可\r\n```\r\nvgck --updatemetadata vg_test\r\n```\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  [WARNING: PV /dev/sdX in VG vgXX is using an old PV header, modify the VG to update - Red Hat Customer Portal](https://access.redhat.com/solutions/5906681)\r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/WOR":{"title":"WOR","content":"## 摘要\r\n[[Paxos]] 是一种实现共识的协议。这意味着它实现了一个称为一次写入寄存器（**[[WOR|Write-Once Register]]**）（或 [[WOR]]）的逻辑对象。WOR 有一个简单的 API：您可以写入一次；你可以从中多次阅读。\r\n\r\n```java\r\nclass WOR{\r\n\tpublic:\r\n\t\t//success means some write succeeded;\r\n\t\t//read after a write to see what was written.\r\n\t\tvoid write(std::string payload);\r\n\t\t//throw an exception if unwritten\r\n\t\tstd::string read();\r\n}\r\n```\r\n推导过程：\r\n[[paxos 抽象理解]]\r\n\r\n最终的 WOR API：\r\n\r\n```java\r\nclass WOR{\r\n\tpublic:\r\n\t\t//lock a quorum\r\n\t\tint lock();\r\n\t\t//success means some write succeeded;\r\n\t\t//read after a write to see what was written.\r\n\t\t//throws an exception if you lost the lock.\r\n\t\tvoid write(std::string payload, int lockId);\r\n\t\t//throw an exception if unwritten\r\n\t\tstd::string read();\r\n}\r\n```\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  From [[paxos 抽象理解]]\r\n- 一个 WOR 的实现必须表现得像一个很大的胖锁（不管它实际上是如何实现的）。\r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/WebAssembly-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BE%88%E5%BF%AB":{"title":"WebAssembly 为什么很快","content":"## 摘要\r\n相比于正常程序，解析-编译 + 优化-再优化（Re-optimizing）-执行-垃圾回收。字节码程序的环境是解码-编译+优化-执行，少了环节，并且同一环节更高效\r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/anaconda-%E5%AE%89%E8%A3%85":{"title":"anaconda 安装","content":"\r\n下载地址： [https://www.anaconda.com/products/distribution](https://www.anaconda.com/products/distribution)\r\n\r\n  \r\n\r\n以 linux 为例，下载的是一个 .sh 的文件，通过 bash 运行他即可\r\n\r\n`bash Anaconda3-2020.11-Linux-x86_64.sh`\r\n\r\n会运行一段交互式脚本，确认安装配置。\r\n\r\n### 内网环境配置\r\n\r\n`conda config --add channels [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)`\r\n\r\n查看添加的镜像：\r\n\r\n`conda config --get channels`\r\n\r\n推荐使用搜到的 `.condarc`直接复制粘贴\r\n\r\n```bash\r\nchannels:\r\n- defaults\r\nshow_channel_urls: true\r\nchannel_alias: http://xxx.com/nexus/repository/anaconda\r\ndefault_channels:\r\n- http://xxx.com/nexus/repository/anaconda/pkgs/main\r\n- http://xxx.com/nexus/repository/anaconda/pkgs/free\r\n- http://xxx.com/nexus/repository/anaconda/pkgs/r\r\n- http://xxx.com/nexus/repository/anaconda/pkgs/pro\r\n- http://xxx.com/nexus/repository/anaconda/pkgs/msys2\r\ncustom_channels:\r\nconda-forge: http://xxx.com/nexus/repository/anaconda\r\nmsys2: http://xxx.com/nexus/repository/anaconda\r\nbioconda: http://xxx.com/nexus/repository/anaconda\r\nmenpo: http://xxx.com/nexus/repository/anaconda\r\npytorch: http://xxx.com/nexus/repository/anaconda\r\nsimpleitk: http://xxx.com/nexus/repository/anaconda\r\nauto_activate_base: false #用于关闭自动启用 base 环境\r\n```\r\n\r\n  \r\n\r\n`pip config set global.index-url [https://pypi.tuna.tsinghua.edu.cn/simple](https://pypi.tuna.tsinghua.edu.cn/simple)`\r\n\r\n### 环境创建\r\n\r\nfrom： https://www.cnblogs.com/xiaojianliu/p/13466666.html\r\n\r\n在Anaconda中conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。所以对虚拟环境进行创建、删除等操作需要使用conda命令。\r\n```bash\r\nconda 本地环境常用操作\r\n#获取版本号\r\nconda --version 或 conda -V\r\n\r\n#检查更新当前conda\r\nconda update conda\r\n\r\n#查看当前存在哪些虚拟环境\r\nconda env list 或 conda info -e\r\n\r\n#查看--安装--更新--删除包\r\n\r\nconda list：\r\nconda search package_name# 查询包\r\nconda install package_name\r\nconda install package_name=1.5.0\r\nconda update package_name\r\nconda remove package_name\r\nconda创建虚拟环境\r\n#创建名为your_env_name的环境\r\nconda create --name your_env_name\r\n#创建制定python版本的环境\r\nconda create --name your_env_name python=2.7\r\nconda create --name your_env_name python=3.6\r\n#创建包含某些包（如numpy，scipy）的环境\r\nconda create --name your_env_name numpy scipy\r\n#创建指定python版本下包含某些包的环境\r\nconda create --name your_env_name python=3.6 numpy scipy\r\n激活虚拟环境\r\n#Linux\r\nsource activate your_env_name\r\n\r\n#Windows\r\nactivate your_env_name\r\n退出虚拟环境\r\n#Linux\r\nsource deactivate your_env_name\r\n\r\n#Windows\r\ndeactivate env_name\r\n删除虚拟环境\r\nconda remove -n your_env_name --all\r\nconda remove --name your_env_name --all\r\n复制某个环境\r\nconda create --name new_env_name --clone old_env_name\r\n在指定环境中管理包\r\nconda list -n your_env_name\r\nconda install --name myenv package_name \r\nconda remove --name myenv package_name\r\n使用国内 conda 软件源加速\r\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\r\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\r\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\r\nconda config --set show_channel_urls yes\r\n```\r\n\r\n`conda create --name projector python=3.9` 创建一个 3.9 的环境\r\n\r\n安装包：\r\n\r\n`conda install matplotlib numpy jupyterlab`\r\n\r\n启动 jupyter lab\r\n\r\n`jupyter lab --ip 0.0.0.0 --port 8888`\r\n\r\n### jupyter 代码提示\r\n\r\nlab 本来就带自动补全的。按 `tab`键就可以。\r\n\r\nlsp\r\n\r\n[https://github.com/jupyter-lsp/jupyterlab-lsp](https://github.com/jupyter-lsp/jupyterlab-lsp)\r\n\r\npip install 'jupyterlab\u003e=3.0.0,\u003c4.0.0a0' jupyterlab-lsp\r\npip install 'python-lsp-server[all]'\r\n\r\n注意在侧边栏 extension manager 中启用安装的 extension。也可以手动在侧边栏搜索 lsp 安装而不用 pip 安装然后重启 jupyter。\r\n\r\n## 离线安装\r\n\r\n有时候需要离线\r\n\r\n[Python pip离线安装package方法总结（以TensorFlow为例）](https://imshuai.com/python-pip-install-package-offline-tensorflow)\r\n\r\n1.  pip download tensorflow\r\n2.  将目录内容拷贝到目标offline机器（比如/offline_package_dir），并目标offline机器执行pip install --no-index --find-links=file:/offline_package_dir tensorflow","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/blogs/Configmap-Secret-Manager":{"title":"Configmap/Secret Manager","content":"## Configmap/Secret Manager\r\n\r\n\u003ca name=\"YjhpG\"\u003e\u003c/a\u003e\r\n## ReadLink\r\n\r\n- [configmap manager](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/configmap/configmap_manager.go)\r\n- [pkg/kubelet/secret/secret_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/secret/secret_manager.go)\r\n\u003ca name=\"GuGtv\"\u003e\u003c/a\u003e\r\n\r\n## Configmap Manager\r\n\r\n```go\r\n// Manager interface provides methods for Kubelet to manage ConfigMap.\r\ntype Manager interface {\r\n    // Get configmap by configmap namespace and name.\r\n    GetConfigMap(namespace, name string) (*v1.ConfigMap, error)\r\n    \r\n    // WARNING: Register/UnregisterPod functions should be efficient,\r\n    // i.e. should not block on network operations.\r\n    \r\n    // RegisterPod registers all configmaps from a given pod.\r\n    RegisterPod(pod *v1.Pod)\r\n    \r\n    // UnregisterPod unregisters configmaps from a given pod that are not\r\n    // used by any other registered pod.\r\n    UnregisterPod(pod *v1.Pod)\r\n}\r\n```\r\n\r\n接口非常简单。\r\n\r\n1. GetConfigMap ： 通过 [[namespace]] 和 name 获取对应 [[ConfigMap]] 对象。\r\n1. RegisterPod(pod *v1.Pod)：把指定 Pod 对象 yaml 指定的 configmap 注册到 Controller 中管理\r\n1. UnregisterPod(pod *v1.Pod)：把指定 Pod 对象 yaml 指定的 configmap 从 Controller 中注册管理中删除，注意 ConfigMap 需要没有任何其他已注册的 Pod 引用（即无被依赖项）才可以删除\r\n\r\n当前代码中有两种 manager 的实现\r\n\r\n-`NewCachingConfigMapManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager`：该实现有两点逻辑\r\n   - 当一个 Pod 创建或者更新时，所有的 configmap 缓存都失效。\r\n   -  GetObject() 调用首先从本地缓存获取，失败则访问 APISever 并刷新 configmap 的缓存。\r\n\r\n```go\r\n// NewCachingConfigMapManager creates a manager that keeps a cache of all configmaps\r\n// necessary for registered pods.\r\n// It implement the following logic:\r\n// - whenever a pod is create or updated, the cached versions of all configmaps\r\n//   are invalidated\r\n// - every GetObject() call tries to fetch the value from local cache; if it is\r\n//   not there, invalidated or too old, we fetch it from apiserver and refresh the\r\n//   value in cache; otherwise it is just fetched from cache\r\nfunc NewCachingConfigMapManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager {\r\n\tgetConfigMap := func(namespace, name string, opts metav1.GetOptions) (runtime.Object, error) {\r\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).Get(context.TODO(), name, opts)\r\n\t}\r\n\tconfigMapStore := manager.NewObjectStore(getConfigMap, clock.RealClock{}, getTTL, defaultTTL)\r\n\treturn \u0026configMapManager{\r\n\t\tmanager: manager.NewCacheBasedManager(configMapStore, getConfigMapNames),\r\n\t}\r\n}\r\n```\r\n\r\n- `NewWatchingConfigMapManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager`：\r\n   - 当一个 Pod 创建或者更新时，会对指定该 Pod 引用的资源，并且该资源未被其他 Pod 引用进行独立的 watch。\r\n   - GetObject() 调用首先从本地缓存获取\r\n\r\n```go\r\n// NewWatchingConfigMapManager creates a manager that keeps a cache of all configmaps\r\n// necessary for registered pods.\r\n// It implements the following logic:\r\n// - whenever a pod is created or updated, we start individual watches for all\r\n//   referenced objects that aren't referenced from other registered pods\r\n// - every GetObject() returns a value from local cache propagated via watches\r\nfunc NewWatchingConfigMapManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager {\r\n\tlistConfigMap := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) {\r\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).List(context.TODO(), opts)\r\n\t}\r\n\twatchConfigMap := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) {\r\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).Watch(context.TODO(), opts)\r\n\t}\r\n\tnewConfigMap := func() runtime.Object {\r\n\t\treturn \u0026v1.ConfigMap{}\r\n\t}\r\n\tisImmutable := func(object runtime.Object) bool {\r\n\t\tif configMap, ok := object.(*v1.ConfigMap); ok {\r\n\t\t\treturn configMap.Immutable != nil \u0026\u0026 *configMap.Immutable\r\n\t\t}\r\n\t\treturn false\r\n\t}\r\n\tgr := corev1.Resource(\"configmap\")\r\n\treturn \u0026configMapManager{\r\n\t\tmanager: manager.NewWatchBasedManager(listConfigMap, watchConfigMap, newConfigMap, isImmutable, gr, resyncInterval, getConfigMapNames),\r\n\t}\r\n}\r\n\r\n```\r\n\r\n\u003ca name=\"gEtqm\"\u003e\u003c/a\u003e\r\n## Secret Manager\r\n\r\nsecret manager 除了资源类型和 configmap 不一样，其他逻辑相同，所以仅列出两种 secret manager 的初始化函数。\u003cbr /\u003e[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[secret /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/secret)[secret_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/secret/secret_manager.go)\r\n\r\n```go\r\n// NewCachingSecretManager creates a manager that keeps a cache of all secrets\r\n// necessary for registered pods.\r\n// It implements the following logic:\r\n// - whenever a pod is created or updated, the cached versions of all secrets\r\n//   are invalidated\r\n// - every GetObject() call tries to fetch the value from local cache; if it is\r\n//   not there, invalidated or too old, we fetch it from apiserver and refresh the\r\n//   value in cache; otherwise it is just fetched from cache\r\nfunc NewCachingSecretManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager {\r\n\tgetSecret := func(namespace, name string, opts metav1.GetOptions) (runtime.Object, error) {\r\n\t\treturn kubeClient.CoreV1().Secrets(namespace).Get(context.TODO(), name, opts)\r\n\t}\r\n\tsecretStore := manager.NewObjectStore(getSecret, clock.RealClock{}, getTTL, defaultTTL)\r\n\treturn \u0026secretManager{\r\n\t\tmanager: manager.NewCacheBasedManager(secretStore, getSecretNames),\r\n\t}\r\n}\r\n\r\n// NewWatchingSecretManager creates a manager that keeps a cache of all secrets\r\n// necessary for registered pods.\r\n// It implements the following logic:\r\n// - whenever a pod is created or updated, we start individual watches for all\r\n//   referenced objects that aren't referenced from other registered pods\r\n// - every GetObject() returns a value from local cache propagated via watches\r\nfunc NewWatchingSecretManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager {\r\n\tlistSecret := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) {\r\n\t\treturn kubeClient.CoreV1().Secrets(namespace).List(context.TODO(), opts)\r\n\t}\r\n\twatchSecret := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) {\r\n\t\treturn kubeClient.CoreV1().Secrets(namespace).Watch(context.TODO(), opts)\r\n\t}\r\n\tnewSecret := func() runtime.Object {\r\n\t\treturn \u0026v1.Secret{}\r\n\t}\r\n\tisImmutable := func(object runtime.Object) bool {\r\n\t\tif secret, ok := object.(*v1.Secret); ok {\r\n\t\t\treturn secret.Immutable != nil \u0026\u0026 *secret.Immutable\r\n\t\t}\r\n\t\treturn false\r\n\t}\r\n\tgr := corev1.Resource(\"secret\")\r\n\treturn \u0026secretManager{\r\n\t\tmanager: manager.NewWatchBasedManager(listSecret, watchSecret, newSecret, isImmutable, gr, resyncInterval, getSecretNames),\r\n\t}\r\n}\r\n```\r\n\r\n\u003ca name=\"RX3SN\"\u003e\u003c/a\u003e\r\n## cache_based_manager\r\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[cache_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/cache_based_manager.go)\r\n\r\n```go\r\n// cacheBasedManager keeps a store with objects necessary\r\n// for registered pods. Different implementations of the store\r\n// may result in different semantics for freshness of objects\r\n// (e.g. ttl-based implementation vs watch-based implementation).\r\ntype cacheBasedManager struct {\r\n    objectStore          Store\r\n\tgetReferencedObjects func(*v1.Pod) sets.String\r\n\r\n\tlock           sync.Mutex\r\n\tregisteredPods map[objectKey]*v1.Pod\r\n}\r\n```\r\n\r\n该 manager 代码位于  [/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[cache_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/cache_based_manager.go)，属于通用的 Manager 结构体工具，用于保留注册的  Pod 所必要引用的 kubernetes 对象（objects）\u003cbr /\u003e如何做到的呢？\u003cbr /\u003e通过 getReferencedObjects 字段，一个可以传入的成员函数，自定义实现用于从 v1.Pod 对象中获取到对应对象（或一组对象）的 name。流程如下：\r\n\r\n```go\r\nfunc (c *cacheBasedManager) RegisterPod(pod *v1.Pod) {\r\n    // 1. 获取名字\r\n\tnames := c.getReferencedObjects(pod)\r\n\tc.lock.Lock()\r\n\tdefer c.lock.Unlock()\r\n    // 2. 给每一个名字和 pod 的命名空间一起添加到 c.objectStore 中存储\r\n\tfor name := range names {\r\n\t\tc.objectStore.AddReference(pod.Namespace, name)\r\n\t}\r\n    // 3. 检查是否之前已经注册了该 Pod\r\n\tvar prev *v1.Pod\r\n\tkey := objectKey{namespace: pod.Namespace, name: pod.Name, uid: pod.UID}\r\n\tprev = c.registeredPods[key]\r\n    // 4. 用新注册的 pod 替换之前存储的注册 Pod 的信息.\r\n\tc.registeredPods[key] = pod\r\n    // 5. 删除旧 Pod 在 c.objectStore 中的引用信息,这是因为在上面第二步 Add 到 c.objectStore\r\n    // 中时,这些资源的引用次数又新增了一次,但实际上只是同一个 Pod 的引用,自然需要删除,当然,也有\r\n    // 可能新 Pod 已经不再引用目标资源了, Delete 函数在下面也处理这个情况\r\n\tif prev != nil {\r\n\t\tfor name := range c.getReferencedObjects(prev) {\r\n\t\t\t// On an update, the .Add() call above will have re-incremented the\r\n\t\t\t// ref count of any existing object, so any objects that are in both\r\n\t\t\t// names and prev need to have their ref counts decremented. Any that\r\n\t\t\t// are only in prev need to be completely removed. This unconditional\r\n\t\t\t// call takes care of both cases.\r\n\t\t\tc.objectStore.DeleteReference(prev.Namespace, name)\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\n1. 获取名字\r\n1. 给每一个名字和 pod 的命名空间一起添加到 c.objectStore 中存储\r\n1. 检查是否之前已经注册了该 Pod\r\n1. 用新注册的 pod 替换之前存储的注册 Pod 的信息.\r\n5. 删除旧 Pod 在 c.objectStore 中的引用信息,这是因为在上面第二步 Add 到 c.objectStore 中时,这些资源的引用次数又新增了一次,但实际上只是同一个 Pod 的引用,自然需要删除,当然,也有可能新 Pod 已经不再引用目标资源了, Delete 函数在下面也处理这个情况\r\n\r\n\u003ca name=\"aQgQM\"\u003e\u003c/a\u003e\r\n### ttl ObjectStore\r\n\r\ncache_based 的 objectStore 通过 ttl 设置缓存有效期。\r\n\u003ca name=\"qzaxL\"\u003e\u003c/a\u003e\r\n\r\n## watch_based_manager\r\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[watch_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/watch_based_manager.go)\u003cbr /\u003e可以看到，watch_based_manager 最后使用了 NewCacheBasedManager ，所以 watch_based_manager  和 cache_based_manager 不同的是 ObjectStore 字段。\r\n\r\n\r\n```go\r\n// NewWatchBasedManager creates a manager that keeps a cache of all objects\r\n// necessary for registered pods.\r\n// It implements the following logic:\r\n// - whenever a pod is created or updated, we start individual watches for all\r\n//   referenced objects that aren't referenced from other registered pods\r\n// - every GetObject() returns a value from local cache propagated via watches\r\nfunc NewWatchBasedManager(\r\n\tlistObject listObjectFunc,\r\n\twatchObject watchObjectFunc,\r\n\tnewObject newObjectFunc,\r\n\tisImmutable isImmutableFunc,\r\n\tgroupResource schema.GroupResource,\r\n\tresyncInterval time.Duration,\r\n\tgetReferencedObjects func(*v1.Pod) sets.String) Manager {\r\n\r\n\t// If a configmap/secret is used as a volume, the volumeManager will visit the objectCacheItem every resyncInterval cycle,\r\n\t// We just want to stop the objectCacheItem referenced by environment variables,\r\n\t// So, maxIdleTime is set to an integer multiple of resyncInterval,\r\n\t// We currently set it to 5 times.\r\n\tmaxIdleTime := resyncInterval * 5\r\n\r\n\t// TODO propagate stopCh from the higher level.\r\n\tobjectStore := NewObjectCache(listObject, watchObject, newObject, isImmutable, groupResource, clock.RealClock{}, maxIdleTime, wait.NeverStop)\r\n\treturn NewCacheBasedManager(objectStore, getReferencedObjects)\r\n}\r\n```\r\n\r\nwatch_based_manager  通过 watch 而不是简单的 ttl 去确认或者刷新缓存。\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/blogs/How-Cloud-Develop-Kit-from-Google-designed-the-docstore-interface":{"title":"How Cloud Develop Kit from Google designed the docstore interface","content":"\n## How [[Cloud Develop Kit]] from Google designed the docstore interface\n\n\u003ca name=\"ljgKZ\"\u003e\u003c/a\u003e\n## Refer\n- [Docstore · Go CDK](https://gocloud.dev/howto/docstore/)\n- [urls.go - google/go-cloud - Sourcegraph](https://sourcegraph.com/github.com/google/go-cloud@master/-/blob/docstore/mongodocstore/urls.go)\n- [driver.go - Go](https://cs.opensource.google/go/go/+/refs/tags/go1.18.3:src/database/sql/driver/driver.go)\n\u003ca name=\"Eu7vN\"\u003e\u003c/a\u003e\n\n## Design objectives: \n\n**through the abstraction layer, we can mask differences, provide services in a standardized way, and configure business applications through description files.** \n\n**Provides design ideas and guidelines for applications that use document storage.** \n\n\u003ca name=\"yaqn6\"\u003e\u003c/a\u003e\n## Intro\ncommon in [[MongoDB]] [document Storage](https://en.wikipedia.org/wiki/Document-oriented_database) provides an abstraction layer. \n\nDocument Storage is a service that stores data in semi-structured JSON-like documents. These documents are grouped into collections. Like other NoSQL databases, document storage is modeless. \n\nThe design needs to support adding, retrieving, modifying, and deleting documents. \ndocstore Driver implementation of various services, including cloud and local solutions. You can develop applications locally and then reconfigure them to multiple cloud providers with minimal initialization. \n\u003ca name=\"FCVI2\"\u003e\u003c/a\u003e\n## 设计\n\u003ca name=\"fg6xL\"\u003e\u003c/a\u003e\n### Structuring Portable Code \nStructuring Portable Code the non-interface design imitates the database/SQL package of golang and wraps the existing common logic into the structure. The internal fields of the structure are driver interfaces. The method provided externally is the method corresponding to the structure rather than the implementation of the direction provided driver.\n\n\u003e The advantage of this design is that there is no need to implement general logic processing for each interface, and the code can be transplanted. In some cases, you only need to add and modify methods on the structure and do not need to destroy the method design in the interface. You can also mask some assertion logic. When switching different drivers, users do not need to determine the implementation of some optional interfaces.\n\n[Structuring Portable Code · Go CDK](https://gocloud.dev/concepts/structure/)\n\n[sql package - database/sql - Go Packages](https://pkg.go.dev/database/sql#DB)\n\n![[blogs/Pasted image 20221011180052.png]]\n\n\u003cbr /\u003eCode like below：\n```go\n// Define\ntype DB interface{\n    Exec(sql string)error\n}\n\n// Realize\ntype mysql struct {}\nfunc (m *mysql) Exec(sql string) error {return nil}\n\n// Execute\nsql.Database.Exec(\"\")\n```\n```go\n// package and structure\npackage sql\n\ntype DB struct {\n    driver driver.DB\n}\n\n// higher level logic\nfunc (db *DB) AnySignature(anyParams string) (anyReturn error) {\n    //... \n    db.driver.Exec(\"...\")\n    //...\n    return nil\n}\n// Define\npackage driver \n\ntype DB interface{\n    Exec(sql string)error\n}\n\n// Realize\ntype mysql struct {}\nfunc (m *mysql) Exec(sql string) error {return nil}\n\n// Execute\nsql.DB.AnySignature(\"\")\n```\n\u003ca name=\"xSKiY\"\u003e\u003c/a\u003e\n### Actions List\nFor MongoDB, batch processing can be carried out to improve efficiency. As the shielding layer of packaging, we hope to obtain this benefit according to the actual processing of driver. A queue or cache is required to submit a batch operation.\n\n[Batch write operations-MongoDB-CN-Manual](https://docs.mongoing.com/mongodb-crud-operations/bulk-write-operations)\n\n- [x] I think it is enough to undertake Google Go CDK design \n\n\u003ca name=\"g9Zj6\"\u003e\u003c/a\u003e\n### Driver Map \u0026 Opener\nInherited from the Mysql Driver registration method, through the golang standard import_\" github.com/xxx/driver\" different database drivers can be introduced. The principle is to use a global Map.\n\n[[Golang]] CDK has upgraded the Opener feature. The original custom URL Parsing method is \"mysql\", \"user:password@/dbname\" the features of the new version are blob+file:///dir even \u003capi\u003e+ \u003ctype\u003e+ prefix (e.g. blob+bucket+file:///dir) for Google Cloud SDK, the same URL can provide different functions. However, in our opinion, this function does not have much effect for the time being, so we will block their design. \n\n\u003ca name=\"MrRZi\"\u003e\u003c/a\u003e\n### Dependency Injection wire \nGo CDK use the wire project to inject dependencies to automatically switch the structure of different backend providers to the SDK. Different from the way Dapr accesses different services, Dapr uses the yaml description to determine the different plug-ins that are enabled. \n\nFor example, you need wire.Build() indicates the new function of the driver to be introduced. \n\nIt has little impact on this project and may not be added for the time being. \n\n\u003ca name=\"tUMOU\"\u003e\u003c/a\u003e\n\n### UUID usage\n\nmongoDB, each entry must have a Key, which can be passed through parameters. \n```go\ntype Player struct {\n    ID   interface{} `docstore:\"_id,omitempty\"`\n    Name string\n}\n```\n\nThe simplest is to indicate the_id field directly in the structure. \n\n```go\ndocstore.OpenCollection(context.Background(), \"mongo://my-db/my-coll?id_field=name\")\n```\nYou can also use the URL Parameter in the high-level abstraction. The following example specifies the_ID as the name field of the above-mentioned high-level abstraction, which is used by the underlying layer `mongodocstore.OpenCollection` mapping relationship, will automatically generate mongo official driver type `primitivie.ObjectID `\n\n```go\ncoll, err := mongodocstore.OpenCollection(mcoll, \"id\", nil)\n\ntype IDer struct {\n\tID primitive.ObjectID\n}\n```\n\nyou can also use `mongodocstore.OpenCollectionWithIDFunc` to specify how to generate an ID.\n\n```go\nnameFromDocument := func(doc docstore.Document) interface{} {\n    return primitive.NewObjectID()\n}\ncoll, err := mongodocstore.OpenCollectionWithIDFunc(mcoll, nameFromDocument, nil)\n```\n\n\u003ca name=\"F0eWR\"\u003e\u003c/a\u003e\n## Summary \nWe have completed the access design and understanding of Document Store and can perform basic operations on adding, deleting, modifying, and querying docstores. Next, we will build service applications based on this layer of abstraction. \n\nFor special functions of different docstores, you can add them to docstore to determine whether they are target-driven and change the method of external exposure.\n\n\u003ca name=\"E8DHr\"\u003e\u003c/a\u003e\n## function \n\nthe following shows the functions of the library. \n\n\u003ca name=\"hVYIf\"\u003e\u003c/a\u003e\n### Connect MongoDB\nThe default mongo driver uses MONGO_SERVER_URL link to the server, so you can use code to set it here or directly set it by using environment variables. \n\nthe following meaning is from mongodb://localhost:27017 the link on the server is called `my-db`in the database `my-coll` document. The unique field name of mongo is `name`. \n\n```go\nos.Setenv(\"MONGO_SERVER_URL\", \"mongodb://localhost:27017\")\n\ncoll, err := docstore.OpenCollection(context.Background(), \"mongo://my-db/my-coll?id_field=name\")\ndefer coll.Close()\n```\n\u003ca name=\"HM4A2\"\u003e\u003c/a\u003e\n### Corresponding display structure \n```go\ntype Player struct {\n\tName             string `docstore:\"name,omitempty\"`\n\tScore            int\n\tDocstoreRevision interface{}\n}\n```\n\u003ca name=\"YFPTq\"\u003e\u003c/a\u003e\n### Create \n```go\ncoll.Create(ctx, \u0026Player{Name: \"Pat\", Score: 7}); \n```\n\u003ca name=\"CuHHJ\"\u003e\u003c/a\u003e\n### Get \n```go\ncoll.Get(ctx, \u0026Player{Name: \"Pat\"});\n```\n\u003ca name=\"syvtX\"\u003e\u003c/a\u003e\n### Queries \nyou may need to manually create indexes to complete the query function. \n```go\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"gocloud.dev/docstore\"\n)\n\n// Ask for all players with scores at least 20.\niter := coll.Query().Where(\"Score\", \"\u003e=\", 20).OrderBy(\"Score\", docstore.Descending).Get(ctx)\ndefer iter.Stop()\n\n// Query.Get returns an iterator. Call Next on it until io.EOF.\nfor {\n\tvar p Player\n\terr := iter.Next(ctx, \u0026p)\n\tif err == io.EOF {\n\t\tbreak\n\t} else if err != nil {\n\t\treturn err\n\t} else {\n\t\tfmt.Printf(\"%s: %d\\n\", p.Name, p.Score)\n\t}\n}\n```\n\u003ca name=\"zif5K\"\u003e\u003c/a\u003e\n### Update a single field of an Update entry\n```go\npat2 := \u0026Player{Name: \"Pat\"}\nerr := coll.Actions().Update(pat, docstore.Mods{\"Score\": 15}).Get(pat2).Do(ctx)\n```\n\u003ca name=\"VJYOy\"\u003e\u003c/a\u003e\n### Replace \ncompletely replace the entire entry \n```go\ncoll.Replace(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"QjVog\"\u003e\u003c/a\u003e\n### Put \nthe Put function is equivalent to CreateOrUpdate\n```go\ncoll.Put(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"wzyMJ\"\u003e\u003c/a\u003e\n### Delete \n```go\ncoll.Delete(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"wer1V\"\u003e\u003c/a\u003e\n### More examples \n\n- [CLI Sample](https://github.com/google/go-cloud/tree/master/samples/gocdk-docstore)\n- [Order Processor sample](https://gocloud.dev/tutorials/order/)\n- [docstore package examples](https://godoc.org/gocloud.dev/docstore#pkg-examples)\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/blogs/Node-Status-Manager":{"title":"Node Status Manager","content":"## Node Status Manager\r\n\r\n\u003ca name=\"eQr2o\"\u003e\u003c/a\u003e\r\n## ReadLink\r\n- [pkg/kubelet/nodestatus/setters.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/nodestatus/setters.go)\r\n- [/](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg/kubelet)[kubelet_node_status.go](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go)\r\n\r\n\u003ca name=\"HvtiD\"\u003e\u003c/a\u003e\r\n## Directory Layout\r\n```go\r\npkg/kubelet/nodestatus\r\n |- setters.go\r\n |- setters_test.go\r\n```\r\n\u003ca name=\"ABxjo\"\u003e\u003c/a\u003e\r\n## Setter\r\n```go\r\n// Setter modifies the node in-place, and returns an error if the modification failed.\r\n// Setters may partially mutate the node before returning an error.\r\ntype Setter func(node *v1.Node) error\r\n```\r\nthe Setter function defines a function that performs operations on the v1.Node object. If an error is returned, the Node object may also be changed. \r\nFrom the function definition, you can see its usage: use functions to generate different setters for a class of modification of Node objects. In this way, you can modify the state of a Node. \r\nUse the simplest func GoRuntime() Setter example: \r\n```go\r\n// GoRuntime returns a Setter that sets GOOS and GOARCH on the node.\r\nfunc GoRuntime() Setter {\r\n\treturn func(node *v1.Node) error {\r\n\t\tnode.Status.NodeInfo.OperatingSystem = goruntime.GOOS\r\n\t\tnode.Status.NodeInfo.Architecture = goruntime.GOARCH\r\n\t\treturn nil\r\n\t}\r\n}\r\n```\r\nthis mode belongs to the [[middleware]] operation mode. You can contact middleware for understanding. \r\n\u003ca name=\"E3eSp\"\u003e\u003c/a\u003e\r\n## Setter List\r\nwe learned the setter mode changed by Node status. Currently, the code contains the following 12 setters:\r\n- **NodeAddress** returns a Setter that updates address-related information on the node.：updates address-related fields, such as IP address and hostname (typically the hostname variable in kubelet). \r\n- **MachineInfo**returns a Setter that updates machine-related information on the node.：updates fields related to host information, such as the maximum number of pods, the number of pods allocated to each core, and the number of resources. \r\n- **VersionInfo**returns a Setter that updates version-related information on the node.：containerRuntime version, cadvisor version\r\n- **DaemonEndpoints**returns a Setter that updates the daemon endpoints on the node.\r\n- **Images**returns a Setter that updates the images on the node.：updates image information. \r\n- **GoRuntime**returns a Setter that sets GOOS and GOARCH on the node.：GOOS GOARCH information \r\n- **ReadyCondition** returns a Setter that updates the v1.NodeReady condition on the node.：determines whether the node is in the Ready state from Kubelet fields such as the error return function in the runtimeState. \r\n- **MemoryPressureCondition**returns a Setter that updates the v1.NodeMemoryPressure condition on the node.\r\n- **PIDPressureCondition**returns a Setter that updates the v1.NodePIDPressure condition on the node.\r\n- **DiskPressureCondition**returns a Setter that updates the v1.NodeDiskPressure condition on the node.\r\n- **VolumesInUse**returns a Setter that updates the volumes in use on the node.\r\n- **VolumeLimits**returns a Setter that updates the volume limits on the node.\r\n\r\n\r\nSetter 的入参通常是 Kubelet 中的字段，自然使用是通过 [[Kubelet]] 去初始化使用。\r\n\u003ca name=\"uRi9a\"\u003e\u003c/a\u003e\r\n## Kubelet Node Status\r\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg/kubelet)[kubelet_node_status.go](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go)\r\n\u003ca name=\"LGGvP\"\u003e\u003c/a\u003e\r\n###  Setter 使用处\r\nafter all setters are initialized in the defaultNodeStatusFuncs function, the function returns a Setter array. \r\n\r\n[kubelet_node_status.go? L613](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L613)\r\n\r\n```go\r\n// defaultNodeStatusFuncs is a factory that generates the default set of\r\n// setNodeStatus funcs\r\nfunc (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error {\r\n\t// if cloud is not nil, we expect the cloud resource sync manager to exist\r\n\tvar nodeAddressesFunc func() ([]v1.NodeAddress, error)\r\n\tif kl.cloud != nil {\r\n\t\tnodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses\r\n\t}\r\n\tvar validateHostFunc func() error\r\n\tif kl.appArmorValidator != nil {\r\n\t\tvalidateHostFunc = kl.appArmorValidator.ValidateHost\r\n\t}\r\n\tvar setters []func(n *v1.Node) error\r\n\tsetters = append(setters,\r\n\t\tnodestatus.NodeAddress(kl.nodeIPs, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),\r\n\t\tnodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,\r\n\t\t\tkl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent),\r\n\t\tnodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),\r\n\t\tnodestatus.DaemonEndpoints(kl.daemonEndpoints),\r\n\t\tnodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),\r\n\t\tnodestatus.GoRuntime(),\r\n\t)\r\n\t// Volume limits\r\n\tsetters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))\r\n\r\n\tsetters = append(setters,\r\n\t\tnodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),\r\n\t\tnodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),\r\n\t\tnodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),\r\n\t\tnodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager.Status, kl.shutdownManager.ShutdownStatus, kl.recordNodeStatusEvent),\r\n\t\tnodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),\r\n\t\t// TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event\r\n\t\t// and record state back to the Kubelet runtime object. In the future, I'd like to isolate\r\n\t\t// these side-effects by decoupling the decisions to send events and partial status recording\r\n\t\t// from the Node setters.\r\n\t\tkl.recordNodeSchedulableEvent,\r\n\t)\r\n\treturn setters\r\n}\r\n```\r\nThe array is assigned to the setNodeStatusFuncs of kubelet.\r\n```go\r\n\t// Generating the status funcs should be the last thing we do,\r\n\t// since this relies on the rest of the Kubelet having been constructed.\r\n\tklet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()\r\n```\r\n\u003ca name=\"n0m8P\"\u003e\u003c/a\u003e\r\n## SyncNodeStatus Procedure\r\nhow do Kubelet use these Kubelet? The core is syncNodeStatus functions.\r\n\r\n[kubelet_node_status](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L435)\r\n```go\r\n// syncNodeStatus should be called periodically from a goroutine.\r\n// It synchronizes node status to master if there is any change or enough time\r\n// passed from the last sync, registering the kubelet first if necessary.\r\nfunc (kl *Kubelet) syncNodeStatus() {\r\n\tkl.syncNodeStatusMux.Lock()\r\n\tdefer kl.syncNodeStatusMux.Unlock()\r\n\r\n\tif kl.kubeClient == nil || kl.heartbeatClient == nil {\r\n\t\treturn\r\n\t}\r\n\tif kl.registerNode {\r\n\t\t// This will exit immediately if it doesn't need to do anything.\r\n\t\tkl.registerWithAPIServer()\r\n\t}\r\n\tif err := kl.updateNodeStatus(); err != nil {\r\n\t\tklog.ErrorS(err, \"Unable to update node status\")\r\n\t}\r\n}\r\n```\r\nsyncNodeStatus the function is called periodically in goroutine to synchronize the node status to the master.\r\n\u003ca name=\"bdMgb\"\u003e\u003c/a\u003e\r\n### 入口 Entry\r\ncurrently, it is called in three places: \r\n\r\n1. [kubelet.go? L1428:26](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet.go?L1428:26)\r\nin the Run function of the Kubelet, start goroutine for periodic synchronization. \r\n```go\r\ngo wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop)\r\n```\r\n\r\n2. [kubelet.go? L2433:7](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet.go?L2433:7)\r\n: performs one-time synchronization in the fastStatusUpdateOnce function.\r\n```go\r\nfunc (kl *Kubelet) fastStatusUpdateOnce() {\r\n\tfor {\r\n\t\t...\r\n        kl.syncNodeStatus()\r\n        return\r\n\t}\r\n}\r\n```\r\n\r\n3. [nodeshutdown_manager_linux.go? L283:11](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go?L283:11)\r\n: it is called in the start() function of nodeshutdownmanager, which is actually a goroutine and is triggered only after the shutdown event is received from the channel.\r\n```go\r\nif isShuttingDown {\r\n    // Update node status and ready condition\r\n    go m.syncNodeStatus()\r\n\r\n    m.processShutdownEvent()\r\n} \r\n```\r\n\r\n\u003ca name=\"FKaYq\"\u003e\u003c/a\u003e\r\n### 注册 RegisterWithAPIserver\r\nif kubelet needs to be registered, a for loop is executed to wait for registration to the APIServer.\r\n```go\r\nfor {\r\n    time.Sleep(step)\r\n    step = step * 2\r\n    if step \u003e= 7*time.Second {\r\n        step = 7 * time.Second\r\n    }\r\n\r\n    // 1. 获取 node 对象及其信息\r\n    node, err := kl.initialNode(context.TODO())\r\n    if err != nil {\r\n        klog.ErrorS(err, \"Unable to construct v1.Node object for kubelet\")\r\n        continue\r\n    }\r\n\r\n    klog.InfoS(\"Attempting to register node\", \"node\", klog.KObj(node))\r\n    // 2. 注册到 APIServer 中去\r\n    registered := kl.tryRegisterWithAPIServer(node)\r\n    if registered {\r\n        klog.InfoS(\"Successfully registered node\", \"node\", klog.KObj(node))\r\n        kl.registrationCompleted = true\r\n        return\r\n    }\r\n}\r\n```\r\n\r\n1. node, err := kl.initialNode(context.TODO()) : obtains the node object and its information. \r\n2. registered := kl.tryRegisterWithAPIServer(node) : Register to APIServer \r\n\r\n\u003ca name=\"bj9z5\"\u003e\u003c/a\u003e\r\n### Use Setter\r\n[Function tryUpdateNodeStatus (kubelet_node_status.go? L470:20)](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L470:20)\r\n: the processing part of the volumeManager is omitted.\r\n```go\r\n// tryUpdateNodeStatus tries to update node status to master if there is any\r\n// change or enough time passed from the last sync.\r\nfunc (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error {\r\n    originalNode := node.DeepCopy()\r\n    ...\r\n\tkl.setNodeStatus(node)\r\n    ...\r\n\t// Patch the current status on the API server\r\n\tupdatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node)\r\n    ...\r\n\treturn nil\r\n}\r\n```\r\nkl.setNodeStatus just traverses all the Setter functions we mentioned just now.\r\n\r\n```go\r\nfunc (kl *Kubelet) setNodeStatus(node *v1.Node) {\r\n\tfor i, f := range kl.setNodeStatusFuncs {\r\n\t\tklog.V(5).InfoS(\"Setting node status condition code\", \"position\", i, \"node\", klog.KObj(node))\r\n\t\tif err := f(node); err != nil {\r\n\t\t\tklog.ErrorS(err, \"Failed to set some node status fields\", \"node\", klog.KObj(node))\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\u003ca name=\"GWa5D\"\u003e\u003c/a\u003e\r\n## Conclusion\r\nwe have learned: \r\n1. what are the change functions of the Node Status and what rules are followed to sign the function.\r\n2. how to register a Setter function to a Kubelet. \r\n3. Kubelet when these setters are called to change the status of a Node. \r\n\r\nNext: \r\n\r\n1. you can try to add a custom setter function. \r\n2. Kubernetes code is not as neat as the design. Some todo can be changed after reading this article and code. Try to decouple the code. (You can also find it by searching todo in the code.)\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/blogs/Open-Source-History-of-Dapr-project":{"title":"Open Source History of Dapr project","content":"[[## Open Source]] History of [[Dapr]] project\r\n\r\nAt the beginning of this open-source column, I wrote this article to describe the birth and development of open-source projects, express my views on the open-source community and ecology, and share it with you. \r\n\r\nSome opinions are out of personal perspective, and there are inevitably some mistakes and mistakes. Please forgive me and correct me. \r\n\r\n## Background\r\nbefore the birth of the Dapr project, I would like to explain the current situation of the Dapr project for readers to understand the project itself. \r\n\r\nDapr is a [[CNCF]] community-driven open source project with Microsoft as its contributor. Microsoft, according to the author, the first author should be [[Bai Haishi]] and [[Yaron]] (he is also the author of the Dapr Learning Manual, who proposed [[OAM]] and Dapr). \r\n\r\nThe work objectives of the Dapr project are described as follows: \r\n\r\nDapr is a portable, event-driven runtime that enables any developer to quickly build flexible, stateless, and stateful applications that can run on cloud platforms or edge computing. \r\nSome community students think Dapr is the next form of the service mesh, and some people also call this runtime software of the new era [[mecha]] (mecha), mecha provides distributed capabilities for business applications, just like the operator wearing a mecha, to do what he could not have done.\r\n\r\nThe following figure shows Bilgin Ibryam. Multi-Runtime Microservices Architecture \r\n\r\n\r\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658032639778/CRnDlZMU-.png align=\"left\")\r\n\r\ntherefore, the Dapr project is open-source software that provides distributed capabilities for modern distributed applications. Currently, it is open-source on GitHub and has gained 18.6K stars, which is very popular. \r\n\r\n## Birth \r\nTracing back to open source on GitHub, the first submission was on June 21, 2019, \r\nthe birth of open-source projects is usually accompanied by the discovery of practical problems. \r\n\r\nBefore saying the problems solved by the Dapr project, there is another project that has to be mentioned, Microsoft's OAM(Open Application Model). \r\n\r\nThe two projects have been known to the public for 19 years. I remember that the co-sponsor of OAM is Ali. At that time, Kubernetes was very popular, and the problems on the computing scheduling platform were Kubernetes solved by Golang's killer application. \r\n\r\nHowever, using Kubernetes puts forward more and higher requirements for Developers, especially its new concept, which covers different APIs and unique working methods. \r\n\r\nHow to solve this problem? \r\n\r\nAny problem in the field of computer science can be solved by adding an indirect intermediate layer.\r\n\r\nIt is believed that smart readers, based on their current knowledge, have already thought that if a new design language can be used as the middle layer to block the similarities and differences of infrastructure developers do not need to pay attention to and focus on business coding, can it be solved? \r\n\r\nIn this way, OAM is naturally ready to come out. (If you are concerned about OAM, you can learn about the implementation of this project standard in Alibaba, namely Kubevela project, this project has great potential)\r\n\r\nDapr came up with an idea when Bai Haishi and his Israeli colleagues discussed OAM Yaron Schneider. It designed a new programming mode to encapsulate the common functions of the distributed system into Sidecar(Kubernetes concept, description, and business application in the same Pod container) and expose them to developers through HTTP or gRPC (two common transmission modes, which are compatible with most applications). \r\n\r\nThe idea is named Distributed Application Runtime, or Dapr for short. [This paragraph is taken from an interview with Bai Haishi, the founder of OAM and DAPR: a simple idea of a 33-year senior programmer -Zhang Shanyou]] \r\n\r\nDapr provides several new features to help solve the problems: \r\n\r\nthe first is to provide services in the form of Sidecar. In the container orchestration platform, Sidecar provides services in a non-intrusive way. \r\n\r\nFor example, Envoy Sidecar acts as a proxy for routing and forwarding. It is independent of major applications and therefore has cross-language features. Users can reuse logic without binding to a programming language, which is especially useful in the microservice era. \r\n\r\nThe second is the concept of Building Block, which allows Dapr users to customize different Building blocks, instead of forcing users to use distributed functions provided by Dapr for all functions.\r\n\r\n## Open source \r\n\r\nAfter talking for so long, I finally talked about the open-source features of the Dapr project. \r\nThe benefits of open source can be seen in the summary of my other article. This article will not go into detail, but mainly explore the reasons why Dapr needs to open source and provide material examples for everyone to understand the open source operation mode. \r\n\r\nDapr can be analyzed from the positioning of its general distributed runtime software, and the standard is its core! \r\n\r\nStandards cannot be achieved by one person or a company. It is necessary to strengthen Dapr's influence and promote its designation of standards that are uniformly recognized by everyone. It is the only choice to establish a community of common contributions through open source.\r\n\r\nIt is not only a matter of standards. Dapr, as an application in the new era, naturally has many new ideas, which need to be verified. A large number of engineers need to be invested in the verification of the programming mode. \r\n\r\nThis part of manpower expenditure and verification cost is extremely large. The continuous development of the project can only be supported by the rapid discussion of design, implementation, and community verification in the form of Community co-construction. \r\n\r\nTherefore, human resources are also considered in most open-source projects. \r\n\r\nFinally, reach users. \r\n\r\nWhen Dapr is a user-oriented project, there are developers who are more enthusiastic than open-source communities. Open-source is the best choice to make Dapr's development closer to users and the wide application of cloud developers that it wants to achieve. \r\n\r\nWe recommend two projects to observe the popularity and activity distribution of open-source projects ( [[Star-History]] and [[OSSInsight]] ), which are [[Bytebase]] and [[PingCap]] open-source tools. One picture wins thousands of words, and two pictures are attached to show its function. \r\n\r\nFigure 1: Star harvest trend of open source Dapr project\r\n\r\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658032918444/FsLL7iu_d.png align=\"left\")\r\n\r\n## Development\r\nSince the Dapr project is to serve developers, it is natural to investigate the main functions that developers use in the programming and provide them to users as different building blocks. \r\n\r\nCurrently, the capabilities it provides include state management, service development, message sending and receiving, publishing and subscription, security information management, Actor mode (originated from the Orleans project in Microsoft's dotnet ecosystem), etc. In the initial form of state management development, concurrent control, version management, and other capabilities are also added. \r\n\r\nNow, building blocks such as distributed locks and workflows are gradually added. These new functions and new building blocks are all built by community users' needs.\r\n\r\nIt can be seen from this that Dapr's open source strategy has achieved remarkable results. \r\n\r\nThe emergence of Dapr also coincides with the wave of XaaS. It reduces the occupation of the edge environment (more than 50 M binary, only 4 M memory is needed during operation), provides edge devices and applications with low capability, flexibly switches between edge environment and cloud, and supports multiple operating environments, which are its excellent sources of competitiveness. \r\n\r\nThe development evaluation of an open source project must pay attention to its related ecology. Dapr, as a similar infrastructure project, will discuss two ecosystems. \r\n\r\nOne is the ecosystem that supports the Dapr project operation. That is, driven by various Building blocks, their ecology determines which infrastructure Dapr users can apply.\r\n\r\nTake [[PubSub]] as an example. Common message queue drivers such as Kafka, Redis, NatsStreaming, and Pulsar provide the runtime capability in the publish/subscribe mode. \r\n\r\nThe ecosystem in this area is rich and colorful. The core problem is that drivers are contributed to the community by themselves. The code quality and the functions provided during application runtime are uneven. It can be seen that the idea of standardization cannot be easily achieved in the real world. \r\n\r\nOne is the Dapr-based project built on it. This ecology can also be reflected in the cases where most companies use Dapr. \r\n\r\nThe main users of Dapr started from the founders Microsoft and Ali, and now companies such as [[Qingyun]] have participated in the co-construction and produced many projects and practical cases. \r\n\r\nTaking Microsoft as an example, users who serve it can easily and painlessly switch the underlying dependencies on the cloud (for example, switching message queues from Rabbitmq to kafka). \r\nFor example, Alibaba provides a large number of distributed capabilities for function applications in its functional computing platform. \r\nFor example, Ant Financial has developed a layotto project based on its excellent ServiceMesh development experience, IT has implemented the distributed runtime concept that conforms to its own IT infrastructure (and is open-source).\r\nFor example, Qingyun's Openfunction is also built using Dapr in the function computing platform.\r\n\r\nEven Microsoft has launched a commercial product container app based on Dapr, which allows users to write function-level services. The infrastructure is provided by context. \r\nDapr provides these services with the choice of only focusing on business code logic. \r\n\r\nThe developer ecosystem of open-source projects is an important criterion. The number of issues created, the speed of response, the richness of proposal submission, the degree of the active contributor, the entry and loss of new contributors and core contributors, and other indicators are all important bases for us to evaluate the developer ecosystem. This section can praise [[OSSInsight]] project, which provides you with query services through the website and provides us with powerful data for evaluating open-source projects. \r\n\r\nFigure 2: analysis of open source activities of the Dapr project\r\n\r\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658033060286/sUkAuPA7c.png align=\"left\")\r\n\r\n## Trend \r\ncompetition trend of domain software: Hot open-source projects actually symbolize the main competition fields in the current industry. PaaS, which took Kubernetes as the core in the past few years, IaaS, which was recently represented by Infra as Code, DevOps and Security, and SaaS and FaaS, which will further compete fiercely in the future, provide better value-added services. Different fields have their own solutions. We can see how to provide more valuable services from the open-source ecosystem. \r\n\r\nThe development trend of programmers: modern developers are generally faced with anxiety problems. As programmers, some of our work contents are boring, but with the passion for programming and the pursuit of a career, we can develop various innovative achievements in our daily work, which may not only achieve ourselves but also benefit the world. At the industry level and even at the national level, open source is embraced. Under such a development trend, open source will integrate young programmers as one of the popular cultures. \r\nMy personal advice is to understand the open source as soon as possible, embrace him, and become a compound talent. The next step for programmers is to explore the open source field. \r\n\r\nAnd this article has roughly described the context of the Dapr project. Only from the project ecology of Dapr, we can see the fierce competition in the development of cloud computing. We don't know how many projects are floating and disappearing in the tide, or they never appear in our eyes after a wave of waves. I hope readers can have a deeper understanding and ideas about the software life cycle, especially open-source software.\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/blogs/React-Hooks-State-Persistence":{"title":"React Hooks State Persistence","content":"\r\n\r\n本文讲述如何分析设计 通过 [[React Hooks]] 进行 State 持久化管理\r\n\r\n  \r\n\r\n## 分析\r\n\r\n正常前端，组件为类文件，自己维持状态，不易复用。\r\n\r\n  \r\n\r\n首先把组件中的 [[UI]] 和 状态分开，用 Action 连接，如下图。\r\n\r\n![[blogs/Pasted image 20221011190847.png]]\r\n\r\nAction 是[[算子]]\r\n\r\n  \r\n\r\n### Function\r\n\r\n则可成为以下函数\r\n\r\n-   UI = `f(S)`\r\n\r\n-   状态驱动组件重新渲染 UI\r\n\r\n-   Scu =`f(Sc, ∆)`\r\n\r\n-   组件会用到的 Scu 和 更改 Sc 的 ∆ 方法决定。\r\n\r\n  \r\n\r\n#### S\r\n\r\n每一个组件有他自己的状态集 s。\r\n\r\n  \r\n\r\n##### scu\r\n\r\n即，component use ：组件用到的状态，比如计数器中的数字\r\n\r\n所有组件的使用到 scu 共同组成一个状态 Scu--渲染一个 UI。\r\n\r\n  \r\n\r\n##### sc\r\n\r\n即，收到组件影响的状态，如登录组件可能每登录一次就会增加计数器，但是对于登录组件并不会用到这个状态，虽然它会更改它。\r\n\r\n  \r\n\r\n### 入参\r\n\r\n#### ∆\r\n\r\n设计 State 框架时，让每一个组件声明 sa 状态时，提供一个更改自己的函数 ma ，在 Action 事件时调用用于更改 State，而多个 ma 的集合为 ∆。\r\n\r\n  \r\n\r\n#### S\r\n\r\nSa =`f(S, ∆)` 中的 S 作为 f 的参数传入，因为并不知道 Action 会更改哪些 State 【甚至不知道有哪些】，故把所有 State 都作为入参。\r\n\r\n  \r\n\r\n### 局部渲染\r\n\r\n更改的状态驱动 UI 渲染，如果相同可以不改变。\r\n\r\n  \r\n\r\n如上所说，UI 由于入参为 S ，会接收所有的 State，组件自己根据自己需要的 sa 变动渲染，而不是 UI 根据 S 改动分发事件。\r\n\r\n  \r\n\r\n观察 Hooks 可知，`useState()` 方法使用`[Object.is](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is#Description)` [比较算法](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is#Description) 来比较 state。\r\n\r\n而 `useEffect()`则提供选择让它 [在只有某些值改变的时候](https://zh-hans.reactjs.org/docs/hooks-reference.html#conditionally-firing-an-effect) 才执行的参数。\r\n\r\n  \r\n\r\n## 设计\r\n\r\n### 实虚部数学模型\r\n\r\n实数并不完备，引入虚部。\r\n\r\n  \r\n\r\n虚数，只需要去掉虚部就可以表示实数。\r\n\r\n  \r\n\r\nCurry Func 也如此。\r\n\r\n  \r\n\r\n以上同理：`f(S,∆)`中`f(S)` 代表实数，不完备，加入 ∆ ，可以表达所有情况。\r\n\r\n  \r\n\r\n更改的维度从一维的 线 成为了 二维的平面。\r\n\r\n  \r\n\r\n另：框架使用的`f(S, ∆)`还是一维的线，但其实是该平面 任意一条线 ，因为 f(S,∆) 已经中的 ∆ 和 S 已经经由使用者确定，即在多维度选择了一个平面降维实现在代码中了。\r\n\r\n  \r\n\r\n### Persistence \r\n\r\n需要一个地方存储数据，local，session，remote 等.\r\n\r\n  \r\n\r\n### Connector\r\n\r\n组件如何把触发的事件分发给 State 处理？需要通信。\r\n\r\n  \r\n\r\n由于 js 单线程模型，选择共享内存设计新增一个 Connector 用于通信。\r\n\r\n  \r\n\r\n组件 Component 如何通知 State 改动。共享内存，采用 Connector 中间层。\r\n\r\n  \r\n\r\n### Action by CurryFunc\r\n\r\nState 如何知道框架使用者定义的 Action 改动了哪些 State ？即不知道 ∆ 的具体值。采用 Curry Func 满足延迟求值的需求。\r\n\r\n使用 `fg(S){return f(∆)`} 代替 `f(S,∆)`\r\n\r\nState 框架使用者自己使用 `f(∆)` 注册自己的状态更改算子 ∆。\r\n\r\n  \r\n\r\nState 框架开发者使用 `fg(S)` ，只管自己传入所有的 State 即可。\r\n\r\n  \r\n\r\n由于 React Hooks 的存在，state 自带使用 `f(S,∆)` 进行更新的功能。故框架留出 useState() 接口，返回 `f(∆)` ，供使用者进行状态管理。\r\n\r\n  \r\n\r\n#### Redux\r\n\r\nRedux 也是基于此函数模型，而在 Hooks 中官方已经使用 `useReducer(reducer, initialState)` 实现了它。其中 reducer 是设定好的 `f(S，∆)` ，而它返回 state 和 dispatch，其中 state 就是 Sa 而 dispatch 就是 `f(∆)`。\r\n\r\n```js\r\nfunction useReducer(reducer, initialState) {\r\n  const [state, setState] = useState(initialState);\r\n  function dispatch(action) {\r\n    const nextState = reducer(state, action);\r\n    setState(nextState);\r\n  }\r\n  return [state, dispatch];\r\n}\r\n```\r\n\r\n在我们看来，它也内部实现了 Connector 的作用。\r\n\r\n  \r\n\r\n## 实现\r\n\r\n### Persistence\r\n\r\n首先是通过 Hooks 实现存储, 使用 Local Store\r\n\r\n```js\r\nfunction useLocalJSONStore(key, defaultValue) {\r\n    const [state, setState] = useState(\r\n      () =\u003e JSON.parse(localStorage.getItem(key)) || defaultValue\r\n    );\r\n    useEffect(() =\u003e {\r\n      localStorage.setItem(key, JSON.stringify(state));\r\n    }, [key, state]);\r\n    return [state, setState];\r\n}\r\n```\r\n\r\n  \r\n\r\n#### 存储位置\r\n\r\n解决了持久化存储，提供外在的状态管理支持。考虑到我们会使用 Go 来做前端：\r\n\r\n1.  使用 Hooks 加 sqlite3 库本地存储\r\n2.  使用 Hooks 和 [[Go]] 通信完成\r\n\r\n  \r\n\r\n### Connector\r\n\r\n为了使用 Hooks 实现全局的状态通知。\r\n\r\n首先明白 `useState()` 获取到的 `setState()` 会触发当前组件的渲染：[https://zh-hans.reactjs.org/docs/hooks-state.html](https://zh-hans.reactjs.org/docs/hooks-state.html)\r\n\r\nConnector 让使用全局状态的组件订阅来连接上全局的状态更新，将自己的 `setState()` 传入更新队列，当其中任何一个组件使用 `dispatch()` 更改状态时会触发这个命名空间下的全部状态更新，从而达到刷新所有状态组件的目的。\r\n\r\n```js\r\nimport { useEffect } from \"react\"\r\n\r\nconst Connector = {}\r\n\r\nconst Broadcast = (name, state) =\u003e {\r\n    if (!Connector[name]) return;\r\n    Connector[name].forEach(setter =\u003e setter(state))\r\n}\r\n\r\nconst Subscribe = (name, setter) =\u003e {\r\n    if (!Connector[name]) Connector[name] =[];\r\n    Connector[name].push(setter)\r\n}\r\n\r\nconst UnSubscribe = (name, setter) =\u003e {\r\n    if (!Connector[name]) return\r\n    const index = Connector[name].indexOf(setter)\r\n    if (index !== -1) Connector[name].splice(index, 1)\r\n}\r\n\r\nconst connect = (name,setState) =\u003e {\r\n    console.log('connect')\r\n    useEffect(() =\u003e{\r\n        Subscribe(name, setState)\r\n        console.log('subscirbe',name)\r\n        return () =\u003e {\r\n            UnSubscribe(name,setState)\r\n            console.log('unsubscribe',name)\r\n        }\r\n    },[])\r\n}\r\n```\r\n  \r\n\r\n### useStore\r\n\r\n使用者使用 `useStore()` 来获取全局状态和 `dispatch()` 函数。内部实现就是 State Hook ，并拿到 `setState()`注册到订阅列表中。\r\n\r\n```js\r\nimport {Broadcast,connect} from './Connector'\r\nimport {useState} from 'react'\r\n\r\nexport function useStore(key,value) {\r\n    const [state,setState] = useState(value)\r\n    connect(key,setState)\r\n\r\n    return [state, (key,value) =\u003e {\r\n        Broadcast(key,value)\r\n    }]\r\n}\r\n```\r\n\r\n  \r\n\r\n### 目前状况\r\n\r\n  \r\n\r\n![](https://cdn.nlark.com/yuque/0/2019/svg/176280/1574064167135-22f14865-31ba-4b6c-a144-0d1315954ec1.svg)\r\n\r\n  \r\n\r\n## 使用\r\n\r\n使用 `useStore(key, value)` 即可。\r\n\r\n```js\r\nimport {useStore} from './useStore'\r\n\r\nexport function Counter({key,initialCount}) {\r\n    // const [count, setCount] = useLocalJSONStore(keyname, initialCount);\r\n    const [state, dispatch] = useStore(key,initialCount)\r\n    return (\r\n      \u003c\u003e\r\n        Count: {state}\r\n        \u003cbutton onClick={() =\u003e dispatch(keyname,initialCount)}\u003eReset\u003c/button\u003e\r\n        \u003cbutton onClick={() =\u003e dispatch(keyname,state-1)}\u003e-\u003c/button\u003e\r\n        \u003cbutton onClick={() =\u003e dispatch(keyname,state+1)}\u003e+\u003c/button\u003e\r\n      \u003c/\u003e\r\n    );\r\n  }\r\n```\r\n\r\n  \r\n\r\n![](https://cdn.nlark.com/yuque/0/2019/png/176280/1574063711150-a567cd8c-2117-47ea-8446-02da34624b22.png)\r\n\r\n## 进阶\r\n\r\n-   异步状态\r\n-   [[装饰器]]","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/blogs/Role-of-microservice-framework":{"title":"Role of microservice framework","content":"## Role of [[microservice framework]]\r\n\r\n## HTTP Channel and [[GRPC]] Channel \r\n\r\nbefore we begin, let's explore the differences between HTTP and RPC. The reason why gRPC is discussed here is that no one uses common RPC. \r\n\r\nHTTP is a common communication method used for business coding, and its popularity is needless to say. As a Web programmer, HTTP Server programming is its core skill. RPC is also indispensable in microservices. \r\n\r\nCan programmers who are familiar with one of the encoding quickly get started with the business coding of another transmission method? \r\n\r\nAfter all, the business logic is consistent, which seems to be only different in network transmission. \r\n\r\nColleagues who have done this know that the differences in business coding are not small. Although the differences are constrained to the transport layer through the abstraction layer during design, there is no framework to block the differences in implementation. Therefore, coding students need to go deep into it and handle it by themselves. \r\n\r\nFor example, you need to learn more about envoy and proto files, how to encode requests and return values, and how to use specific [[protobuf]] to parse message packets in your business. \r\n\r\nThe differences can be shielded at the abstraction level. We still need to write detailed differences in implementation. These are the operations that some programmers can replace with frameworks. \r\n\r\n## Top programmers and beginners, beginners, and idiots \r\n\r\nthe role of the framework is to make correct coding behavior without thinking.\r\n\r\nThere are enough ecological libraries for the current language to help compile various coding types. When lacking, some ecosystems can be transplanted by referring to other languages to reduce the workload. However, not every programmer can do such behavior at any time. \r\n\r\nGoogle is a friend. Business Code they often get into trouble because of something they don't know so that no matter how their skills or intelligence are, they can't solve the problem.\r\n\r\nIn the business, some coding work will be compared to moving bricks. Programmers are described as manual work to move code from here to there. However, when someone participates in the process, the error probability will also be related to the state of a person.\r\n\r\nThrough Murphy's law, we can recognize that errors must occur in these processes. How do reduce personal decisions to ensure high quality and high output of assembly line coding manpower?\r\n\r\nIf you want to treat yourself as an idiot coder and leave the error-prone parts to tools, the framework will generate great benefits. \r\n\r\n\r\n\u003e Nothing is built on Stone; Everything is built on sand, but we must build sand as stone.\r\n\u003e                                 -Jorge Louis Borges \r\n\r\n\r\nthe following are some examples: \r\n\r\n- code review: \r\n\r\narchitects not only need to formulate process standards, but also need to supervise the implementation. Code review is the major part of the workload. However, there are thousands of people, and code writers have their ideas. There may even be a design-based cohesion function, which is scattered at all levels in implementation, and the review process is even more inefficient. \r\n\r\nConstraints can be carried out through the framework, which is also the wisdom of software engineering. By increasing restrictions, standards can be formulated to provide efficiency.\r\n\r\n- Best Practices: \r\n\r\nbusiness code usually uses simple addition, deletion, modification, query data, and target resources. At the same time, there are some common functional requirements, such as [[JWT]]. \r\n\r\nThe framework can shield these differences. For example, JWT only has different types of tokens carried by HTTP, and ORM shields the actual data storage software interfaces in the background for addition, deletion, query, modification, and modification. \r\n\r\nThis is another wisdom of computer science, solving problems by adding a middle layer. Framework users can switch to different implementations without thinking.\r\n\r\nIf the best practices provided by the framework cannot meet the requirements, it is time for the document to show its role. Technical personnel-oriented documentation is useful only when problems occur.\r\n\r\n## The dilemma of microservices caused by abstract hierarchy and abstract leakage \r\n\r\n\u003e Google software engineering mentions three key differences between programming and software engineering: **time**, **scope**, and **trade-offs**. \r\n\r\nHowever, the idea of the framework is beautiful enough, but the realization, in reality, is full of trade-offs and the pursuit of perfection. \r\n\r\nEven if the strange requirements of a specific time limit on the business side are excluded. The design cannot be accomplished overnight and a perfect abstract design can be completed.\r\n\r\nAbstract leakage refers to the abstraction of implementation details that should be hidden during software development, which inevitably exposes the underlying details and limitations.\r\n\r\nNot to mention that a complete system has more than one or two levels. How to make reasonable abstraction and promote it as a standard is a long-term practice and change in many microservice frameworks and coding fields. \r\n\r\nAbstraction means unification, while behind the abstraction level, it usually means the actual services with different characteristics. Do you use the union or intersection of these services for abstraction? Whether to consider extended compatibility or functionality.\r\n\r\n\u003e For more information, see another article. [Mongo Doc access design](), is practical experience. \r\n\r\n\u003e Also The API of Dapr. Many Interfaces of Golang (IO, SQL, and Net) can see abstract practical practices.\r\n\r\nFor example, designers will struggle with whether to provide a certain function to the outside, so they have done a lot of work to provide it. However, in terms of function usage, it may be a pseudo requirement or a simple shielding. However, in actual scenarios, it is necessary to have a lower layer of functions, and the abstraction level is still broken down. \r\n\r\nAt this point, everyone understands that it falls into specific scenarios and analyzes specific problems. Therefore, a microservice framework that has passed the postgraduate entrance examination for a long time must have solved many problems in the target scenario. \r\n\r\n\u003e This reminds me that programmers always pursue new technologies. New microservice frameworks usually have high expectations, hoping that they can completely solve the problems encountered in practice that the old frameworks cannot solve. Finally, expectations often fail. Why can we expect a new untested framework to meet the needs of the technical framework that has been designed and modified many times in practice in specific fields?\r\n\r\nBack to our question at the beginning, is there a framework that unifies the HTTP Channel and gRPC Channel, and only needs to write the handler's internal code without paying attention to other work?\r\n\r\n In the modern framework, Dapr did accomplish this. \r\n\r\nWhat about the abstract cost? \r\n\r\nThe field type in the [[Protobuf]] is lost, and it is considered a payload. The handler has different self-processing types, which is consistent with HTTP abstraction. \r\n\r\nIs it true that such an abstraction layer has just come up with now? If you have a deeper understanding of computer science, you will find that some past ideas shine brilliantly in new scenarios. \r\n\r\nTime is the most significant variable (for example, previous programmers needed to deduct bytes. Now, do you still need to care about insufficient memory for personal PC and cloud coding?). \r\n\r\n## Summary\r\nThe above describes the problems related to the microservice framework considered in the experience. Just raising questions is a hooligan. My opinions and suggestions are mentioned a lot in the article. \r\n\r\nTo make a summary, it is: \r\n\r\nproviding a fool-like automated microservice framework enables programmers to make fewer decisions and make better decisions. \r\n\r\nOnly by using the time saved to innovate business links and business models, and not being involved in non-creative work such as environment building, can workers feel the value of innovation and self-achievement.","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/blogs/Technician-and-experiencer":{"title":"Technician and experiencer","content":"## Technician and experiencer\r\n\r\nWe believe that the experience of the experience is linked to the actual things and actual behaviors, while the technology of the technician symbolizes more general knowledge. \r\n\r\nFirst of all, in practical operation, it can be seen that skilled people are inferior to experienced people in all aspects, but they usually do better than inexperienced people. \r\nBy many views gained from the experience of the universal Judgment (Generic), we believe that technology was born. \r\n\r\nGenerally, we think that people with technology have a deeper understanding of this kind of thing, (in short, smarter) because their behaviors will be guided, and their starting point is reason rather than inertia. We also believe that people with technology master the ability to impart theories, but experienced people cannot teach others. \r\n\r\nDo you want to judge whether you are good at a certain technology and a master in this field? \r\n\r\nwhen you think that you can do something if others ask you but you can only tell the specific situation and specific practices, you are not a wise professor. If the universal judgment you say is not considered dialectically by yourself, you are not a Master of this event. \r\n\r\nAccording to this statement, the concept of a Technologist is close to a wise man, and wisdom is also explained as knowledge about the origin and principle of things. Readers can now think about what characteristics people with high IQ have in the concept we are talking about? \r\n \r\nfrom the above ideas, it is not surprising that mathematicians call them philosophers or wise men, because this discipline is based on the basic principles (Root) rather than a series of secondary disciplines. \r\n\r\nTherefore, the more common the principle is, the more it is regarded as truth. People who think they have mastered the truth tend to be frustrated in the field of new knowledge. \r\n\r\nSince the opinions that can be collected correspond to the infinite things, in reality, I am inconvenient to think that the universal principle of a kind of things also lacks the existence of truth because there are infinite kinds. \r\n\r\nSo when experts claim to be masters in other fields, I don't fear to think that they are not very good in this field.\r\n\r\nThe above metaphysical discussion is not to explore whether the truth exists in a pessimistic way. Instead, I want to express my praise for practical operation from the perspective of reality and the method of getting the so-called twice the result with half the effort by deeply learning the principles of things. \r\n\r\nOn the other hand, as programmers, top programmers, and technicians' technologies, they can explain the measurement of the difference in value they create. \r\n\r\nIt is not difficult to become an expert in a certain field through practice but based on the viewpoint of experience summary, it will be our goal to put forward universal fragments (.e. creation, design, and the invention of new technologies). ","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/blogs/TokenBucket":{"title":"TokenBucket","content":"## TokenBucket\r\n\r\n\r\n## Overview\r\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1657442131915/dUeLDBuYM.png align=\"left\")\r\n\r\n\r\n\r\n- available per second Limit put tokens into the bucket, or, every time 1/Limit add a token to the second bucket \r\n- maximum storage in buckets burst tokens. If the bucket is full, the new token will be discarded. \r\n- when an N is consumed when the data packet of the unit arrives N tokens, and then send the packet \r\n- if the available token in the bucket is less than N, the packet will be cached or discarded \r\n\r\n## token bucket algorithm \r\n\r\nthe token bucket algorithm is the most commonly used algorithm in network Traffic Shaping (Traffic Shaping) and Rate Limiting (Rate Limiting). \r\n\r\nTypically, the token bucket algorithm is used to control the number of data sent to the network and allow the sending of burst data. \r\n\r\n### overview \r\n\r\nthis package is based on the Token Bucket algorithm (Token Bucket) to implement throttling, which is very easy to use. RateLimiter is often used to limit the access rate to some physical or logical resources. It supports three methods, \r\n\r\n- AllowN() If you can't get it, return it immediately.\r\n- WaitN() It is temporarily lined up. When the token is sufficient, it may be returned to the position because of the Cancel of Context.\r\n- ReserveN() Started directly, but the predecessors dug the pit and filled it. The next request will pay the price for this, and wait until the tokens will make up for the air. There is enough token in the barrel.\r\n\r\n### Working instance \r\nassume that one is working RateLimiter \r\n\r\n#### allow and wait \r\nFor a Ratelimiter that generates a token per second, every second without a token, we will add a token 1.\r\n\r\n If the Ratelimiter does not use it in 10 seconds, then tokens become 10.0. At this time, a request arrives and requests three tokens, we will serve it from the token in Ratelimiter, tokens to 7.0. After this request, another request comes and requests 10 tokens.\r\n\r\n We will from the remaining 7 token cards from RatelimiterFor this request, there are three tokens left, we will get them from the new token produced by Ratelimiter. \r\n\r\nWe already know that the Ratelimiter produces 1 new token per second, which means that the above request still requires the three commands required for above request. The card requires it to wait for 3 seconds.\r\n\r\n#### reserve\r\nImagine a Ratelimiter generated a token per second, and now it is not used (in the initial state). If an expensive request requires 100 token cards. If we choose to let this request wait for 100 seconds before allowing it to execute, this is obviously ridiculous. \r\n\r\nWhy do we do nothing but just wait for 100 seconds? A better approach is to allow this request to execute immediately (no different from all), and then postpone the subsequent request to the right time point. \r\n\r\nWe allow this expensive task to perform immediately and delay the subsequent request for 100 seconds. This strategy is to let the task execute and wait at the same time.\r\n\r\n#### About timetoact\r\nAn important conclusion: Ratelimit does not remember the last request, but the next request allows the time to execute. This can also tell us very straightforwardly that the time interval of reaching the next scheduling time point. \r\n\r\nThe Ratelimiter is also very simple: the next scheduling time has passed. The difference between this time and the current time is how long the Ratelimiter has not been used. We will translate this time into tokens.Limit == 1), and just one request per second, then tokens will not grow.\r\n\r\n#### burst\r\nRatelimiter has a barrel capacity that is directly discarded when the request is greater than the capacity of this barrel.\r\n\r\nhttps://github.com/golang/time/blob/master/rate/rate.go\r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/chrome-%E7%9A%84-lighthouse-%E6%B5%8B%E8%AF%95":{"title":"","content":"[google 官方介绍文档](https://developer.chrome.com/docs/lighthouse/overview/)\r\n\r\n一个 [[chrome]] 内置功能，评判网页性能打开速度等工作。评判很准所以传播广泛","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/desmos":{"title":"desmos","content":"## 摘要\r\n用于可视化数学知识的非常好的工具\r\n\r\n## 主要笔记\r\n-  [Desmos | 免费领略数学之美](https://www.desmos.com/?lang=zh-CN)\r\n- 发现的地方： [An Easier-to-Use Sine Bar / Jacob Rus / Observable](https://observablehq.com/@jrus/sinebar)\r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2021-07-19":{"title":"2021-07-19","content":"\u003cspan \n\t  class='ob-timelines' \n\t  data-date='2021-07-19' \n\t  data-title='入职深信服' \n\t  data-class='orange' \n\t  data-img = '' \n\t  data-type='range' \n\t  data-end='2021-07-19'\u003e \n\t就是这样\n\u003c/span\u003e\n\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-08-09":{"title":"","content":"观察到的好产品经理的共有特质是[[洞察(insight) ]]而在公司里有话语权的产品经理共有特质则是[[雄辩(eloquence)]]这两个特质许多时候是不兼容的，因为说的多了，看和听的时间就少了。 所以大公司产品到后来都做烂，除非顶层负责人还能保持产品初心。 来自 [@天舟](https://www.yuque.com/tianzhou-7jgjb)的分享","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-08-29":{"title":"2022-08-29","content":"[[未来人才的学习目标]]","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-09-27":{"title":"2022-09-27","content":"- [[不要片面的根据一部分想法下决定]]\r\n\r\n\r\n[[做总结和感想的时候需要具体的事例]]\r\n  \r\n\r\n[[CTO 确实同时需要扮演优秀的 engineer]]","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-09-28":{"title":"2022-09-28","content":"\r\n - #todo \r\n\t* \r\n\t* \r\n- #journal \r\n\t- 17:50 离群的特异学习会远离共识，共识通常是正确的，因此稀有的技能选择存在巨大的风险，为了降低风险，我们必须保持谦虚。\r\n\t- 18:10 看到一篇很好的关于选择学习什么技能的文章： https://medium.com/accelerated-intelligence/while-most-people-fight-to-learn-in-demand-skills-smart-people-are-secretly-learning-rare-skills-f9b26856c9d6 学习笔记：[[学习稀缺的技能]]","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-09-29":{"title":"2022-09-29","content":"\r\n - #todo \r\n\t* \r\n\t* \r\n- #journal\r\n- 15:34 添加测试\u003cbr\u003e\u003cbr\u003e\r\n- 20:23 memory 真好用，可以用来吐槽，就是没有发送快捷键\r\n\r\n- 20:25 最近需要思考学点什么东西。把昨天的博客看完吧\r\n- 20:30 学术文章\u003cbr\u003e我领域之外的学科，其他人甚至都不知道\u003cbr\u003e许可专有数据\u003cbr\u003e与可能不会公开分享某些见解的领域内部人士建立深厚的关系\u003cbr\u003e心智模型（难以评估的抽象值）\r\n  这些才是应该在我们的日常学习中去学习的东西，总结的面很到位。因为稀缺性，才有放大的价格。 ^cff4f1","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-09-30":{"title":"2022-09-30","content":"\r\n - #todo \r\n\t* golang devcloud 使用 minio\r\n\t* 学习稀有技能的进一步研究，找一个目标 [[学习稀缺的技能]]\r\n- #journal ^e61eca","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-10-08":{"title":"2022-10-08","content":"\r\n- [[buildkit]] \r\n\t- [高策写过 buildkit 的使用体验](http://gaocegege.com/Blog/kubernetes/buildkit)，在他的 [[envd]] 的工作中 ^42517f\r\n\t- 新 [[Dockerfile]] 语法特性：buildkit 可以通过 docker buildx build 解析新的语法特性。from [[2022-10-08#^42517f]] \r\n\t\t- 多次 build 之间的缓存 `RUN --mount=type=cache,target=/root/.cache/pip pip install ...`\r\n\t\t- 多体系架构的支持 `docker buildx build --platform linux/amd64,linux/arm64 .`\r\n\t\t- 多行脚本 [[Dockerfile 多行脚本]] 只增加一个构建层\r\n- [[rocksdb]] 是单节点 KV 数据库, 设计基于 [[LSMs]] .[[rocksdb]] 是早期 [[Google]] 项目[[LevelDB]] 的一个分支。from  [[为什么我们在RocksDB上创建CockroachDB项目？]]\r\n- [demo 网址](https://postgres-wasm.netlify.app/) [[postgreSQL]] 跑在浏览器里，通过 [[wasm]]\r\n- [[novelai]] 使用 [[stable diffusion]] 生成了大量的二次元图片。repo 库： [github.com/NovelAI/stable-diffusion](https://github.com/NovelAI/stable-diffusion) 用 [[jupyter nodebook]] 写的\r\n- 试用一下 [[maigret]] 一个通过用户 id 收集全网账户报告的工具","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-10-09":{"title":"2022-10-09","content":"尝试使用软件进行思路的梳理，现在是个人时间看板的梳理。[[MnicTime]] 这个软件是可以记录所有的软件的时间的。手机 iPhone 也可以记录软件使用时间。\r\n不能像上面流水线记录，不然跟不上思路。总结然后记录：\r\n\r\n目的：需要找到哪些是可以优化的时间习惯。 通过目前对自我的观察：\r\n1. 各种群聊的聊天，然后发消息，消耗了部分时间\r\n2. 消磨时间的操作，这部分时间应该属于可以优化的。\r\n3. 最后是因为好奇心去看文章的时间\r\n\r\n三类大时间中，首先优化聊天时间，去掉大部分要进入查看的群聊即可，感觉很简单嘛。\r\n\r\n[[使用 quartz 托管 obsidian 到网络上]] \r\n\r\n[[清理 GIt 中的历史文件]]\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-10-10":{"title":"2022-10-10","content":"[[Pricing as Code]] 是[[Tier]]这个产品使用的理念。","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/2022-10-12":{"title":"2022-10-12","content":"今天研究了一下 [[etcd 的性能瓶颈]]。[[etcd]] 的 event 事件太多，影响 etcd 性能，有很多大公司有过分离存储 event 的实践。\r\n通过参数`--etcd-servers-overrides stringSlice`可以将event存储到单独的集群。\r\n\u003e Etcd is used for two distinct cluster purposes - cluster state and event processing. These have different i/o characteristics. It is important to scalability testing efforts that the iops provided by the servers to etcd be consistent and protected. These leads to two requirements:\r\n\u003e  - Split etcd: Two different etcd clusters for events and cluster state (note: this is currently the GKE production default as well).\r\n\u003e  - Dedicated separate IOPS for the etcd clusters on each control plane node. On a bare metal install this could look like a dedicated SSD. This requires a more specific configuration per provider. Config table below\r\n\r\n今天翻以前的[[语雀]]笔记，找到了之前想了解的[[开源项目]]，他们都有巨大的改变或者进步。比如 [[VictoriaMetrics]]已经成为大家都愿意无缝替代 Prometheus 的新软件了。另外是 Code Server 领域中的  sail，之前试用的时候还有 chrome 插件，用起来由于天朝网络问题并不好用。现在也进步成为新项目了。这个项目停止开发了，然而 Coder 项目从中新生，结合 Terraform 描述基础架构的能力，[Coder](https://coder.com) 不仅可以构建一致共享的开发环境，还能通过 code 声明式利用云上和自己的基础架构设施（比如在 Google Cloud 申请一个 ec2 用于开发环境的运行）。当然，基础的能力应该还是 Code Server 提供的。\r\nCoder 首页一个问题我很喜欢，正好问到了我的关注点：当我可以用 Terraform 来配置开发环境的时候为什么还需要 Coder？：Coder 支持开/关调度以降低成本，让您控制您的团队可以配置哪些基础设施值（例如，我只想让我的团队控制 CPU），将您的云开发环境网络连接到您的本地机器等等。\r\n这个笔记是 #DevelopmentEnvironment 主题下的，当时的想法其实是构建学习环境，降低新人培养成本。\r\n\r\n[[slipbox]] \r\n\r\n[[Terraform 配置本地离线源]]\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/diary/README":{"title":"","content":"这是无压力的记录，有些日志在里面，但是价值不大。\n如果有价值，我会记着，或者作为独立文章输出观点。\n\n因为遗忘也是重要的技能。","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/etcd":{"title":"etcd","content":"[[分布式]][[键值数据库]]","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/etcd-%E7%9A%84%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88":{"title":"etcd 的性能瓶颈","content":"[[etcd]] 通常和 [[API Server]] 一起作为整个 [[Kubernetes 的性能瓶颈]]\r\n\r\n\r\n## Refer Link\r\n[蚂蚁集团万级规模 K8s 集群 etcd 高可用建设之路 · SOFAStack](https://www.sofastack.tech/blog/ant-groups-10000-scale-k8s-cluster-etcd-high-availability-construction-road/) 以下摘抄的经验数据\r\n\r\n\t当 K8s 集群规模扩大时，etcd 承载着 KV 数据剧增、event 消息暴增以及消息写放大的三种压力。 为了证明所言非虚，特引用部分数据为佐证：\r\n\t1.  etcd KV 数据量级在 100 万以上；\r\n\t2.  etcd event 数据量在 10 万以上；\r\n\t3.  etcd 读流量压力峰值在 30 万 pqm 以上，其中读 event 在 10k qpm 以上；\r\n\t4.  etcd 写流量压力峰值在 20 万 pqm 以上，其中写 event 在 15k qpm 以上；\r\n\t5.  etcd CPU 经常性飙升到 900% 以上；\r\n\t6.  etcd 内存 RSS 在 60 GiB 以上；\r\n\t7.  etcd 磁盘使用量可达 100 GiB 以上；\r\n\t8.  etcd 自身的 goroutine 数量 9k 以上；\r\n\t9.  etcd 使用的用户态线程达 1.6k 以上；\r\n\t10.  etcd gc 单次耗时常态下可达 15ms。\r\n\t\r\n\t使用 Go 语言实现的 etcd 管理这些数据非常吃力，无论是 CPU、内存、gc、goroutine 数量还是线程使用量，基本上都接近 go runtime 管理能力极限：经常在 CPU profile 中观测到 go runtime 和 gc 占用资源超过 50% 以上。\r\n\r\n\t蚂蚁的 K8s 集群在经历高可用项目维护之前，当集群规模突破 7 千节点规模时，曾出现如下性能瓶颈问题：\r\n\t\r\n\t1.  etcd 出现大量的读写延迟，延迟甚至可达分钟级；\r\n\t2.  kube-apiserver 查询 pods / nodes / configmap / crd 延时很高，导致 etcd oom；\r\n\t3.  etcd list-all pods 时长可达 30 分钟以上；\r\n\t4.  2020 年 etcd 集群曾因 list-all 压力被打垮导致的事故就达好几起；\r\n\t5.  控制器无法及时感知数据变化，如出现 watch 数据延迟可达 30s 以上。","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/etcd-%E7%9A%84%E9%9B%86%E6%88%90%E6%B5%8B%E8%AF%95":{"title":"etcd 的集成测试","content":"## 摘要\r\n[[etcd]]\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  一个 todo 改进：etcd 的检查泄露的 [[goroutine]] 方法需要重构为 [[uber]] 的 [go-leak](https://github.com/uber-go/goleak) \r\n\t- [源码](https://sourcegraph.com/github.com/etcd-io/etcd/-/blob/client/pkg/testutil/leak.go?L19\u0026subtree=true)\r\n- 项目结构：etcd 的集成测试也同样有一个 [/](https://sourcegraph.com/github.com/etcd-io/etcd)[tests /](https://sourcegraph.com/github.com/etcd-io/etcd/-/tree/tests)[framework](https://sourcegraph.com/github.com/etcd-io/etcd/-/tree/tests/framework) 目录下的 integration 文件夹。\r\n- 每次测试初始化使用 [`NewCluster(t testutil.TB, cfg *ClusterConfig) *Cluster`](https://sourcegraph.com/github.com/etcd-io/etcd/-/blob/tests/framework/integration/cluster.go?L117) 启动一个 etcd 集群用于集成测试，集成测试的入口是 [IntegrationTestRunner](https://sourcegraph.com/github.com/etcd-io/etcd/-/blob/tests/framework/framework.go?L24)。\r\n- 在 framework 中有区分[Test Runner](https://sourcegraph.com/github.com/etcd-io/etcd/-/blob/tests/framework/framework.go?L24)：不同的 runner 代表测试场景的不同，有以下区别\r\n\t- UnitTestRunner：专门为单元测试准备的 runner，如果在这种测试入口中创建集群，则会失败（通过 \\*[[testing.M]] 控制的）\r\n\t- E2eTestRunner：E2E 测试会真实运行 etcd 和 etcdctl 的二进制文件作为两个进程，用于模拟真实用户的操作来测试。\r\n\t- IntegrationTestRunner 通过使用内部库的代码，在多个 goroutine 上创建 etcd 集群，通信方式是 client 库。以此运行集成测试。\r\n- 集成测试触发方式：Makefile，运行 `make test-integration` 触发\t`PASSES=\"integration\" ./scripts/test.sh $(GO_TEST_FLAGS)`\r\n\t- 无独有偶：etcd 的实际执行也在自己写的脚本里[/](https://sourcegraph.com/github.com/etcd-io/etcd)[scripts /](https://sourcegraph.com/github.com/etcd-io/etcd/-/tree/scripts)[test.sh](https://sourcegraph.com/github.com/etcd-io/etcd/-/blob/scripts/test.sh)，经过阅读 shell 判断，脚本中主要做了各种自定义参数和环境变量的传递，相当于原版 go test 的增强，\r\n\t\t- 比如添加了三个测试模式可以选择\r\n\t\t\t- 并行 parallel\r\n\t\t\t- 报错马上退出 keep_going\r\n\t\t\t- 全部运行最后收集报错 fail_fast\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/events/README":{"title":"","content":"记录我过去的 events。\n\n```timeline\nmyevent\n```\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/find-%E5%8A%A0-xargs-%E4%BD%BF%E7%94%A8":{"title":"","content":"\n`find . -name \"install.log\" -print | xargs cat`","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/fuzzing-%E6%B5%8B%E8%AF%95":{"title":"fuzzing 测试","content":"## 摘要\r\nFuzzing（模糊测试）是一种通过压力测试的形式来自动化软件测试过程的技术。关键思想是编写一个类似于单元测试或集成测试的 fuzzing 工具，它将使用一些任意的输入来执行被测试的应用程序。\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/git%E5%A6%82%E4%BD%95%E6%9F%A5%E6%89%BE%E5%B7%B2%E7%BB%8F%E8%A2%AB%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E7%9A%84%E5%8E%86%E5%8F%B2%E4%BF%AE%E6%94%B9%E8%AE%B0%E5%BD%95":{"title":"git如何查找已经被删除文件的历史修改记录？","content":"## 摘要\r\n```bash\r\ngit log --all --full-history -- \u003cpath-to-file\u003e\r\n```\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/git-%E9%85%8D%E7%BD%AE%E5%A4%9A%E4%B8%AA%E6%8F%90%E4%BA%A4%E7%94%A8%E6%88%B7%E8%87%AA%E5%8A%A8%E5%8C%B9%E9%85%8D%E5%88%87%E6%8D%A2":{"title":"git 配置多个提交用户自动匹配切换","content":"## 摘要\nfrom：[如何配置多个提交用户？ - Git 进阶指南](https://gb.yekai.net/questions/git-config-user)\nConditional Includes\n\n在 git 2.13 版本中，增加了 [conditional includes](https://git-scm.com/docs/git-config#_includes) 配置，可以创建多个 gitconfig 文件，并针对不同的根目录使用不同的配置文件。例如，以下全局配置文件 `~/.gitconfig` 中包含以下用户配置信息，当项目 clone 在 `~/dev/` 目录下时，会自动使用另外一份配置文件：\n\n```\n[user]\n\nname = Your Name\n\nemail = your_email@example.com\n\n[includeIf \"gitdir:~/dev/\"]\n\npath = .gitconfig-dev\n```\n\n以下是 `~/.gitconfig-dev` 文件的配置：\n```\n[user]\n\nname = Another Name\n\nemail = another_email@example.com\n\n```","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/github-%E4%B8%8A%E4%BC%A0-ssh-key-%E5%90%8E%E4%BB%8D%E9%A1%BB%E8%BE%93%E5%85%A5%E5%AF%86%E7%A0%81":{"title":"github 上传 ssh-key 后仍须输入密码","content":"## 摘要\r\n`.git/config` 文件写明了通过 http 协议，没有走 ssh\r\n`url = https://github.com/Name/project.git`\r\n改为\r\n`url = git@github.com:Name/project.git`\r\n\r\n只想走 ssh 协议。from：[Git - git-config Documentation](https://git-scm.com/docs/git-config#Documentation/git-config.txt-urlltbasegtinsteadOf)\r\n```bash\r\ngit config --global url.ssh://git@github.com/.insteadOf https://github.com/\r\n```\r\n\r\n#### 测试连通性\r\n```bash\r\nssh -T git@github.com\r\n```\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/golang-timer-%E8%A7%A3%E6%9E%90":{"title":"golang timer 解析","content":"网址： https://tonybai.com/2016/12/21/how-to-use-timer-reset-in-golang-correctly/\r\n\r\n近期[gopheracademy blog](https://blog.gopheracademy.com/)发布了一篇 《[How Do They Do It: Timers in Go](https://blog.gopheracademy.com/advent-2016/go-timers)》，通过对timer源码的分析，讲述了timer的原理，大家可以看看。\r\n\r\ngo runtime 实际上仅仅是启动了一个单独的 [[goroutine]]，运行 timerproc函数，维护了一个”[[最小堆]]”，定期wake up后，读取堆顶的timer，执行timer对应的f函数，并移除该timer element。创建一个Timer实则就是在这个最小堆中添加一个element，Stop一个timer，则是从堆中删除对应的element。","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/golang-timer-reset-%E9%97%AE%E9%A2%98":{"title":"golang timer reset 问题","content":"## 摘要\r\n定时器创建后是单独运行的，超时后会向通道写入数据，你从通道中把数据读走。**当前一次的超时数据没有被读取，而设置了新的定时器，然后去通道读数据，结果读到的是上次超时的超时事件，看似成功，实则失败**\r\n\r\n所以使用 Reset 的时候，\r\n```go\r\nif !t.Stop() {\r\n\t\u003c-t.C \r\n} \r\nt.Reset(d)\r\n```\r\n然而这种方式会遇到问题 [[time: Timer.Reset is not possible to use correctly #14038]]，即如果计时器已经过期，且 channel 被取走数据，这个时候不要先调用 Stop()。\r\n\r\n如果通过变量标明是否取出了数据，又会有竞态的问题，比如 goroutine 顺序完全依赖于调度器调度。\r\n综上，没有 Reset() 完全理想的正确使用方式。\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  Refer： [论golang Timer Reset方法使用的正确姿势 | Tony Bai](https://tonybai.com/2016/12/21/how-to-use-timer-reset-in-golang-correctly/)\r\n- Golang官方有关Timer的issue list：\r\n\t- runtime: special case timer channels #8898  \r\n\t- time:timer stop ,how to use? #14947  \r\n\t- time: document proper usage of Timer.Stop #14383  \r\n\t- *time: Timer.Reset is not possible to use correctly #14038  \r\n\t- Time.After doesn’t release memory #15781  \r\n\t- runtime: timerproc does not get to run under load #15706  \r\n\t- time: time.After uses memory until duration times out #15698  \r\n\t- time:timer stop panic #14946  \r\n\t- *time: Timer.C can still trigger even after Timer.Reset is called #11513  \r\n\t- time: Timer.Stop documentation incorrect for Timer returned by AfterFunc #17600\r\n\r\n","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/hugo-extended":{"title":"hugo-extended","content":"\r\nhugo 安装网址：[https://gohugo.io/getting-started/installing/](https://gohugo.io/getting-started/installing/)\r\n\r\nwindows 我直接使用了安装\r\n```bash\r\nscoop install hugo-extended\r\n```","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/hugo-obsidian":{"title":"hugo-obsidian","content":"使用 [[Golang]] install 功能安装\r\n\r\n```\r\n# Install and link `hugo-obsidian` locally\r\ngo install github.com/jackyzha0/hugo-obsidian@latest\r\n```","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/kube-vip-%E7%9A%84%E4%B8%93%E9%97%A8%E7%9A%84%E6%84%9F%E8%B0%A2-issue":{"title":"kube-vip 的专门的感谢 issue","content":"#开源趣闻\r\nhttps://github.com/kube-vip/kube-vip/issues/239\r\n\r\n# Not an issue, just a thank you #239 \r\n\r\n [Closed](https://github.com/kube-vip/kube-vip/issues/239#event-7371032974)\r\n\r\nopened this issue 4 comments \r\n\r\n## Comments\r\n\r\n[![@kylos101](https://avatars.githubusercontent.com/u/1272076?s=80\u0026u=3a1662f1bacd974dedcf0117fd84193b6d0feb37\u0026v=4)](https://github.com/kylos101)\r\n\r\n### **[kylos101](https://github.com/kylos101)** [on 11 Jul 2021](https://github.com/kube-vip/kube-vip/issues/239#issue-941363311)\r\n\r\nThis is amazing, I just got it setup, and cannot believe it was that easy to get working!\r\n\r\nI setup my Traefik ingress to use the kube-vip IP address as its external IP, on my bare-metal cluster at home.\r\n\r\nThank you, `kube-vip` team. Y'all rock!","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/kubeadm-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B":{"title":"kubeadm 初始化流程","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n- 代码位置： 在 Kubernetes 源码包里`kubernetes/cmd/kubeadm/app`，kubelet 这个 repo 只是用来做聚合 issues 用的。\r\n- [sourcegraph 代码位置](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kubeadm/app/cmd/init.go)\r\n- kubeadm init 由一系列的 workflow 脚本组成，[截取如下](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kubeadm/app/cmd/init.go?L141)：\r\n\t ```go\r\n\t// initialize the workflow runner with the list of phases\r\n\tinitRunner.AppendPhase(phases.NewPreflightPhase())\r\n\tinitRunner.AppendPhase(phases.NewCertsPhase())\r\n\tinitRunner.AppendPhase(phases.NewKubeConfigPhase())\r\n\t```\r\n\t- 每一个 Phase 内部都是一系列的 Golang 脚本，通过面向对象的设计，用包的调用操作函数。\r\n\t\t- 当然，底层的脚本调用通过[接口实现](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kubeadm/app/util/initsystem/initsystem.go?L17)的方式，兼容了不同系统的调用启动，比如 [[Unix]] 系统上有 [[systemctl]] 就用，或者用 [[openrc]]。\r\n\t\t- kubeadm 自己写了一个遍历添加脚本的代码用作工作流调度：[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[cmd /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd)[kubeadm /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm)[app /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm/app)[cmd /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm/app/cmd)[phases /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm/app/cmd/phases)[workflow /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm/app/cmd/phases/workflow)[runner.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kubeadm/app/cmd/phases/workflow/runner.go)\r\n- [官方文档对于实现细节的展示](https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/implementation-details/#core-design-principles)","lastmodified":"2022-12-15T12:39:01.348929429Z","tags":null},"/kubernetes-%E6%B3%A8%E9%87%8A-taints-%E8%84%9A%E6%9C%AC":{"title":"","content":"单节点 [[Kubernetes]] 需要注释一下 [[NoSchedule]] 的污点（[[taints]]），让 [[Pod]] 可以调度到节点上。\r\n查询方式：\r\n```bash\r\nkubectl get no -o yaml | grep taint -A 5\r\n```\r\n删除方式\r\n```bash\r\nkubectl taint nodes --all node-role.kubernetes.io/control-plane-\r\n```\r\n\r\n[总所周知的标签、注解和污点](https://kubernetes.io/zh-cn/docs/reference/labels-annotations-taints/)","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/lvm":{"title":"lvm","content":"## 摘要\r\n\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n- [LVM逻辑卷------基础命令详解（三分钟入门）](https://blog.csdn.net/huhaiyangFIVE/article/details/109368379) 左边这个更好 [系统运维|Linux LVM简明教程](https://linux.cn/article-3218-1.html)\r\n- [[LVM 属性含义]]\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/mac-%E8%AE%BE%E7%BD%AE%E6%B0%B8%E4%B9%85%E8%B7%AF%E7%94%B1%E5%90%8C%E6%97%B6%E8%AE%BF%E9%97%AE%E5%86%85%E5%A4%96%E7%BD%91":{"title":"mac 设置永久路由，同时访问内外网","content":"\n[[2022-12-15]] 今天想起来以前设置了路由没有记笔记：\n\nmac 添加永久路由的方法\n1. 查看网卡名称列表 `networksetup -listallnetworkservices`\n2. 设置路由：`networksetup -setadditionalroutes \"AX1862\" 172.11.0.0 255.255.255.0 172.16.198.1 192.160.0.0 255.255.255.0 172.16.198.1` 代表为指定网卡添加路由，网卡名称由第一条命令查询，依次是目的 IP，子关掩码，目标网关（通常是你的路由器的地址，可以从自己的 IPV4 连接状态中获取）。\n3. 查看添加的路由 `networksetup -getadditionalroutes \"AX1862\"`\n4. 清空路由表 `networksetup -setadditionalroutes \"AX1862\"`\n\n常规方法就是使用 route 命令。\n\n需要注意，mac 上设置默认网卡的方式是通过打开网络连接设置，然后通过调整多个网络之间的顺序来完成的。\n\n所以要达到内外网同时访问的目的，可以设置内网的特定网卡路由，并把外网的网卡顺序置顶。","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/maigret":{"title":"maigret","content":"https://github.com/soxoj/maigret","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/namespace":{"title":"namespace","content":"Kubernetes 中的命名空间概念，可参考 [[Linux Namespace]]","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/nginx-%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6":{"title":"","content":"```nginx\r\nserver {\r\n  listen 80;\r\n  # gzip config\r\n  gzip on;\r\n  gzip_min_length 1k;\r\n  gzip_comp_level 9;\r\n  gzip_types text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml;\r\n  gzip_vary on;\r\n  gzip_disable \"MSIE [1-6]\\.\";\r\n\r\n  root /usr/share/nginx/html;\r\n\r\n  server_name  {{domain}};\r\n\r\n  location / {\r\n    try_files $uri $uri/ /index.html;\r\n  }\r\n  location /api {\r\n    proxy_pass http://{{ip:port}}/;\r\n    proxy_set_header   X-Forwarded-Proto $scheme;\r\n    proxy_set_header   Host              $http_host;\r\n    proxy_set_header   X-Real-IP         $remote_addr;\r\n  }\r\n}\r\n# server {\r\n#   # 如果有资源，建议使用 https + http2，配合按需加载可以获得更好的体验\r\n#   listen 443 ssl http2 default_server;\r\n#   # \u0008证书的公私钥\r\n#   ssl_certificate /path/to/public.crt;\r\n#   ssl_certificate_key /path/to/private.key;\r\n#   location / {\r\n#         # 用于配合 browserHistory使用\r\n#         try_files $uri $uri/ /index.html;\r\n#   }\r\n#   location /api {\r\n#       proxy_pass https://preview.pro.ant.design;\r\n#       proxy_set_header   X-Forwarded-Proto $scheme;\r\n#       proxy_set_header   Host              $http_host;\r\n#       proxy_set_header   X-Real-IP         $remote_addr;\r\n#   }\r\n# }\r\n```","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/paxos-%E6%8A%BD%E8%B1%A1%E7%90%86%E8%A7%A3":{"title":"paxos 抽象理解","content":"## 摘要\r\n[[两阶段锁]]+[[WOR]]\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n- [博客访问地址](https://maheshba.bitbucket.io/blog/2021/11/15/Paxos.html) \r\n- [[Paxos]] 使用 [[Quorum 机制]] 实现了 [[WOR]]\r\n\t- 这是因为 WOR 的单个 Server 实现，（数据存储在内存或者磁盘）没法持久化数据，必须跨服务器复制才能保证。然而我们对跨服务器的情况也要假设少数服务器会爆炸（比如爆炸导致该服务器数据不可恢复）所以需要 [[Quorum 机制]] 来进行一个多数写入的保证，为了容忍 F 台服务器爆炸，Paxos 需要 2F+1 台服务器。\r\n- 使用[[两阶段锁]]定来实现并发控制\r\n\t- 以上问题完成后，引入新的问题：多个客户端同时写入一个 [[Quorum 机制|Quorum]] 的时候，如果不互斥，则最终会在不同的少数服务器上获取到不同的值。（并发写入问题！哈。）\r\n\t- 锁的主要问题在于\r\n\t\t1. 可能会遇到[[死锁]]，以[[严格的顺序]]获取锁以防止死锁会增加延迟。\r\n\t\t2. [[分布式系统]]中的锁有一种新的故障模式：客户端在获取锁后可能会[[崩溃]]。\r\n\t- [[Paxos]] 通过一种锁窃取形式为这两个问题提供了解决方案。锁有版本或编号；编号较大的锁可以覆盖编号较小的锁。客户将选择一个唯一的锁号；然后尝试用它锁定一个仲裁。如果 acceptor 被解锁，或者被锁定为较小的数字，则获取锁成功；如果接受器被更高的锁号锁定，则失败（在这种情况下，锁定客户端可以使用更高的锁号重试）。锁不是建议性的；写入基于锁号，如果锁被盗，将在接受者处失败。\r\n- _完成写入_：\r\n\t- 回想一下，我们假设少数服务器会爆炸。如果客户端锁定了大多数服务器；无法访问剩余的少数；并找到一个值已经写入该多数的单个接受器，它必须假设该值也写入不可访问的少数并确认回一些旧客户端。因此，前进的唯一途径是让新客户端采用该值作为自己的值并将其写入它可以访问的多数。如果存在多个这样的值，则客户端必须选择具有最高关联锁号的值。\r\n- _Livelock_：\r\n\t- 如果两个客户端不断地互相窃取锁，显然上面的协议可以活锁。这个问题在理论上是不可能解决的：[[FLP 不可能性]] 结果（早于 Paxos 协议）表明，容错共识不能同时具有活跃性和安全性。Paxos 中的 Livelock 是 FLP 结果的实际示例。\r\n- _不同的锁定/写入仲裁_：\r\n\t- 事实证明，您可以锁定一些多数仲裁并写入另一个多数仲裁；两个阶段的法定人数不必相同。（但是对未锁定的接受者的写入必须被解释为先锁定后写入，否则会出现[此错误](https://stackoverflow.com/questions/29880949/contradiction-in-lamports-paxos-made-simple-paper)）。Flexible Paxos 进一步指出，如果可以接受较低的持久性，则写入仲裁不一定必须是多数。\r\n- _预锁定_：\r\n\t- 客户端可以在写入值时预先锁定仲裁以避免往返。为了提供对锁定的细粒度控制，我们可以显式地向 WOR 公开一个名为 lock() 的额外 API。如果客户端正在与多个 WOR 交互，而这些 WOR 恰好存在于同一组接受器上，我们可以预先锁定整批 WOR。事实证明，预锁定涵盖了 MultiPaxos 中的关键优化，我们将在后面讨论。\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/slipbox":{"title":"","content":"原来就是卢曼笔记啊","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/terraform-minio":{"title":"","content":"结合 devcloud 和之前的 minio 中间件思考一下问题：如何集成使用？\r\n\r\n\r\n首先描述 [Minio Provier](https://registry.terraform.io/providers/refaktory/minio/latest/docs)， [[MInio]] 是需要自己部署的，[[Terraform]] minio 这个 [[terraform provider|provicer]]只是提供了配置选项。不过我们使用基础设施其实也就是配置了。没有启动功能用什么启动呢？\r\n1. 自己本地启动\r\n2. 通过 terraform 的其他云 provider 在云上申请一个这样的资源，比如 [AWS Terraform Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)\r\n\r\n当然，由于 [[terraform]] 的 [[terraform provider]] 编写比较简单，其实也可以自己编辑一个以 [[docker]] 作为 [[infrastructure]] 的包含 minio 启动流程的 provider 啦。比如组合 docker provider 声明一个  minio 镜像的 [[container]] 就好啦。\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/testing.M":{"title":"testing.M","content":"## 摘要\r\n[Why use TestMain for testing in Go? - Medium](https://medium.com/goingogo/why-use-testmain-for-testing-in-go-dafb52b406bc)\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n-  \r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/timeline-%E6%8F%92%E4%BB%B6%E4%BD%BF%E7%94%A8":{"title":"","content":"### 添加事件实例\n\u003cspan \n\t  class='ob-timelines' \n\t  data-date='{{date}}' \n\t  data-title='' \n\t  data-class='orange' \n\t  data-img = '' \n\t  data-type='range' \n\t  data-end='{{date}}'\u003e \n\t{{title}}\n\u003c/span\u003e\n\n注意 tags 需要同时满足要求并且有 timeline 标识的 markdown 文件才会被搜索上述 span 并生成 timeline。\n### 展示示例\n```obsidian\n```timeline\nmyevent\n```\n### 垂直可以这样设置\n```obsidian\n```timeline-vis\ntags=2\nstartDate=2020\nendDate=2023\nfivHeight=8\nminDate=10\n```\n\n### 静态页面\n生成静态 html 页面这样设置，然后使用 `Timelines: Render Timeline` 命令生成。\n```html\n\u003c!--TIMELINE BEGIN tags='test;now'--\u003e\u003c!--TIMELINE END--\u003e\n```","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/vim-%E6%9B%BF%E6%8D%A2%E6%89%80%E6%9C%89%E8%A1%8C%E7%9A%84%E5%86%85%E5%AE%B9":{"title":"vim 替换所有行的内容","content":"## 摘要\r\n替换所有行的内容：      `:%s/from/to/g`\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/wagyu":{"title":"wagyu","content":"#github\r\n- Intro::: A [[Rust]] library for generating cryptocurrency [[区块链钱包|wallet]]\r\n- Link::: https://github.com/AleoHQ/wagyu\r\n\r\n[[Rust 学习]]  [[区块链钱包]]\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/web-%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B":{"title":"web 技术发展历程","content":"## 摘要\r\n\r\n## 问题、提示\r\n-  \r\n\r\n## 主要笔记\r\n- https://www.epicweb.dev/the-webs-next-transition ：具有详尽的关于 Web 技术发展的经验和论证优缺点。以及整个 web 历史的架构，都画了图说明。 能写出来需要优秀的经验和总结能力，这篇文章写的很好。毕竟 [[Focus and Different]]\r\n\t- 其中提到 [[SPA]] 的[[状态管理]]是一个大问题，按我的经验来说也是，[[Redux]] 等状态管理的框架，[[UI]] 的[[单向渲染逻辑]]，都是状态管理的大问题。当然核心问题就是[[缓存失效]]！毕竟缓[[存失效是软件中最困难的问题之一]]。\r\n\t- 提到了框架 [[Remix]]，找机会学学了解下 #todo  \r\n\t\t- https://twitter.com/shamwhoah/status/1575619809714503681 : 还能快速在 [[chrome 的 lighthouse 测试]]中获得很高分！\r\n\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/windows-%E5%BC%80%E5%90%AF%E8%87%AA%E5%8A%A8%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC":{"title":"windows 开启自动执行脚本","content":"## 摘要\r\n前者是针对某个账户的开机启动，账户名没改默认是Administrator，后者是针对所有账户的启动：\r\n放在指定路径即可：\r\n- `C:\\Users\\你的账户名\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup`\r\n- `C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\StartUp`","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null},"/windows-%E8%87%AA%E5%8A%A8%E4%BB%A5%E7%AE%A1%E7%90%86%E5%91%98%E6%9D%83%E9%99%90%E8%BF%90%E8%A1%8C-bat-%E8%84%9A%E6%9C%AC":{"title":"windows 自动以管理员权限运行 bat 脚本","content":"```bat\r\n@echo off\r\n\u003enul 2\u003e\u00261 \"%SYSTEMROOT%\\system32\\cacls.exe\" \"%SYSTEMROOT%\\system32\\config\\system\"\r\nif '%errorlevel%' NEQ '0' (\r\ngoto UACPrompt\r\n) else ( goto gotAdmin )\r\n:UACPrompt\r\necho Set UAC = CreateObject^(\"Shell.Application\"^) \u003e \"%temp%\\getadmin.vbs\"\r\necho UAC.ShellExecute \"%~s0\", \"\", \"\", \"runas\", 1 \u003e\u003e \"%temp%\\getadmin.vbs\"\r\n\"%temp%\\getadmin.vbs\"\r\nexit /B\r\n:gotAdmin\r\nif exist \"%temp%\\getadmin.vbs\" ( del \"%temp%\\getadmin.vbs\" )\r\npushd \"%CD%\"\r\nCD /D \"%~dp0\"\r\n```\r\n\r\n使用方法：最下面接写的批处理即可，以下以[[自动内外网路由 bat 脚本]]举例：\r\n","lastmodified":"2022-12-15T12:39:01.352929427Z","tags":null}}