{"/":{"title":"🎁首页","content":"Obsidian + Quartz Publish Web HomePage\n\nHello👋，本页面提供了一个网络访问我的笔记的途径，使用 Hugo 搭建（Quartz），笔记使用 Obsidian 编写。\n\n## 双链的阅读建议\n- 通过鼠标悬浮预览进行上下文不中断的阅读。\n- 通过底部图和双向链接找到更多感兴趣的。\n- 想查找直接使用搜索按钮\n\n也可以查看这个导航：[标签](/tags/)\n\n\n目前的写作工作流：\n### 写\n在任意地方都可以写，打开一个 obsidian 目标文件夹即可。\n\n### 发布到网页 notes.abser.top\n1. 通过 abserari/quartz 这个 repo，直接使用 obsidian-git 插件，push 到 quartz 库中。\n2. 最后通过 git 提交到 github 触发 action 自动构建\n3. 构建使用 [[hugo-extended]]  [[hugo-obsidian]] 工具\n\n### 同步功能\n然后使用 [remotely save](https://github.com/remotely-save/remotely-save) 插件同步，\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AD%A6%E4%B9%A0%E7%A8%80%E7%BC%BA%E7%9A%84%E6%8A%80%E8%83%BD":{"title":"为什么学习稀缺的技能","content":"\u003e _应用异常值算法的困难正是这样做的原因！更困难意味着更稀有。_\n\n为了帮助您开始应对这些挑战，以下是我一遍又一遍地回顾的四个经验法则。虽然只有四个，但它们的冲击力很大。它们是数百小时思考如何更好地找到稀有和有价值的技能的简化：\n\n1.  **成为第一个学习有价值的技能的人。** 跟踪新兴科学、技术、应用程序、工具、行业和领域。当它们呈指数增长时，花几个小时探索它们，看看是否值得投入更多时间。它可以帮助您尝试新事物，而不会将时间浪费在变成无用的事物上。\n2.  **学习很难的有价值的技能。** 愿意投资于禁忌、不是超级性感的、耗时、费力、看起来有风险或超级技术或学术的领域。\n3.  **学习具有隐藏好处的宝贵技能。** 人类有价值盲点。他们低估了具有抽象、长期回报的技能；微技能；其他学科的技能；和被遗忘的经典思想。\n4.  **重新定义价值比共识更好。** 在我们的职业生涯中，我们利用我们的技能为他人服务。可能是客户、老板、我们的粉丝、招聘人员或其他人。如果您能够比其他人更好地了解您所服务的人的未满足需求，您将能够更好地满足这些需求。\n\n或者，简而言之，请记住：\n\n-   成为第一\n-   面对困难\n-   寻找隐藏的好处\n-   重新定义价值\n\n以上是 Michael Simomons 的学习稀有技能的理论，其中总结的几点我们应该尝试的规则，在他的规则下，他指导有如下学习资源（随个人的情景可以有变种，这是当然的）\n![[2022-09-29#^cff4f1]] \n\n## 第一步，解释：我们现在从规则开始解释，从而找到我们自己情景下的学习\n1. 首先，所有的技能都很有价值，然后我们要去寻找稀缺性的技能。这个前提是我们先找到有价值的技能，通过一个指数增长的模型判断一个事物发展是否超出常规，这通常都代表这件事有价值。那我们以程序员的视角举例， Web3 正好是急需技能之一（因为需求而有价值），物联网，边缘方面的技能也并驾齐驱。\n2. 然后我们去寻找技能的**稀缺性**！（因为稀缺性决定价格）这也有一个模型来描述他，即被禁止的，未被大部分人发现的，可能枯燥的，耗时，费力的，或者看起来非常有风险的，以及非常学术的，非常技术专业的领域。（毕竟你不能指望一蹴而就的领域有多稀缺，信息差是很难一直保持的）\n3. 同时不要忽视长期的**被动技能**，复利主义，以及一些经典思想，他们由于长期存在于我们的身边，会被我们的价值评估器忽视。以抽象的，长期回报的技能为例：写笔记，坚持总结输出；微技能，一些工作上的小技巧，比如电脑使用领域上的一些工具快捷键；**其他领域的技能！** 往往会产生巨大的**化学反应**，比如生物对计算机领域的影响，当然我还是建议学数学，比如金融割韭菜融入区块链；以及一些经典思想：分治和中间层的思想贯彻整个计算机科学，经常发现计算机的一些新领域应用了老的经典的思想从而大放异彩，比如深度学习啦！\n4. 重新定义价值而非共识，这句类比对一个行业需求的深挖。技能可以为每一个使用者提供相同影响的结果，但精准的需求分析和挖掘，能让某项技能发掘出更多的价值，他提升了价值的同时也提升了稀缺性。\n\n当然，以上规则其实主要是个人视角出发，如何发挥个人最大的影响力的思考，从团队等方向还能有更多，比如规模效应，然而我们希望将讨论范围限制在个人视角上以期提供一些切实的指导。\n\n\t道理的扩散成本是非常低的，但是让一个人相信这个道理的成本是极高的。\n\n## 第二步 分析学习资源的新变化\n| 考虑稀缺性前                                 | 后                                                           |\n| -------------------------------------------- | ------------------------------------------------------------ |\n| 阅读最新的畅销书                             | 学术论文                                                     |\n| 时刻检查社交媒体查看最具影响力的人的发言     | 领域之外的学科（加密学、经济学）                             |\n| 时刻保持关注行业最新的新闻（所有人都在关注） | 获取一些专有数据的授权并研究                                 |\n|                                              | 和领域内部人士建立深厚的关系（并且他们很少公开分享自己的观点 |\n|                                              | 一些抽象的心理模型                                                             |\n\n可以看到其中明显的差距，同样的例子还有很多，就不一一列举了。笔者这里最想强调的还是稀缺代表远离共识，意味着你的选择几乎总是错的（即使你是个顶尖聪明的人），这是需要谦虚的态度，避免一次投入过多，需要有足够多的证据、验证。所以保持谦虚，离群才能尽量保证个人的稳定。\n\n以程序员的视角来讲，可以先有这几条经验：\n1. 不需要看书，书几乎总是过时的，不稀缺的，有需求的技能学习途径或者基础，尽量看学术论文。\n2. 避免关注颇具影响力的人的一言一行，避免一直关注，要么深挖，要么略过。\n3. 多接触一些其他领域的学识，当前行业的其他领域：安全、底层、抽象论证，其他行业的各个领域：哲学（游戏理论）、数学、经济学、等等。\n4. 专有数据并不一定需要授权，也需要自己收集，当你的一些独特视角需要验证时，可以轻松通过编程（最好合法）爬取需要的信息分析验证（如果有一些微技能比如数据分析，会让这个过程更顺理成章）\n5. 不用关注网红，而是转而建立深厚的个人关系，这其实在个人认知（资源等）不足的情况下的最好选择，请寻找自己身边的这种人，如果接触不到，或许你应该换一个环境。\n6. 保持写笔记的习惯，持续锻炼自己的心理模型，这个过于抽象，不过当你阅读这篇笔记的时候，其实就在做这样的事。延续它！\n\n## 下一步，选择\n\n现在，实践一下这个稀缺性的模型，找到适合你的技能，至少不盲目的学习了是吗？","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E5%9C%A8RocksDB%E4%B8%8A%E5%88%9B%E5%BB%BACockroachDB%E9%A1%B9%E7%9B%AE":{"title":"为什么我们在RocksDB上创建CockroachDB项目？","content":"https://www.cockroachlabs.com/blog/cockroachdb-on-rocksd/","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/%E4%BD%BF%E7%94%A8-quartz-%E6%89%98%E7%AE%A1-obsidian-%E5%88%B0%E7%BD%91%E7%BB%9C%E4%B8%8A":{"title":"使用 quartz 托管 obsidian 到网络上","content":"## Refer\nhttps://quartz.jzhao.xyz/notes/setup/\nhttps://quartz.jzhao.xyz/notes/obsidian/","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/%E6%97%85%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8":{"title":"旅行的本质","content":"用异化思想去思考：\n旅行的本质是\n-   个人远离他们文化的真实道路\n-   深入未知的深处重塑自我\n-   然后将这种学习带回他们的文化，这样它就可以发展","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/%E6%B8%85%E7%90%86-GIt-%E4%B8%AD%E7%9A%84%E5%8E%86%E5%8F%B2%E6%96%87%E4%BB%B6":{"title":"清理 GIt 中的历史文件","content":"```\ngit filter-branch --force --index-filter 'git rm --cached --ignore-unmatch path-to-your-remove-file' --prune-empty --tag-name-filter cat -- --all\n```\n\n其中, path-to-your-remove-file 就是你要删除的文件的相对路径(相对于git仓库的跟目录), 替换成你要删除的文件即可. 注意一点，这里的文件或文件夹，都不能以 '/' 开头，否则文件或文件夹会被认为是从 git 的安装目录开始。\n\n如果你要删除的目标不是文件，而是文件夹，那么请在 `git rm --cached` 命令后面添加 -r 命令，表示递归的删除（子）文件夹和文件夹下的文件，类似于 `rm -rf` 命令。\n\n更多请参考：[ https://help.github.com/articles/remove-sensitive-data](https://help.github.com/articles/remove-sensitive-data)","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/%E8%8B%B1%E9%9B%84%E4%B9%8B%E6%97%85":{"title":"英雄之旅","content":"\n英雄之旅就是人类社会中的最常见的原型神话。\n\n![[Pasted image 20220929203142.png]]![[Pasted image 20220929203529.png]]\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/%E9%80%90%E6%AD%A5%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%AA%E6%96%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%BC%8F":{"title":"逐步描述一个新模型的方式","content":"通过描述一个东西，能提供什么，能做什么来感知他。\n\n不要急着下定义，这会让自己丧失求知欲。\n\n有点像编程领域的[[鸭子类型]]的延伸。","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B":{"title":"鸭子类型","content":"同 Duck-Type\n\n原理是：只要一个东西会“嘎嘎”叫，就认为它是一只鸭子。\n\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/Dockerfile-%E5%A4%9A%E8%A1%8C%E8%84%9A%E6%9C%AC":{"title":"Dockerfile 多行脚本","content":"```\n# syntax = docker/dockerfile:1.4\nFROM debian \n-RUN apt-get \u0026\u0026 \\ \n- apt-get install -y vim \n +RUN \u003c\u003ceot bash \n+ apt-get update \n+ apt-get install -y vim \neot\n```","lastmodified":"2022-10-11T11:25:57.630335428Z","tags":null},"/Golang":{"title":"","content":"","lastmodified":"2022-10-11T11:25:57.630335428Z","tags":null},"/LSMs":{"title":"LSMs","content":"基于日志结构的合并树\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/LVM-%E9%87%8D%E5%90%AF%E9%9C%80%E8%A6%81%E9%87%8D%E6%96%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E5%90%A6%E5%88%99%E4%B8%A2%E5%A4%B1-vg-%E8%AE%BE%E5%A4%87%E7%9A%84%E9%97%AE%E9%A2%98":{"title":"LVM 重启需要重新初始化否则丢失 vg 设备的问题","content":"https://www.aboutyun.com/thread-16065-1-1.html\n\n设置  开机挂载 ，ok了  \n  \ncat /etc/rc.d/rc.local |grep cinder-volumes || echo 'losetup -f /var/lib/cinder/cinder-volumes \u0026\u0026 vgchange -a y cinder-volumes ' \u003e\u003e /etc/rc.d/rc.local\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/PriceOps":{"title":"PriceOps","content":"什么是 PriceOps ?\n解释网址： https://priceops.org/\n\n做一下笔记：\n\n\tPriceOps 部分定义\n\t\n\tPriceOps 是一种实现迭代和灵活性的方法。它描述了一组通过有效管理固有复杂性来促进定价模型探索的实现属性。将此视为一组架构蓝图和最佳实践，可以帮助您持续开发和完善您的定价基础架构。\n\t\n\tPriceOps_不是_关于任何特定产品应如何定价或如何确定此类价格的规定性指南。相反，它是如何实施定价模型以最大限度地提高灵活性和稳定性的指南。\n\n我理解如下：\n\n将定价模型通过代码描述出来，现在是 json，因为有 code 极强的表达能力，所以可能可以从中看出定价对于产品的影响有哪些。从而做出调整。\n\n列出了支撑模型的五个支柱（英文：[[pillar]])\n\n1. 定价模型的定义：例如版本化代表不会影响之前的用户\n2. 用户的时间表：这样一个定价计划就可以规定在哪些时间里面可以使用哪些功能，并能有多少使用量\n3. 计量系统：用于收集所有用户使用信息，一个数据中心中存储，来帮助定价模型的更新\n4. 权限检查：这样应用程序代码只需要提供功能，不需要知道功能在哪些计划中。\n5. PriceOps 工具：为以上行为提供操作的工具\n\n我很喜欢这种 [[逐步描述一个新模型的方式]]，它还能持续迭代一个东西定义，如果一开始就下结论，这个模型就老死了。","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/Pricing-as-Code":{"title":"Pricing as Code","content":"一种很新的东西.\n概念模型: [[PriceOps]]\n示例产品：[[Tier]]  https://www.tier.run/","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/Tier":{"title":"Tier","content":"https://www.tier.run/\n理念： [[Pricing as Code]] 、[[PriceOps]] ","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/blogs/Configmap-Secret-Manager":{"title":"Configmap/Secret Manager","content":"## Configmap/Secret Manager\n\n\u003ca name=\"YjhpG\"\u003e\u003c/a\u003e\n## ReadLink\n\n- [configmap manager](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/configmap/configmap_manager.go)\n- [pkg/kubelet/secret/secret_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/secret/secret_manager.go)\n\u003ca name=\"GuGtv\"\u003e\u003c/a\u003e\n\n## Configmap Manager\n\n```go\n// Manager interface provides methods for Kubelet to manage ConfigMap.\ntype Manager interface {\n    // Get configmap by configmap namespace and name.\n    GetConfigMap(namespace, name string) (*v1.ConfigMap, error)\n    \n    // WARNING: Register/UnregisterPod functions should be efficient,\n    // i.e. should not block on network operations.\n    \n    // RegisterPod registers all configmaps from a given pod.\n    RegisterPod(pod *v1.Pod)\n    \n    // UnregisterPod unregisters configmaps from a given pod that are not\n    // used by any other registered pod.\n    UnregisterPod(pod *v1.Pod)\n}\n```\n\n接口非常简单。\n\n1. GetConfigMap ： 通过 namespace 和 name 获取对应 ConfigMap 对象。\n1. RegisterPod(pod *v1.Pod)：把指定 Pod 对象 yaml 指定的 configmap 注册到 Controller 中管理\n1. UnregisterPod(pod *v1.Pod)：把指定 Pod 对象 yaml 指定的 configmap 从 Controller 中注册管理中删除，注意 ConfigMap 需要没有任何其他已注册的 Pod 引用（即无被依赖项）才可以删除\n\n当前代码中有两种 manager 的实现\n\n-`NewCachingConfigMapManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager`：该实现有两点逻辑\n   - 当一个 Pod 创建或者更新时，所有的 configmap 缓存都失效。\n   -  GetObject() 调用首先从本地缓存获取，失败则访问 APISever 并刷新 configmap 的缓存。\n\n```go\n// NewCachingConfigMapManager creates a manager that keeps a cache of all configmaps\n// necessary for registered pods.\n// It implement the following logic:\n// - whenever a pod is create or updated, the cached versions of all configmaps\n//   are invalidated\n// - every GetObject() call tries to fetch the value from local cache; if it is\n//   not there, invalidated or too old, we fetch it from apiserver and refresh the\n//   value in cache; otherwise it is just fetched from cache\nfunc NewCachingConfigMapManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager {\n\tgetConfigMap := func(namespace, name string, opts metav1.GetOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).Get(context.TODO(), name, opts)\n\t}\n\tconfigMapStore := manager.NewObjectStore(getConfigMap, clock.RealClock{}, getTTL, defaultTTL)\n\treturn \u0026configMapManager{\n\t\tmanager: manager.NewCacheBasedManager(configMapStore, getConfigMapNames),\n\t}\n}\n```\n\n- `NewWatchingConfigMapManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager`：\n   - 当一个 Pod 创建或者更新时，会对指定该 Pod 引用的资源，并且该资源未被其他 Pod 引用进行独立的 watch。\n   - GetObject() 调用首先从本地缓存获取\n\n```go\n// NewWatchingConfigMapManager creates a manager that keeps a cache of all configmaps\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, we start individual watches for all\n//   referenced objects that aren't referenced from other registered pods\n// - every GetObject() returns a value from local cache propagated via watches\nfunc NewWatchingConfigMapManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager {\n\tlistConfigMap := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).List(context.TODO(), opts)\n\t}\n\twatchConfigMap := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) {\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).Watch(context.TODO(), opts)\n\t}\n\tnewConfigMap := func() runtime.Object {\n\t\treturn \u0026v1.ConfigMap{}\n\t}\n\tisImmutable := func(object runtime.Object) bool {\n\t\tif configMap, ok := object.(*v1.ConfigMap); ok {\n\t\t\treturn configMap.Immutable != nil \u0026\u0026 *configMap.Immutable\n\t\t}\n\t\treturn false\n\t}\n\tgr := corev1.Resource(\"configmap\")\n\treturn \u0026configMapManager{\n\t\tmanager: manager.NewWatchBasedManager(listConfigMap, watchConfigMap, newConfigMap, isImmutable, gr, resyncInterval, getConfigMapNames),\n\t}\n}\n\n```\n\n\u003ca name=\"gEtqm\"\u003e\u003c/a\u003e\n## Secret Manager\n\nsecret manager 除了资源类型和 configmap 不一样，其他逻辑相同，所以仅列出两种 secret manager 的初始化函数。\u003cbr /\u003e[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[secret /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/secret)[secret_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/secret/secret_manager.go)\n\n```go\n// NewCachingSecretManager creates a manager that keeps a cache of all secrets\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, the cached versions of all secrets\n//   are invalidated\n// - every GetObject() call tries to fetch the value from local cache; if it is\n//   not there, invalidated or too old, we fetch it from apiserver and refresh the\n//   value in cache; otherwise it is just fetched from cache\nfunc NewCachingSecretManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager {\n\tgetSecret := func(namespace, name string, opts metav1.GetOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().Secrets(namespace).Get(context.TODO(), name, opts)\n\t}\n\tsecretStore := manager.NewObjectStore(getSecret, clock.RealClock{}, getTTL, defaultTTL)\n\treturn \u0026secretManager{\n\t\tmanager: manager.NewCacheBasedManager(secretStore, getSecretNames),\n\t}\n}\n\n// NewWatchingSecretManager creates a manager that keeps a cache of all secrets\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, we start individual watches for all\n//   referenced objects that aren't referenced from other registered pods\n// - every GetObject() returns a value from local cache propagated via watches\nfunc NewWatchingSecretManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager {\n\tlistSecret := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().Secrets(namespace).List(context.TODO(), opts)\n\t}\n\twatchSecret := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) {\n\t\treturn kubeClient.CoreV1().Secrets(namespace).Watch(context.TODO(), opts)\n\t}\n\tnewSecret := func() runtime.Object {\n\t\treturn \u0026v1.Secret{}\n\t}\n\tisImmutable := func(object runtime.Object) bool {\n\t\tif secret, ok := object.(*v1.Secret); ok {\n\t\t\treturn secret.Immutable != nil \u0026\u0026 *secret.Immutable\n\t\t}\n\t\treturn false\n\t}\n\tgr := corev1.Resource(\"secret\")\n\treturn \u0026secretManager{\n\t\tmanager: manager.NewWatchBasedManager(listSecret, watchSecret, newSecret, isImmutable, gr, resyncInterval, getSecretNames),\n\t}\n}\n```\n\n\u003ca name=\"RX3SN\"\u003e\u003c/a\u003e\n## cache_based_manager\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[cache_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/cache_based_manager.go)\n\n```go\n// cacheBasedManager keeps a store with objects necessary\n// for registered pods. Different implementations of the store\n// may result in different semantics for freshness of objects\n// (e.g. ttl-based implementation vs watch-based implementation).\ntype cacheBasedManager struct {\n    objectStore          Store\n\tgetReferencedObjects func(*v1.Pod) sets.String\n\n\tlock           sync.Mutex\n\tregisteredPods map[objectKey]*v1.Pod\n}\n```\n\n该 manager 代码位于  [/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[cache_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/cache_based_manager.go)，属于通用的 Manager 结构体工具，用于保留注册的  Pod 所必要引用的 kubernetes 对象（objects）\u003cbr /\u003e如何做到的呢？\u003cbr /\u003e通过 getReferencedObjects 字段，一个可以传入的成员函数，自定义实现用于从 v1.Pod 对象中获取到对应对象（或一组对象）的 name。流程如下：\n\n```go\nfunc (c *cacheBasedManager) RegisterPod(pod *v1.Pod) {\n    // 1. 获取名字\n\tnames := c.getReferencedObjects(pod)\n\tc.lock.Lock()\n\tdefer c.lock.Unlock()\n    // 2. 给每一个名字和 pod 的命名空间一起添加到 c.objectStore 中存储\n\tfor name := range names {\n\t\tc.objectStore.AddReference(pod.Namespace, name)\n\t}\n    // 3. 检查是否之前已经注册了该 Pod\n\tvar prev *v1.Pod\n\tkey := objectKey{namespace: pod.Namespace, name: pod.Name, uid: pod.UID}\n\tprev = c.registeredPods[key]\n    // 4. 用新注册的 pod 替换之前存储的注册 Pod 的信息.\n\tc.registeredPods[key] = pod\n    // 5. 删除旧 Pod 在 c.objectStore 中的引用信息,这是因为在上面第二步 Add 到 c.objectStore\n    // 中时,这些资源的引用次数又新增了一次,但实际上只是同一个 Pod 的引用,自然需要删除,当然,也有\n    // 可能新 Pod 已经不再引用目标资源了, Delete 函数在下面也处理这个情况\n\tif prev != nil {\n\t\tfor name := range c.getReferencedObjects(prev) {\n\t\t\t// On an update, the .Add() call above will have re-incremented the\n\t\t\t// ref count of any existing object, so any objects that are in both\n\t\t\t// names and prev need to have their ref counts decremented. Any that\n\t\t\t// are only in prev need to be completely removed. This unconditional\n\t\t\t// call takes care of both cases.\n\t\t\tc.objectStore.DeleteReference(prev.Namespace, name)\n\t\t}\n\t}\n}\n```\n\n1. 获取名字\n1. 给每一个名字和 pod 的命名空间一起添加到 c.objectStore 中存储\n1. 检查是否之前已经注册了该 Pod\n1. 用新注册的 pod 替换之前存储的注册 Pod 的信息.\n5. 删除旧 Pod 在 c.objectStore 中的引用信息,这是因为在上面第二步 Add 到 c.objectStore 中时,这些资源的引用次数又新增了一次,但实际上只是同一个 Pod 的引用,自然需要删除,当然,也有可能新 Pod 已经不再引用目标资源了, Delete 函数在下面也处理这个情况\n\n\u003ca name=\"aQgQM\"\u003e\u003c/a\u003e\n### ttl ObjectStore\n\ncache_based 的 objectStore 通过 ttl 设置缓存有效期。\n\u003ca name=\"qzaxL\"\u003e\u003c/a\u003e\n\n## watch_based_manager\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[watch_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/watch_based_manager.go)\u003cbr /\u003e可以看到，watch_based_manager 最后使用了 NewCacheBasedManager ，所以 watch_based_manager  和 cache_based_manager 不同的是 ObjectStore 字段。\n\n\n```go\n// NewWatchBasedManager creates a manager that keeps a cache of all objects\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, we start individual watches for all\n//   referenced objects that aren't referenced from other registered pods\n// - every GetObject() returns a value from local cache propagated via watches\nfunc NewWatchBasedManager(\n\tlistObject listObjectFunc,\n\twatchObject watchObjectFunc,\n\tnewObject newObjectFunc,\n\tisImmutable isImmutableFunc,\n\tgroupResource schema.GroupResource,\n\tresyncInterval time.Duration,\n\tgetReferencedObjects func(*v1.Pod) sets.String) Manager {\n\n\t// If a configmap/secret is used as a volume, the volumeManager will visit the objectCacheItem every resyncInterval cycle,\n\t// We just want to stop the objectCacheItem referenced by environment variables,\n\t// So, maxIdleTime is set to an integer multiple of resyncInterval,\n\t// We currently set it to 5 times.\n\tmaxIdleTime := resyncInterval * 5\n\n\t// TODO propagate stopCh from the higher level.\n\tobjectStore := NewObjectCache(listObject, watchObject, newObject, isImmutable, groupResource, clock.RealClock{}, maxIdleTime, wait.NeverStop)\n\treturn NewCacheBasedManager(objectStore, getReferencedObjects)\n}\n```\n\nwatch_based_manager  通过 watch 而不是简单的 ttl 去确认或者刷新缓存。\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/blogs/How-Cloud-Develop-Kit-from-Google-designed-the-docstore-interface":{"title":"How Cloud Develop Kit from Google designed the docstore interface","content":"\n## How Cloud Develop Kit from Google designed the docstore interface\n\n\u003ca name=\"ljgKZ\"\u003e\u003c/a\u003e\n## Refer\n- [Docstore · Go CDK](https://gocloud.dev/howto/docstore/)\n- [urls.go - google/go-cloud - Sourcegraph](https://sourcegraph.com/github.com/google/go-cloud@master/-/blob/docstore/mongodocstore/urls.go)\n- [driver.go - Go](https://cs.opensource.google/go/go/+/refs/tags/go1.18.3:src/database/sql/driver/driver.go)\n\u003ca name=\"Eu7vN\"\u003e\u003c/a\u003e\n\n## Design objectives: \n\n**through the abstraction layer, we can mask differences, provide services in a standardized way, and configure business applications through description files. **\n\n**Provides design ideas and guidelines for applications that use document storage. **\n\u003ca name=\"yaqn6\"\u003e\u003c/a\u003e\n## Intro\ncommon in MongoDB [document Storage](https://en.wikipedia.org/wiki/Document-oriented_database) provides an abstraction layer. \n\nDocument Storage is a service that stores data in semi-structured JSON-like documents. These documents are grouped into collections. Like other NoSQL databases, document storage is modeless. \n\nThe design needs to support adding, retrieving, modifying, and deleting documents. \ndocstore Driver implementation of various services, including cloud and local solutions. You can develop applications locally and then reconfigure them to multiple cloud providers with minimal initialization. \n\u003ca name=\"FCVI2\"\u003e\u003c/a\u003e\n## 设计\n\u003ca name=\"fg6xL\"\u003e\u003c/a\u003e\n### Structuring Portable Code \nStructuring Portable Code the non-interface design imitates the database/SQL package of golang and wraps the existing common logic into the structure. The internal fields of the structure are driver interfaces. The method provided externally is the method corresponding to the structure rather than the implementation of the direction provided driver.\n\n\u003e The advantage of this design is that there is no need to implement general logic processing for each interface, and the code can be transplanted. In some cases, you only need to add and modify methods on the structure and do not need to destroy the method design in the interface. You can also mask some assertion logic. When switching different drivers, users do not need to determine the implementation of some optional interfaces.\n\n[Structuring Portable Code · Go CDK](https://gocloud.dev/concepts/structure/)\n\n[sql package - database/sql - Go Packages](https://pkg.go.dev/database/sql#DB)\n\n![[blogs/Pasted image 20221011180052.png]]\n\n\u003cbr /\u003eCode like below：\n```go\n// Define\ntype DB interface{\n    Exec(sql string)error\n}\n\n// Realize\ntype mysql struct {}\nfunc (m *mysql) Exec(sql string) error {return nil}\n\n// Execute\nsql.Database.Exec(\"\")\n```\n```go\n// package and structure\npackage sql\n\ntype DB struct {\n    driver driver.DB\n}\n\n// higher level logic\nfunc (db *DB) AnySignature(anyParams string) (anyReturn error) {\n    //... \n    db.driver.Exec(\"...\")\n    //...\n    return nil\n}\n// Define\npackage driver \n\ntype DB interface{\n    Exec(sql string)error\n}\n\n// Realize\ntype mysql struct {}\nfunc (m *mysql) Exec(sql string) error {return nil}\n\n// Execute\nsql.DB.AnySignature(\"\")\n```\n\u003ca name=\"xSKiY\"\u003e\u003c/a\u003e\n### Actions List\nFor MongoDB, batch processing can be carried out to improve efficiency. As the shielding layer of packaging, we hope to obtain this benefit according to the actual processing of driver. A queue or cache is required to submit a batch operation.\n\n[Batch write operations-MongoDB-CN-Manual](https://docs.mongoing.com/mongodb-crud-operations/bulk-write-operations)\n\n- [x] I think it is enough to undertake Google Go CDK design \n\n\u003ca name=\"g9Zj6\"\u003e\u003c/a\u003e\n### Driver Map \u0026 Opener\nInherited from the Mysql Driver registration method, through the golang standard import_\" github.com/xxx/driver\" different database drivers can be introduced. The principle is to use a global Map.\n\nGo CDK has upgraded the Opener feature. The original custom URL Parsing method is \"mysql\", \"user:password@/dbname\" the features of the new version are blob+file:///dir even \u003capi\u003e+ \u003ctype\u003e+ prefix (e.g. blob+bucket+file:///dir) for Google Cloud SDK, the same URL can provide different functions. However, in our opinion, this function does not have much effect for the time being, so we will block their design. \n\n\u003ca name=\"MrRZi\"\u003e\u003c/a\u003e\n### Dependency Injection wire \nGo CDK use the wire project to inject dependencies to automatically switch the structure of different backend providers to the SDK. Different from the way Dapr accesses different services, Dapr uses the yaml description to determine the different plug-ins that are enabled. \n\nFor example, you need wire.Build() indicates the new function of the driver to be introduced. \n\nIt has little impact on this project and may not be added for the time being. \n\n\u003ca name=\"tUMOU\"\u003e\u003c/a\u003e\n\n### UUID usage\n\nmongoDB, each entry must have a Key, which can be passed through parameters. \n```go\ntype Player struct {\n    ID   interface{} `docstore:\"_id,omitempty\"`\n    Name string\n}\n```\n\nThe simplest is to indicate the_id field directly in the structure. \n\n```go\ndocstore.OpenCollection(context.Background(), \"mongo://my-db/my-coll?id_field=name\")\n```\nYou can also use the URL Parameter in the high-level abstraction. The following example specifies the_ID as the name field of the above-mentioned high-level abstraction, which is used by the underlying layer `mongodocstore.OpenCollection` mapping relationship, will automatically generate mongo official driver type `primitivie.ObjectID `\n\n```go\ncoll, err := mongodocstore.OpenCollection(mcoll, \"id\", nil)\n\ntype IDer struct {\n\tID primitive.ObjectID\n}\n```\n\nyou can also use `mongodocstore.OpenCollectionWithIDFunc` to specify how to generate an ID.\n\n```go\nnameFromDocument := func(doc docstore.Document) interface{} {\n    return primitive.NewObjectID()\n}\ncoll, err := mongodocstore.OpenCollectionWithIDFunc(mcoll, nameFromDocument, nil)\n```\n\n\u003ca name=\"F0eWR\"\u003e\u003c/a\u003e\n## Summary \nWe have completed the access design and understanding of Document Store and can perform basic operations on adding, deleting, modifying, and querying docstores. Next, we will build service applications based on this layer of abstraction. \n\nFor special functions of different docstores, you can add them to docstore to determine whether they are target-driven and change the method of external exposure.\n\n\u003ca name=\"E8DHr\"\u003e\u003c/a\u003e\n## function \n\nthe following shows the functions of the library. \n\n\u003ca name=\"hVYIf\"\u003e\u003c/a\u003e\n### Connect MongoDB\nThe default mongo driver uses MONGO_SERVER_URL link to the server, so you can use code to set it here or directly set it by using environment variables. \n\nthe following meaning is from mongodb://localhost:27017 the link on the server is called `my-db`in the database `my-coll` document. The unique field name of mongo is `name`. \n\n```go\nos.Setenv(\"MONGO_SERVER_URL\", \"mongodb://localhost:27017\")\n\ncoll, err := docstore.OpenCollection(context.Background(), \"mongo://my-db/my-coll?id_field=name\")\ndefer coll.Close()\n```\n\u003ca name=\"HM4A2\"\u003e\u003c/a\u003e\n### Corresponding display structure \n```go\ntype Player struct {\n\tName             string `docstore:\"name,omitempty\"`\n\tScore            int\n\tDocstoreRevision interface{}\n}\n```\n\u003ca name=\"YFPTq\"\u003e\u003c/a\u003e\n### Create \n```go\ncoll.Create(ctx, \u0026Player{Name: \"Pat\", Score: 7}); \n```\n\u003ca name=\"CuHHJ\"\u003e\u003c/a\u003e\n### Get \n```go\ncoll.Get(ctx, \u0026Player{Name: \"Pat\"});\n```\n\u003ca name=\"syvtX\"\u003e\u003c/a\u003e\n### Queries \nyou may need to manually create indexes to complete the query function. \n```go\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"gocloud.dev/docstore\"\n)\n\n// Ask for all players with scores at least 20.\niter := coll.Query().Where(\"Score\", \"\u003e=\", 20).OrderBy(\"Score\", docstore.Descending).Get(ctx)\ndefer iter.Stop()\n\n// Query.Get returns an iterator. Call Next on it until io.EOF.\nfor {\n\tvar p Player\n\terr := iter.Next(ctx, \u0026p)\n\tif err == io.EOF {\n\t\tbreak\n\t} else if err != nil {\n\t\treturn err\n\t} else {\n\t\tfmt.Printf(\"%s: %d\\n\", p.Name, p.Score)\n\t}\n}\n```\n\u003ca name=\"zif5K\"\u003e\u003c/a\u003e\n### Update a single field of an Update entry\n```go\npat2 := \u0026Player{Name: \"Pat\"}\nerr := coll.Actions().Update(pat, docstore.Mods{\"Score\": 15}).Get(pat2).Do(ctx)\n```\n\u003ca name=\"VJYOy\"\u003e\u003c/a\u003e\n### Replace \ncompletely replace the entire entry \n```go\ncoll.Replace(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"QjVog\"\u003e\u003c/a\u003e\n### Put \nthe Put function is equivalent to CreateOrUpdate\n```go\ncoll.Put(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"wzyMJ\"\u003e\u003c/a\u003e\n### Delete \n```go\ncoll.Delete(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"wer1V\"\u003e\u003c/a\u003e\n### More examples \n\n- [CLI Sample](https://github.com/google/go-cloud/tree/master/samples/gocdk-docstore)\n- [Order Processor sample](https://gocloud.dev/tutorials/order/)\n- [docstore package examples](https://godoc.org/gocloud.dev/docstore#pkg-examples)\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/blogs/Node-Status-Manager":{"title":"Node Status Manager","content":"## Node Status Manager\n\n\u003ca name=\"eQr2o\"\u003e\u003c/a\u003e\n## ReadLink\n- [pkg/kubelet/nodestatus/setters.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/nodestatus/setters.go)\n- [/](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg/kubelet)[kubelet_node_status.go](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go)\n\n\u003ca name=\"HvtiD\"\u003e\u003c/a\u003e\n## Directory Layout\n```go\npkg/kubelet/nodestatus\n |- setters.go\n |- setters_test.go\n```\n\u003ca name=\"ABxjo\"\u003e\u003c/a\u003e\n## Setter\n```go\n// Setter modifies the node in-place, and returns an error if the modification failed.\n// Setters may partially mutate the node before returning an error.\ntype Setter func(node *v1.Node) error\n```\nthe Setter function defines a function that performs operations on the v1.Node object. If an error is returned, the Node object may also be changed. \nFrom the function definition, you can see its usage: use functions to generate different setters for a class of modification of Node objects. In this way, you can modify the state of a Node. \nUse the simplest func GoRuntime() Setter example: \n```go\n// GoRuntime returns a Setter that sets GOOS and GOARCH on the node.\nfunc GoRuntime() Setter {\n\treturn func(node *v1.Node) error {\n\t\tnode.Status.NodeInfo.OperatingSystem = goruntime.GOOS\n\t\tnode.Status.NodeInfo.Architecture = goruntime.GOARCH\n\t\treturn nil\n\t}\n}\n```\nthis mode belongs to the middleware operation mode. You can contact middleware for understanding. \n\u003ca name=\"E3eSp\"\u003e\u003c/a\u003e\n## Setter List\nwe learned the setter mode changed by Node status. Currently, the code contains the following 12 setters:\n- **NodeAddress **returns a Setter that updates address-related information on the node.：updates address-related fields, such as IP address and hostname (typically the hostname variable in kubelet). \n- **MachineInfo **returns a Setter that updates machine-related information on the node.：updates fields related to host information, such as the maximum number of pods, the number of pods allocated to each core, and the number of resources. \n- **VersionInfo **returns a Setter that updates version-related information on the node.：containerRuntime version, cadvisor version\n- **DaemonEndpoints **returns a Setter that updates the daemon endpoints on the node.\n- **Images **returns a Setter that updates the images on the node.：updates image information. \n- **GoRuntime **returns a Setter that sets GOOS and GOARCH on the node.：GOOS GOARCH information \n- **ReadyCondition** returns a Setter that updates the v1.NodeReady condition on the node.：determines whether the node is in the Ready state from Kubelet fields such as the error return function in the runtimeState. \n- **MemoryPressureCondition **returns a Setter that updates the v1.NodeMemoryPressure condition on the node.\n- **PIDPressureCondition **returns a Setter that updates the v1.NodePIDPressure condition on the node.\n- **DiskPressureCondition **returns a Setter that updates the v1.NodeDiskPressure condition on the node.\n- **VolumesInUse **returns a Setter that updates the volumes in use on the node.\n- **VolumeLimits **returns a Setter that updates the volume limits on the node.\n\n\nSetter 的入参通常是 Kubelet 中的字段，自然使用是通过 Kubelet 去初始化使用。\n\u003ca name=\"uRi9a\"\u003e\u003c/a\u003e\n## Kubelet Node Status\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg/kubelet)[kubelet_node_status.go](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go)\n\u003ca name=\"LGGvP\"\u003e\u003c/a\u003e\n###  Setter 使用处\nafter all setters are initialized in the defaultNodeStatusFuncs function, the function returns a Setter array. \n\n[kubelet_node_status.go? L613](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L613)\n\n```go\n// defaultNodeStatusFuncs is a factory that generates the default set of\n// setNodeStatus funcs\nfunc (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error {\n\t// if cloud is not nil, we expect the cloud resource sync manager to exist\n\tvar nodeAddressesFunc func() ([]v1.NodeAddress, error)\n\tif kl.cloud != nil {\n\t\tnodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses\n\t}\n\tvar validateHostFunc func() error\n\tif kl.appArmorValidator != nil {\n\t\tvalidateHostFunc = kl.appArmorValidator.ValidateHost\n\t}\n\tvar setters []func(n *v1.Node) error\n\tsetters = append(setters,\n\t\tnodestatus.NodeAddress(kl.nodeIPs, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),\n\t\tnodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,\n\t\t\tkl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent),\n\t\tnodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),\n\t\tnodestatus.DaemonEndpoints(kl.daemonEndpoints),\n\t\tnodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),\n\t\tnodestatus.GoRuntime(),\n\t)\n\t// Volume limits\n\tsetters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))\n\n\tsetters = append(setters,\n\t\tnodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),\n\t\tnodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),\n\t\tnodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),\n\t\tnodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager.Status, kl.shutdownManager.ShutdownStatus, kl.recordNodeStatusEvent),\n\t\tnodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),\n\t\t// TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event\n\t\t// and record state back to the Kubelet runtime object. In the future, I'd like to isolate\n\t\t// these side-effects by decoupling the decisions to send events and partial status recording\n\t\t// from the Node setters.\n\t\tkl.recordNodeSchedulableEvent,\n\t)\n\treturn setters\n}\n```\nThe array is assigned to the setNodeStatusFuncs of kubelet.\n```go\n\t// Generating the status funcs should be the last thing we do,\n\t// since this relies on the rest of the Kubelet having been constructed.\n\tklet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()\n```\n\u003ca name=\"n0m8P\"\u003e\u003c/a\u003e\n## SyncNodeStatus Procedure\nhow do Kubelet use these Kubelet? The core is syncNodeStatus functions.\n\n[kubelet_node_status](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L435)\n```go\n// syncNodeStatus should be called periodically from a goroutine.\n// It synchronizes node status to master if there is any change or enough time\n// passed from the last sync, registering the kubelet first if necessary.\nfunc (kl *Kubelet) syncNodeStatus() {\n\tkl.syncNodeStatusMux.Lock()\n\tdefer kl.syncNodeStatusMux.Unlock()\n\n\tif kl.kubeClient == nil || kl.heartbeatClient == nil {\n\t\treturn\n\t}\n\tif kl.registerNode {\n\t\t// This will exit immediately if it doesn't need to do anything.\n\t\tkl.registerWithAPIServer()\n\t}\n\tif err := kl.updateNodeStatus(); err != nil {\n\t\tklog.ErrorS(err, \"Unable to update node status\")\n\t}\n}\n```\nsyncNodeStatus the function is called periodically in goroutine to synchronize the node status to the master.\n\u003ca name=\"bdMgb\"\u003e\u003c/a\u003e\n### 入口 Entry\ncurrently, it is called in three places: \n\n1. [kubelet.go? L1428:26](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet.go?L1428:26)\nin the Run function of the Kubelet, start goroutine for periodic synchronization. \n```go\ngo wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop)\n```\n\n2. [kubelet.go? L2433:7](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet.go?L2433:7)\n: performs one-time synchronization in the fastStatusUpdateOnce function.\n```go\nfunc (kl *Kubelet) fastStatusUpdateOnce() {\n\tfor {\n\t\t...\n        kl.syncNodeStatus()\n        return\n\t}\n}\n```\n\n3. [nodeshutdown_manager_linux.go? L283:11](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go?L283:11)\n: it is called in the start() function of nodeshutdownmanager, which is actually a goroutine and is triggered only after the shutdown event is received from the channel.\n```go\nif isShuttingDown {\n    // Update node status and ready condition\n    go m.syncNodeStatus()\n\n    m.processShutdownEvent()\n} \n```\n\n\u003ca name=\"FKaYq\"\u003e\u003c/a\u003e\n### 注册 RegisterWithAPIserver\nif kubelet needs to be registered, a for loop is executed to wait for registration to the APIServer.\n```go\nfor {\n    time.Sleep(step)\n    step = step * 2\n    if step \u003e= 7*time.Second {\n        step = 7 * time.Second\n    }\n\n    // 1. 获取 node 对象及其信息\n    node, err := kl.initialNode(context.TODO())\n    if err != nil {\n        klog.ErrorS(err, \"Unable to construct v1.Node object for kubelet\")\n        continue\n    }\n\n    klog.InfoS(\"Attempting to register node\", \"node\", klog.KObj(node))\n    // 2. 注册到 APIServer 中去\n    registered := kl.tryRegisterWithAPIServer(node)\n    if registered {\n        klog.InfoS(\"Successfully registered node\", \"node\", klog.KObj(node))\n        kl.registrationCompleted = true\n        return\n    }\n}\n```\n\n1. node, err := kl.initialNode(context.TODO()) : obtains the node object and its information. \n2. registered := kl.tryRegisterWithAPIServer(node) : Register to APIServer \n\n\u003ca name=\"bj9z5\"\u003e\u003c/a\u003e\n### Use Setter\n[Function tryUpdateNodeStatus (kubelet_node_status.go? L470:20)](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L470:20)\n: the processing part of the volumeManager is omitted.\n```go\n// tryUpdateNodeStatus tries to update node status to master if there is any\n// change or enough time passed from the last sync.\nfunc (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error {\n    originalNode := node.DeepCopy()\n    ...\n\tkl.setNodeStatus(node)\n    ...\n\t// Patch the current status on the API server\n\tupdatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node)\n    ...\n\treturn nil\n}\n```\nkl.setNodeStatus just traverses all the Setter functions we mentioned just now.\n\n```go\nfunc (kl *Kubelet) setNodeStatus(node *v1.Node) {\n\tfor i, f := range kl.setNodeStatusFuncs {\n\t\tklog.V(5).InfoS(\"Setting node status condition code\", \"position\", i, \"node\", klog.KObj(node))\n\t\tif err := f(node); err != nil {\n\t\t\tklog.ErrorS(err, \"Failed to set some node status fields\", \"node\", klog.KObj(node))\n\t\t}\n\t}\n}\n```\n\u003ca name=\"GWa5D\"\u003e\u003c/a\u003e\n## Conclusion\nwe have learned: \n1. what are the change functions of the Node Status and what rules are followed to sign the function.\n2. how to register a Setter function to a Kubelet. \n3. Kubelet when these setters are called to change the status of a Node. \n\nNext: \n\n1. you can try to add a custom setter function. \n2. Kubernetes code is not as neat as the design. Some todo can be changed after reading this article and code. Try to decouple the code. (You can also find it by searching todo in the code.)\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/blogs/Open-Source-History-of-Dapr-project":{"title":"Open Source History of Dapr project","content":"## Open Source History of Dapr project\n\nAt the beginning of this open-source column, I wrote this article to describe the birth and development of open-source projects, express my views on the open-source community and ecology, and share it with you. \n\nSome opinions are out of personal perspective, and there are inevitably some mistakes and mistakes. Please forgive me and correct me. \n\n## Background\nbefore the birth of the Dapr project, I would like to explain the current situation of the Dapr project for readers to understand the project itself. \n\nDapr is a CNCF community-driven open source project with Microsoft as its contributor. Microsoft, according to the author, the first author should be Bai Haishi and Yaron (he is also the author of the Dapr Learning Manual, who proposed OAM and Dapr). \n\nThe work objectives of the Dapr project are described as follows: \n\nDapr is a portable, event-driven runtime that enables any developer to quickly build flexible, stateless, and stateful applications that can run on cloud platforms or edge computing. \nSome community students think Dapr is the next form of the service mesh, and some people also call this runtime software of the new era mecha (mecha), mecha provides distributed capabilities for business applications, just like the operator wearing a mecha, to do what he could not have done.\n\nThe following figure shows Bilgin Ibryam. Multi-Runtime Microservices Architecture \n\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658032639778/CRnDlZMU-.png align=\"left\")\n\ntherefore, the Dapr project is open-source software that provides distributed capabilities for modern distributed applications. Currently, it is open-source on GitHub and has gained 18.6K stars, which is very popular. \n\n## Birth \nTracing back to open source on GitHub, the first submission was on June 21, 2019, \nthe birth of open-source projects is usually accompanied by the discovery of practical problems. \n\nBefore saying the problems solved by the Dapr project, there is another project that has to be mentioned, Microsoft's OAM(Open Application Model). \n\nThe two projects have been known to the public for 19 years. I remember that the co-sponsor of OAM is Ali. At that time, Kubernetes was very popular, and the problems on the computing scheduling platform were Kubernetes solved by Golang's killer application. \n\nHowever, using Kubernetes puts forward more and higher requirements for Developers, especially its new concept, which covers different APIs and unique working methods. \n\nHow to solve this problem? \n\nAny problem in the field of computer science can be solved by adding an indirect intermediate layer.\n\nIt is believed that smart readers, based on their current knowledge, have already thought that if a new design language can be used as the middle layer to block the similarities and differences of infrastructure developers do not need to pay attention to and focus on business coding, can it be solved? \n\nIn this way, OAM is naturally ready to come out. (If you are concerned about OAM, you can learn about the implementation of this project standard in Alibaba, namely Kubevela project, this project has great potential)\n\nDapr came up with an idea when Bai Haishi and his Israeli colleagues discussed OAM Yaron Schneider. It designed a new programming mode to encapsulate the common functions of the distributed system into Sidecar(Kubernetes concept, description, and business application in the same Pod container) and expose them to developers through HTTP or gRPC (two common transmission modes, which are compatible with most applications). \n\nThe idea is named Distributed Application Runtime, or Dapr for short. [This paragraph is taken from an interview with Bai Haishi, the founder of OAM and DAPR: a simple idea of a 33-year senior programmer -Zhang Shanyou]] \n\nDapr provides several new features to help solve the problems: \n\nthe first is to provide services in the form of Sidecar. In the container orchestration platform, Sidecar provides services in a non-intrusive way. \n\nFor example, Envoy Sidecar acts as a proxy for routing and forwarding. It is independent of major applications and therefore has cross-language features. Users can reuse logic without binding to a programming language, which is especially useful in the microservice era. \n\nThe second is the concept of Building Block, which allows Dapr users to customize different Building blocks, instead of forcing users to use distributed functions provided by Dapr for all functions.\n\n## Open source \n\nAfter talking for so long, I finally talked about the open-source features of the Dapr project. \nThe benefits of open source can be seen in the summary of my other article. This article will not go into detail, but mainly explore the reasons why Dapr needs to open source and provide material examples for everyone to understand the open source operation mode. \n\nDapr can be analyzed from the positioning of its general distributed runtime software, and the standard is its core! \n\nStandards cannot be achieved by one person or a company. It is necessary to strengthen Dapr's influence and promote its designation of standards that are uniformly recognized by everyone. It is the only choice to establish a community of common contributions through open source.\n\nIt is not only a matter of standards. Dapr, as an application in the new era, naturally has many new ideas, which need to be verified. A large number of engineers need to be invested in the verification of the programming mode. \n\nThis part of manpower expenditure and verification cost is extremely large. The continuous development of the project can only be supported by the rapid discussion of design, implementation, and community verification in the form of Community co-construction. \n\nTherefore, human resources are also considered in most open-source projects. \n\nFinally, reach users. \n\nWhen Dapr is a user-oriented project, there are developers who are more enthusiastic than open-source communities. Open-source is the best choice to make Dapr's development closer to users and the wide application of cloud developers that it wants to achieve. \n\nWe recommend two projects to observe the popularity and activity distribution of open-source projects ( Star-History and OSSInsight ), which are Bytebase and PingCap open-source tools. One picture wins thousands of words, and two pictures are attached to show its function. \n\nFigure 1: Star harvest trend of open source Dapr project\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658032918444/FsLL7iu_d.png align=\"left\")\n\n## Development\nSince the Dapr project is to serve developers, it is natural to investigate the main functions that developers use in the programming and provide them to users as different building blocks. \n\nCurrently, the capabilities it provides include state management, service development, message sending and receiving, publishing and subscription, security information management, Actor mode (originated from the Orleans project in Microsoft's dotnet ecosystem), etc. In the initial form of state management development, concurrent control, version management, and other capabilities are also added. \n\nNow, building blocks such as distributed locks and workflows are gradually added. These new functions and new building blocks are all built by community users' needs.\n\nIt can be seen from this that Dapr's open source strategy has achieved remarkable results. \n\nThe emergence of Dapr also coincides with the wave of XaaS. It reduces the occupation of the edge environment (more than 50 M binary, only 4 M memory is needed during operation), provides edge devices and applications with low capability, flexibly switches between edge environment and cloud, and supports multiple operating environments, which are its excellent sources of competitiveness. \n\nThe development evaluation of an open source project must pay attention to its related ecology. Dapr, as a similar infrastructure project, will discuss two ecosystems. \n\nOne is the ecosystem that supports the Dapr project operation. That is, driven by various Building blocks, their ecology determines which infrastructure Dapr users can apply.\n\nTake PubSub as an example. Common message queue drivers such as Kafka, Redis, NatsStreaming, and Pulsar provide the runtime capability in the publish/subscribe mode. \n\nThe ecosystem in this area is rich and colorful. The core problem is that drivers are contributed to the community by themselves. The code quality and the functions provided during application runtime are uneven. It can be seen that the idea of standardization cannot be easily achieved in the real world. \n\nOne is the Dapr-based project built on it. This ecology can also be reflected in the cases where most companies use Dapr. \n\nThe main users of Dapr started from the founders Microsoft and Ali, and now companies such as Qingyun have participated in the co-construction and produced many projects and practical cases. \n\nTaking Microsoft as an example, users who serve it can easily and painlessly switch the underlying dependencies on the cloud (for example, switching message queues from Rabbitmq to kafka). \nFor example, Alibaba provides a large number of distributed capabilities for function applications in its functional computing platform. \nFor example, Ant Financial has developed a layotto project based on its excellent ServiceMesh development experience, IT has implemented the distributed runtime concept that conforms to its own IT infrastructure (and is open-source).\nFor example, Qingyun's Openfunction is also built using Dapr in the function computing platform.\n\nEven Microsoft has launched a commercial product container app based on Dapr, which allows users to write function-level services. The infrastructure is provided by context. \nDapr provides these services with the choice of only focusing on business code logic. \n\nThe developer ecosystem of open-source projects is an important criterion. The number of issues created, the speed of response, the richness of proposal submission, the degree of the active contributor, the entry and loss of new contributors and core contributors, and other indicators are all important bases for us to evaluate the developer ecosystem. This section can praise ossinsight project, which provides you with query services through the website and provides us with powerful data for evaluating open-source projects. \n\nFigure 2: analysis of open source activities of the Dapr project\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658033060286/sUkAuPA7c.png align=\"left\")\n\n## Trend \ncompetition trend of domain software: Hot open-source projects actually symbolize the main competition fields in the current industry. PaaS, which took Kubernetes as the core in the past few years, IaaS, which was recently represented by Infra as Code, DevOps and Security, and SaaS and FaaS, which will further compete fiercely in the future, provide better value-added services. Different fields have their own solutions. We can see how to provide more valuable services from the open-source ecosystem. \n\nThe development trend of programmers: modern developers are generally faced with anxiety problems. As programmers, some of our work contents are boring, but with the passion for programming and the pursuit of a career, we can develop various innovative achievements in our daily work, which may not only achieve ourselves but also benefit the world. At the industry level and even at the national level, open source is embraced. Under such a development trend, open source will integrate young programmers as one of the popular cultures. \nMy personal advice is to understand the open source as soon as possible, embrace him, and become a compound talent. The next step for programmers is to explore the open source field. \n\nAnd this article has roughly described the context of the Dapr project. Only from the project ecology of Dapr, we can see the fierce competition in the development of cloud computing. We don't know how many projects are floating and disappearing in the tide, or they never appear in our eyes after a wave of waves. I hope readers can have a deeper understanding and ideas about the software life cycle, especially open-source software.\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/blogs/React-Hooks-State-Persistence":{"title":"React Hooks State Persistence","content":"\n\n本文讲述如何分析设计 通过 React Hooks 进行 State 持久化管理\n\n  \n\n## 分析\n\n正常前端，组件为类文件，自己维持状态，不易复用。\n\n  \n\n首先把组件中的 UI 和 状态分开，用 Action 连接，如下图。\n\n![[blogs/Pasted image 20221011190847.png]]\n\nAction 是算子\n\n  \n\n### Function\n\n则可成为以下函数\n\n-   UI = `f(S)`\n\n-   状态驱动组件重新渲染 UI\n\n-   Scu =`f(Sc, ∆)`\n\n-   组件会用到的 Scu 和 更改 Sc 的 ∆ 方法决定。\n\n  \n\n#### S\n\n每一个组件有他自己的状态集 s。\n\n  \n\n##### scu\n\n即，component use ：组件用到的状态，比如计数器中的数字\n\n所有组件的使用到 scu 共同组成一个状态 Scu--渲染一个 UI。\n\n  \n\n##### sc\n\n即，收到组件影响的状态，如登录组件可能每登录一次就会增加计数器，但是对于登录组件并不会用到这个状态，虽然它会更改它。\n\n  \n\n### 入参\n\n#### ∆\n\n设计 State 框架时，让每一个组件声明 sa 状态时，提供一个更改自己的函数 ma ，在 Action 事件时调用用于更改 State，而多个 ma 的集合为 ∆。\n\n  \n\n#### S\n\nSa =`f(S, ∆)` 中的 S 作为 f 的参数传入，因为并不知道 Action 会更改哪些 State 【甚至不知道有哪些】，故把所有 State 都作为入参。\n\n  \n\n### 局部渲染\n\n更改的状态驱动 UI 渲染，如果相同可以不改变。\n\n  \n\n如上所说，UI 由于入参为 S ，会接收所有的 State，组件自己根据自己需要的 sa 变动渲染，而不是 UI 根据 S 改动分发事件。\n\n  \n\n观察 Hooks 可知，`useState()` 方法使用`[Object.is](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is#Description)` [比较算法](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is#Description) 来比较 state。\n\n而 `useEffect()`则提供选择让它 [在只有某些值改变的时候](https://zh-hans.reactjs.org/docs/hooks-reference.html#conditionally-firing-an-effect) 才执行的参数。\n\n  \n\n## 设计\n\n### 实虚部数学模型\n\n实数并不完备，引入虚部。\n\n  \n\n虚数，只需要去掉虚部就可以表示实数。\n\n  \n\nCurry Func 也如此。\n\n  \n\n以上同理：`f(S,∆)`中`f(S)` 代表实数，不完备，加入 ∆ ，可以表达所有情况。\n\n  \n\n更改的维度从一维的 线 成为了 二维的平面。\n\n  \n\n另：框架使用的`f(S, ∆)`还是一维的线，但其实是该平面 任意一条线 ，因为 f(S,∆) 已经中的 ∆ 和 S 已经经由使用者确定，即在多维度选择了一个平面降维实现在代码中了。\n\n  \n\n### Persistence \n\n需要一个地方存储数据，local，session，remote 等.\n\n  \n\n### Connector\n\n组件如何把触发的事件分发给 State 处理？需要通信。\n\n  \n\n由于 js 单线程模型，选择共享内存设计新增一个 Connector 用于通信。\n\n  \n\n组件 Component 如何通知 State 改动。共享内存，采用 Connector 中间层。\n\n  \n\n### Action by CurryFunc\n\nState 如何知道框架使用者定义的 Action 改动了哪些 State ？即不知道 ∆ 的具体值。采用 Curry Func 满足延迟求值的需求。\n\n使用 `fg(S){return f(∆)`} 代替 `f(S,∆)`\n\nState 框架使用者自己使用 `f(∆)` 注册自己的状态更改算子 ∆。\n\n  \n\nState 框架开发者使用 `fg(S)` ，只管自己传入所有的 State 即可。\n\n  \n\n由于 React Hooks 的存在，state 自带使用 `f(S,∆)` 进行更新的功能。故框架留出 useState() 接口，返回 `f(∆)` ，供使用者进行状态管理。\n\n  \n\n#### Redux\n\nRedux 也是基于此函数模型，而在 Hooks 中官方已经使用 `useReducer(reducer, initialState)` 实现了它。其中 reducer 是设定好的 `f(S，∆)` ，而它返回 state 和 dispatch，其中 state 就是 Sa 而 dispatch 就是 `f(∆)`。\n\n```js\nfunction useReducer(reducer, initialState) {\n  const [state, setState] = useState(initialState);\n  function dispatch(action) {\n    const nextState = reducer(state, action);\n    setState(nextState);\n  }\n  return [state, dispatch];\n}\n```\n\n在我们看来，它也内部实现了 Connector 的作用。\n\n  \n\n## 实现\n\n### Persistence\n\n首先是通过 Hooks 实现存储, 使用 Local Store\n\n```js\nfunction useLocalJSONStore(key, defaultValue) {\n    const [state, setState] = useState(\n      () =\u003e JSON.parse(localStorage.getItem(key)) || defaultValue\n    );\n    useEffect(() =\u003e {\n      localStorage.setItem(key, JSON.stringify(state));\n    }, [key, state]);\n    return [state, setState];\n}\n```\n\n  \n\n#### 存储位置\n\n解决了持久化存储，提供外在的状态管理支持。考虑到我们会使用 Go 来做前端：\n\n1.  使用 Hooks 加 sqlite3 库本地存储\n2.  使用 Hooks 和 Go 通信完成\n\n  \n\n### Connector\n\n为了使用 Hooks 实现全局的状态通知。\n\n首先明白 `useState()` 获取到的 `setState()` 会触发当前组件的渲染：[https://zh-hans.reactjs.org/docs/hooks-state.html](https://zh-hans.reactjs.org/docs/hooks-state.html)\n\nConnector 让使用全局状态的组件订阅来连接上全局的状态更新，将自己的 `setState()` 传入更新队列，当其中任何一个组件使用 `dispatch()` 更改状态时会触发这个命名空间下的全部状态更新，从而达到刷新所有状态组件的目的。\n\n```js\nimport { useEffect } from \"react\"\n\nconst Connector = {}\n\nconst Broadcast = (name, state) =\u003e {\n    if (!Connector[name]) return;\n    Connector[name].forEach(setter =\u003e setter(state))\n}\n\nconst Subscribe = (name, setter) =\u003e {\n    if (!Connector[name]) Connector[name] =[];\n    Connector[name].push(setter)\n}\n\nconst UnSubscribe = (name, setter) =\u003e {\n    if (!Connector[name]) return\n    const index = Connector[name].indexOf(setter)\n    if (index !== -1) Connector[name].splice(index, 1)\n}\n\nconst connect = (name,setState) =\u003e {\n    console.log('connect')\n    useEffect(() =\u003e{\n        Subscribe(name, setState)\n        console.log('subscirbe',name)\n        return () =\u003e {\n            UnSubscribe(name,setState)\n            console.log('unsubscribe',name)\n        }\n    },[])\n}\n```\n  \n\n### useStore\n\n使用者使用 `useStore()` 来获取全局状态和 `dispatch()` 函数。内部实现就是 State Hook ，并拿到 `setState()`注册到订阅列表中。\n\n```js\nimport {Broadcast,connect} from './Connector'\nimport {useState} from 'react'\n\nexport function useStore(key,value) {\n    const [state,setState] = useState(value)\n    connect(key,setState)\n\n    return [state, (key,value) =\u003e {\n        Broadcast(key,value)\n    }]\n}\n```\n\n  \n\n### 目前状况\n\n  \n\n![](https://cdn.nlark.com/yuque/0/2019/svg/176280/1574064167135-22f14865-31ba-4b6c-a144-0d1315954ec1.svg)\n\n  \n\n## 使用\n\n使用 `useStore(key, value)` 即可。\n\n```js\nimport {useStore} from './useStore'\n\nexport function Counter({key,initialCount}) {\n    // const [count, setCount] = useLocalJSONStore(keyname, initialCount);\n    const [state, dispatch] = useStore(key,initialCount)\n    return (\n      \u003c\u003e\n        Count: {state}\n        \u003cbutton onClick={() =\u003e dispatch(keyname,initialCount)}\u003eReset\u003c/button\u003e\n        \u003cbutton onClick={() =\u003e dispatch(keyname,state-1)}\u003e-\u003c/button\u003e\n        \u003cbutton onClick={() =\u003e dispatch(keyname,state+1)}\u003e+\u003c/button\u003e\n      \u003c/\u003e\n    );\n  }\n```\n\n  \n\n![](https://cdn.nlark.com/yuque/0/2019/png/176280/1574063711150-a567cd8c-2117-47ea-8446-02da34624b22.png)\n\n## 进阶\n\n-   异步状态\n-   装饰器","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/blogs/Role-of-microservice-framework":{"title":"Role of microservice framework","content":"## Role of microservice framework\n\n## HTTP Channel and GRPC Channel \n\nbefore we begin, let's explore the differences between HTTP and RPC. The reason why gRPC is discussed here is that no one uses common RPC. \n\nHTTP is a common communication method used for business coding, and its popularity is needless to say. As a Web programmer, HTTP Server programming is its core skill. RPC is also indispensable in microservices. \n\nCan programmers who are familiar with one of the encoding quickly get started with the business coding of another transmission method? \n\nAfter all, the business logic is consistent, which seems to be only different in network transmission. \n\nColleagues who have done this know that the differences in business coding are not small. Although the differences are constrained to the transport layer through the abstraction layer during design, there is no framework to block the differences in implementation. Therefore, coding students need to go deep into it and handle it by themselves. \n\nFor example, you need to learn more about envoy and proto files, how to encode requests and return values, and how to use specific protobuf to parse message packets in your business. \n\nThe differences can be shielded at the abstraction level. We still need to write detailed differences in implementation. These are the operations that some programmers can replace with frameworks. \n\n## Top programmers and beginners, beginners, and idiots \n\nthe role of the framework is to make correct coding behavior without thinking.\n\nThere are enough ecological libraries for the current language to help compile various coding types. When lacking, some ecosystems can be transplanted by referring to other languages to reduce the workload. However, not every programmer can do such behavior at any time. \n\nGoogle is a friend. Business Code they often get into trouble because of something they don't know so that no matter how their skills or intelligence are, they can't solve the problem.\n\nIn the business, some coding work will be compared to moving bricks. Programmers are described as manual work to move code from here to there. However, when someone participates in the process, the error probability will also be related to the state of a person.\n\nThrough Murphy's law, we can recognize that errors must occur in these processes. How do reduce personal decisions to ensure high quality and high output of assembly line coding manpower?\n\nIf you want to treat yourself as an idiot coder and leave the error-prone parts to tools, the framework will generate great benefits. \n\n\n\u003e Nothing is built on Stone; Everything is built on sand, but we must build sand as stone.\n\u003e                                 -Jorge Louis Borges \n\n\nthe following are some examples: \n\n- code review: \n\narchitects not only need to formulate process standards, but also need to supervise the implementation. Code review is the major part of the workload. However, there are thousands of people, and code writers have their ideas. There may even be a design-based cohesion function, which is scattered at all levels in implementation, and the review process is even more inefficient. \n\nConstraints can be carried out through the framework, which is also the wisdom of software engineering. By increasing restrictions, standards can be formulated to provide efficiency.\n\n- Best Practices: \n\nbusiness code usually uses simple addition, deletion, modification, query data, and target resources. At the same time, there are some common functional requirements, such as JWT. \n\nThe framework can shield these differences. For example, JWT only has different types of tokens carried by HTTP, and ORM shields the actual data storage software interfaces in the background for addition, deletion, query, modification, and modification. \n\nThis is another wisdom of computer science, solving problems by adding a middle layer. Framework users can switch to different implementations without thinking.\n\nIf the best practices provided by the framework cannot meet the requirements, it is time for the document to show its role. Technical personnel-oriented documentation is useful only when problems occur.\n\n## The dilemma of microservices caused by abstract hierarchy and abstract leakage \n\n\u003e Google software engineering mentions three key differences between programming and software engineering: **time**, **scope**, and **trade-offs**. \n\nHowever, the idea of the framework is beautiful enough, but the realization, in reality, is full of trade-offs and the pursuit of perfection. \n\nEven if the strange requirements of a specific time limit on the business side are excluded. The design cannot be accomplished overnight and a perfect abstract design can be completed.\n\nAbstract leakage refers to the abstraction of implementation details that should be hidden during software development, which inevitably exposes the underlying details and limitations.\n\nNot to mention that a complete system has more than one or two levels. How to make reasonable abstraction and promote it as a standard is a long-term practice and change in many microservice frameworks and coding fields. \n\nAbstraction means unification, while behind the abstraction level, it usually means the actual services with different characteristics. Do you use the union or intersection of these services for abstraction? Whether to consider extended compatibility or functionality.\n\n\u003e For more information, see another article. [Mongo Doc access design](), is practical experience. \n\n\u003e Also The API of Dapr. Many Interfaces of Golang (IO, SQL, and Net) can see abstract practical practices.\n\nFor example, designers will struggle with whether to provide a certain function to the outside, so they have done a lot of work to provide it. However, in terms of function usage, it may be a pseudo requirement or a simple shielding. However, in actual scenarios, it is necessary to have a lower layer of functions, and the abstraction level is still broken down. \n\nAt this point, everyone understands that it falls into specific scenarios and analyzes specific problems. Therefore, a microservice framework that has passed the postgraduate entrance examination for a long time must have solved many problems in the target scenario. \n\n\u003e This reminds me that programmers always pursue new technologies. New microservice frameworks usually have high expectations, hoping that they can completely solve the problems encountered in practice that the old frameworks cannot solve. Finally, expectations often fail. Why can we expect a new untested framework to meet the needs of the technical framework that has been designed and modified many times in practice in specific fields?\n\nBack to our question at the beginning, is there a framework that unifies the HTTP Channel and gRPC Channel, and only needs to write the handler's internal code without paying attention to other work?\n\n In the modern framework, Dapr did accomplish this. \n\nWhat about the abstract cost? \n\nThe field type in the Protobuf is lost, and it is considered a payload. The handler has different self-processing types, which is consistent with HTTP abstraction. \n\nIs it true that such an abstraction layer has just come up with now? If you have a deeper understanding of computer science, you will find that some past ideas shine brilliantly in new scenarios. \n\nTime is the most significant variable (for example, previous programmers needed to deduct bytes. Now, do you still need to care about insufficient memory for personal PC and cloud coding?). \n\n## Summary\nThe above describes the problems related to the microservice framework considered in the experience. Just raising questions is a hooligan. My opinions and suggestions are mentioned a lot in the article. \n\nTo make a summary, it is: \n\nproviding a fool-like automated microservice framework enables programmers to make fewer decisions and make better decisions. \n\nOnly by using the time saved to innovate business links and business models, and not being involved in non-creative work such as environment building, can workers feel the value of innovation and self-achievement.","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/blogs/Technician-and-experiencer":{"title":"Technician and experiencer","content":"## Technician and experiencer\n\nWe believe that the experience of the experience is linked to the actual things and actual behaviors, while the technology of the technician symbolizes more general knowledge. \n\nFirst of all, in practical operation, it can be seen that skilled people are inferior to experienced people in all aspects, but they usually do better than inexperienced people. \nBy many views gained from the experience of the universal Judgment (Generic), we believe that technology was born. \n\nGenerally, we think that people with technology have a deeper understanding of this kind of thing, (in short, smarter) because their behaviors will be guided, and their starting point is reason rather than inertia. We also believe that people with technology master the ability to impart theories, but experienced people cannot teach others. \n\nDo you want to judge whether you are good at a certain technology and a master in this field? \n\nwhen you think that you can do something if others ask you but you can only tell the specific situation and specific practices, you are not a wise professor. If the universal judgment you say is not considered dialectically by yourself, you are not a Master of this event. \n\nAccording to this statement, the concept of a Technologist is close to a wise man, and wisdom is also explained as knowledge about the origin and principle of things. Readers can now think about what characteristics people with high IQ have in the concept we are talking about? \n \nfrom the above ideas, it is not surprising that mathematicians call them philosophers or wise men, because this discipline is based on the basic principles (Root) rather than a series of secondary disciplines. \n\nTherefore, the more common the principle is, the more it is regarded as truth. People who think they have mastered the truth tend to be frustrated in the field of new knowledge. \n\nSince the opinions that can be collected correspond to the infinite things, in reality, I am inconvenient to think that the universal principle of a kind of things also lacks the existence of truth because there are infinite kinds. \n\nSo when experts claim to be masters in other fields, I don't fear to think that they are not very good in this field.\n\nThe above metaphysical discussion is not to explore whether the truth exists in a pessimistic way. Instead, I want to express my praise for practical operation from the perspective of reality and the method of getting the so-called twice the result with half the effort by deeply learning the principles of things. \n\nOn the other hand, as programmers, top programmers, and technicians' technologies, they can explain the measurement of the difference in value they create. \n\nIt is not difficult to become an expert in a certain field through practice but based on the viewpoint of experience summary, it will be our goal to put forward universal fragments (.e. creation, design, and the invention of new technologies). ","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/blogs/TokenBucket":{"title":"TokenBucket","content":"## TokenBucket\n\n\n## Overview\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1657442131915/dUeLDBuYM.png align=\"left\")\n\n\n\n- available per second Limit put tokens into the bucket, or, every time 1/Limit add a token to the second bucket \n- maximum storage in buckets burst tokens. If the bucket is full, the new token will be discarded. \n- when an N is consumed when the data packet of the unit arrives N tokens, and then send the packet \n- if the available token in the bucket is less than N, the packet will be cached or discarded \n\n## token bucket algorithm \n\nthe token bucket algorithm is the most commonly used algorithm in network Traffic Shaping (Traffic Shaping) and Rate Limiting (Rate Limiting). \n\nTypically, the token bucket algorithm is used to control the number of data sent to the network and allow the sending of burst data. \n\n### overview \n\nthis package is based on the Token Bucket algorithm (Token Bucket) to implement throttling, which is very easy to use. RateLimiter is often used to limit the access rate to some physical or logical resources. It supports three methods, \n\n- AllowN() If you can't get it, return it immediately.\n- WaitN() It is temporarily lined up. When the token is sufficient, it may be returned to the position because of the Cancel of Context.\n- ReserveN() Started directly, but the predecessors dug the pit and filled it. The next request will pay the price for this, and wait until the tokens will make up for the air. There is enough token in the barrel.\n\n### Working instance \nassume that one is working RateLimiter \n\n#### allow and wait \nFor a Ratelimiter that generates a token per second, every second without a token, we will add a token 1.\n\n If the Ratelimiter does not use it in 10 seconds, then tokens become 10.0. At this time, a request arrives and requests three tokens, we will serve it from the token in Ratelimiter, tokens to 7.0. After this request, another request comes and requests 10 tokens.\n\n We will from the remaining 7 token cards from RatelimiterFor this request, there are three tokens left, we will get them from the new token produced by Ratelimiter. \n\nWe already know that the Ratelimiter produces 1 new token per second, which means that the above request still requires the three commands required for above request. The card requires it to wait for 3 seconds.\n\n#### reserve\nImagine a Ratelimiter generated a token per second, and now it is not used (in the initial state). If an expensive request requires 100 token cards. If we choose to let this request wait for 100 seconds before allowing it to execute, this is obviously ridiculous. \n\nWhy do we do nothing but just wait for 100 seconds? A better approach is to allow this request to execute immediately (no different from all), and then postpone the subsequent request to the right time point. \n\nWe allow this expensive task to perform immediately and delay the subsequent request for 100 seconds. This strategy is to let the task execute and wait at the same time.\n\n#### About timetoact\nAn important conclusion: Ratelimit does not remember the last request, but the next request allows the time to execute. This can also tell us very straightforwardly that the time interval of reaching the next scheduling time point. \n\nThe Ratelimiter is also very simple: the next scheduling time has passed. The difference between this time and the current time is how long the Ratelimiter has not been used. We will translate this time into tokens.Limit == 1), and just one request per second, then tokens will not grow.\n\n#### burst\nRatelimiter has a barrel capacity that is directly discarded when the request is greater than the capacity of this barrel.\n\nhttps://github.com/golang/time/blob/master/rate/rate.go\n\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/diary/2022-09-28":{"title":"2022-09-28","content":"\n - #todo \n\t* \n\t* \n- #journal \n\t- 17:50 离群的特异学习会远离共识，共识通常是正确的，因此稀有的技能选择存在巨大的风险，为了降低风险，我们必须保持谦虚。\n\t- 18:10 看到一篇很好的关于选择学习什么技能的文章： https://medium.com/accelerated-intelligence/while-most-people-fight-to-learn-in-demand-skills-smart-people-are-secretly-learning-rare-skills-f9b26856c9d6 学习笔记：[[为什么学习稀缺的技能]]","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/diary/2022-09-29":{"title":"2022-09-29","content":"\n - #todo \n\t* \n\t* \n- #journal\n- 15:34 添加测试\u003cbr\u003e\u003cbr\u003e\n- 20:23 memory 真好用，可以用来吐槽，就是没有发送快捷键\n\n- 20:25 最近需要思考学点什么东西。把昨天的博客看完吧\n- 20:30 学术文章\u003cbr\u003e我领域之外的学科，其他人甚至都不知道\u003cbr\u003e许可专有数据\u003cbr\u003e与可能不会公开分享某些见解的领域内部人士建立深厚的关系\u003cbr\u003e心智模型（难以评估的抽象值）\n  这些才是应该在我们的日常学习中去学习的东西，总结的面很到位。因为稀缺性，才有放大的价格。 ^cff4f1","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/diary/2022-09-30":{"title":"2022-09-30","content":"\n - #todo \n\t* golang devcloud 使用 minio\n\t* 学习稀有技能的进一步研究，找一个目标 [[为什么学习稀缺的技能]]\n- #journal ^e61eca","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/diary/2022-10-08":{"title":"2022-10-08","content":"\n- [[buildkit]] \n\t- [高策写过 buildkit 的使用体验](http://gaocegege.com/Blog/kubernetes/buildkit)，在他的 [[envd]] 的工作中 ^42517f\n\t- 新 [[Dockerfile]] 语法特性：buildkit 可以通过 docker buildx build 解析新的语法特性。from [[2022-10-08#^42517f]] \n\t\t- 多次 build 之间的缓存 `RUN --mount=type=cache,target=/root/.cache/pip pip install ...`\n\t\t- 多体系架构的支持 `docker buildx build --platform linux/amd64,linux/arm64 .`\n\t\t- 多行脚本 [[Dockerfile 多行脚本]] 只增加一个构建层\n- [[rocksdb]] 是单节点 KV 数据库, 设计基于 [[LSMs]] .[[rocksdb]] 是早期 [[Google]] 项目[[LevelDB]] 的一个分支。from  [[为什么我们在RocksDB上创建CockroachDB项目？]]\n- [demo 网址](https://postgres-wasm.netlify.app/) [[postgreSQL]] 跑在浏览器里，通过 [[wasm]]\n- [[novelai]] 使用 [[stable diffusion]] 生成了大量的二次元图片。repo 库： [github.com/NovelAI/stable-diffusion](https://github.com/NovelAI/stable-diffusion) 用 [[jupyter nodebook]] 写的\n- 试用一下 [[maigret]] 一个通过用户 id 收集全网账户报告的工具","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/diary/2022-10-09":{"title":"2022-10-09","content":"尝试使用软件进行思路的梳理，现在是个人时间看板的梳理。[[MnicTime]] 这个软件是可以记录所有的软件的时间的。手机 iPhone 也可以记录软件使用时间。\n不能像上面流水线记录，不然跟不上思路。总结然后记录：\n\n目的：需要找到哪些是可以优化的时间习惯。 通过目前对自我的观察：\n1. 各种群聊的聊天，然后发消息，消耗了部分时间\n2. 消磨时间的操作，这部分时间应该属于可以优化的。\n3. 最后是因为好奇心去看文章的时间\n\n三类大时间中，首先优化聊天时间，去掉大部分要进入查看的群聊即可，感觉很简单嘛。\n\n[[使用 quartz 托管 obsidian 到网络上]] \n\n[[清理 GIt 中的历史文件]]\n","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/diary/2022-10-10":{"title":"2022-10-10","content":"[[Pricing as Code]] 是[[Tier]]这个产品使用的理念。","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/diary/2022-10-11":{"title":"2022-10-11","content":"","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/hugo-extended":{"title":"hugo-extended","content":"\nhugo 安装网址：[https://gohugo.io/getting-started/installing/](https://gohugo.io/getting-started/installing/)\n\nwindows 我直接使用了安装\n```bash\nscoop install hugo-extended\n```","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/hugo-obsidian":{"title":"hugo-obsidian","content":"使用 [[Golang]] install 功能安装\n\n```\n# Install and link `hugo-obsidian` locally\ngo install github.com/jackyzha0/hugo-obsidian@latest\n```","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/maigret":{"title":"maigret","content":"https://github.com/soxoj/maigret","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/obsidian":{"title":"obsidian","content":"","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null},"/rocksdb":{"title":"rocksdb","content":"","lastmodified":"2022-10-11T11:25:57.63433546Z","tags":null}}