{"/":{"title":"🎁首页","content":"[[obsidian]] + [[Quartz]] Publish Web HomePage\n\nHello👋，本页面提供了一个网络访问我的笔记的途径，使用 [[Hugo]] 搭建（Quartz），笔记使用 Obsidian 编写。\n\n## 双链的阅读建议\n- 通过鼠标悬浮预览进行上下文不中断的阅读。\n- 通过底部图和双向链接找到更多感兴趣的。\n- 想查找直接使用搜索按钮\n\n也可以查看这个导航：[标签](/tags/)\n\n\n目前的写作工作流：\n### 写\n在任意地方都可以写，打开一个 [[obsidian]] 目标文件夹即可。\n\n### 发布到网页 notes.abser.top\n1. 通过 abserari/quartz 这个 repo，直接使用 obsidian-git 插件，push 到 quartz 库中。\n2. 最后通过 git 提交到 github 触发 action 自动构建\n3. 构建使用 [[hugo-extended]]  [[hugo-obsidian]] 工具\n\n### 同步功能\n然后使用 [remotely save](https://github.com/remotely-save/remotely-save) 插件同步，\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/%E4%B8%8D%E8%A6%81%E7%89%87%E9%9D%A2%E7%9A%84%E6%A0%B9%E6%8D%AE%E4%B8%80%E9%83%A8%E5%88%86%E6%83%B3%E6%B3%95%E4%B8%8B%E5%86%B3%E5%AE%9A":{"title":"不要片面的根据一部分想法下决定","content":"这个事例是在日常的生活中，会想到自己如果做什么事就好了。以写日记和记录照片为例，在自己翻看的时候就会想看到自己的记录，但实际上为了这个目的养成一个记录日记，照片，整理数据库的习惯是性价比不高的。以单满足一个回忆的需求来说，在年轻的时候是价值不高的，所以不推荐做。\n那么一个简短总结就是标题。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E5%9C%A8RocksDB%E4%B8%8A%E5%88%9B%E5%BB%BACockroachDB%E9%A1%B9%E7%9B%AE":{"title":"为什么我们在RocksDB上创建CockroachDB项目？","content":"https://www.cockroachlabs.com/blog/cockroachdb-on-rocksd/","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E4%BA%91%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F":{"title":"","content":"\n### Notes\n- 和 [[分布式]] 有关\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E4%BD%BF%E7%94%A8-quartz-%E6%89%98%E7%AE%A1-obsidian-%E5%88%B0%E7%BD%91%E7%BB%9C%E4%B8%8A":{"title":"使用 quartz 托管 obsidian 到网络上","content":"## Refer\nhttps://quartz.jzhao.xyz/notes/setup/\nhttps://quartz.jzhao.xyz/notes/obsidian/","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%81%9A%E6%80%BB%E7%BB%93%E5%92%8C%E6%84%9F%E6%83%B3%E7%9A%84%E6%97%B6%E5%80%99%E9%9C%80%E8%A6%81%E5%85%B7%E4%BD%93%E7%9A%84%E4%BA%8B%E4%BE%8B":{"title":"做总结和感想的时候需要具体的事例","content":"\n由前文想到，不同情况下人会有不同的感想，但是事例不会变，如果只记录当时的感悟和道理，并不能和后续的想法做对比。没有事例也没有说服力。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%85%AC%E6%9C%89%E4%BA%91%E5%92%8C%E7%A7%81%E6%9C%89%E4%BA%91%E5%8D%A0%E6%AF%94":{"title":"","content":"[[AWS]] CEO Andy Jessy在AWS 2019 re:Invent大会上分享了如下消息：[[公有云]]的总支出只占到总IT支出的3% 。IT支出中，[[私有云]]仍占绝大多数","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%88%86%E5%B8%83%E5%BC%8F":{"title":"分布式","content":"\n[[巨石应用]] 其实也能满足老旧小公司的需求，只是技术含量跟不上时代罢了。\n\n其实真正需要的是低成本，灵活性，随取随用，轻松连接，这也是云时代的需求。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%89%8D%E6%B2%BF%E7%9F%A5%E8%AF%86":{"title":"","content":"- 梳理一下我学习的过去的前沿知识和未来要学习的范畴\n\t- [[SaaS]]、[[IaaS]]\n\t- [[GitOps]]：真没用\n\t- [[AIops]]：算法真是锦上添花的东西\n\t- [[DevSecOps]]：融入安全理念，希望结合出新东西吧\n\t- [[Workflow as Code|Temporal]]\n\t- [[PriceOps]]","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E7%9A%84%E5%9F%B9%E8%82%B2":{"title":"基础设施的培育","content":"## 摘要\n\n\n## 问题、提示\n-  \n\n## 主要笔记\n-  [[Hashicorp]] 现在就像是 [[IaaS]] 行业的标准了，但是现在我们需要的是各个行业的细分标准的确立\n- 像 [[altogic]] 这样的软件服务，只要[[IaaS|基础设施即服务]]能够做好，上层这些应用构建都不太难。\n- [[行业的细分标准-基础设施层|如何定义当前行业的细分标准]]？\n\t- 不清楚，现在要做的是尝试去做这方面的工作。以快速提供一个 [[firestore]] 类似服务的框架为载体探究一下如何完成这件事。\n\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%A4%A7%E5%9E%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B":{"title":"大型深度学习模型","content":"## 摘要\n\n\n## 问题、提示\n- 为什么大型深度学习模型需要极大的内存 #query \n\n## 主要笔记\n- 需要大内存储存==**中间层**==的==**激活函数输出**==和==**权重**==等\n- 模型训练限制\n\t- 只能在单个 GPU 上训练，批大小（batch size）设置得极小\n\t- 太大的模型，单个 GPU 又放不下\n- 大型模型训练方法：\n\t- [[数据并行]]\n\t- [[模型并行]]\n- [[2022-10-18]] 当前流行的九大深度学习库\n\t1. **[[Megatron-LM]]**\n\t2. **[[DeepSpeed]]**\n\t3. **[[FairScale]]**\n\t4. **[[ParallelFormers]]**\n\t5. **[[ColossalAI]]**\n\t6. [[Alpa]]\n\t7. [[Hivemind]]\n\t8. **[[OneFlow]]**\n\t9. [[Mesh-Tensorflow]]\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86":{"title":"","content":"存储管理关注的不是高性能，而是高扩展，管理面的能力以存储为例，其实就是贯彻整个存储生命周期，通过管理的手段提高存储价值。\n\n因此，数据的生产、[[备份]]、[[故障隔离]]、[[容灾]]、恢复、[[持续数据保护]]、安全能力接入、数据 AI 接入这些属于管理面的能力。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%AD%98%E5%82%A8%E9%A2%86%E5%9F%9F%E7%9A%84%E4%BA%8B%E4%BB%B6":{"title":"存储领域的事件","content":"## 摘要\n\n\n## 问题、提示\n-  全球存储发展的几大趋势：**分布式**、**云化**、**[[闪存化]]**、**[[存储领域的智能化|智能]]**\n\n\n## 主要笔记\n### 事件\n2010年：EMC收购Isilon，22.5亿美元 ;\n\n2010年：惠普收购3Par，23.5亿美元 ;\n\n2011年：戴尔收购Compellent，9.6亿美元 ;\n\n2011年：[[希捷]]收购[[三星]]硬盘，13.75亿美元 ;\n\n2011年：[[西部数据]]收购日立GST，48亿美元 ;\n\n**2012**年：[[VMware]]提出[[SDDC（软件定义数据中心）]]和[[SDS（软件定义存储）]]的概念，之后推出SDS产品 – Virtual SAN的预览版 \n\n2013年：Avago收购LSI，66亿美元 ;\n\n2014年：闪迪收购Fusion-io，11亿美元 ;\n\n**2015**年：戴尔收购EMC，630亿美元 ;\n\n2015年：[[西部数据]]收购闪迪，160亿美元 ;\n\n2016年：IBM收购对象存储，约13亿美元 ;\n\n**2016**年：[[HCI]]（超融合）概念提出者[[Nutanix]]上市；\n\n2016年：博通收购博科，59亿美元 ;\n\n2016年：OpenText收购Dell EMC企业内容部门(包括Documentum等)，16.2亿美元 ;\n\n2017年：HPE收购Nimble Storage，10.9亿美元 ;\n\n**2018**年：[[微软]]收购混合云数据存储公司Avere Systems ;\n\n**2019**年:  [[AWS]]**收购E8 Storage**，估计在5000万美元至6000万美元之间 ;\n\n2019年: [[谷歌]]收购存储企业Elastifile, 2亿美元 ;\n\n2019年:  [[IBM]]收购 RedHat, 340亿美元，[[RedHat]]有两款开源存储产品：[[Ceph]]和[[Gluster]]。\n### Refer\n- [[未来人才的学习目标#^eda002]]\n- [中国存储网存储历史时间线](https://www.chinastor.com/history/)","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%AD%98%E5%82%A8%E9%A2%86%E5%9F%9F%E7%9A%84%E6%99%BA%E8%83%BD%E5%8C%96":{"title":"","content":"![[Pasted image 20221013145353.png]]","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%AD%A6%E4%B9%A0%E7%A8%80%E7%BC%BA%E7%9A%84%E6%8A%80%E8%83%BD":{"title":"学习稀缺的技能","content":"\n\u003e _应用异常值算法的困难正是这样做的原因！更困难意味着更稀有。_\n\n为了帮助您开始应对这些挑战，以下是我一遍又一遍地回顾的四个经验法则。虽然只有四个，但它们的冲击力很大。它们是数百小时思考如何更好地找到稀有和有价值的技能的简化：\n\n1.  **成为第一个学习有价值的技能的人。** 跟踪新兴科学、技术、应用程序、工具、行业和领域。当它们呈指数增长时，花几个小时探索它们，看看是否值得投入更多时间。它可以帮助您尝试新事物，而不会将时间浪费在变成无用的事物上。\n2.  **学习很难的有价值的技能。** 愿意投资于禁忌、不是超级性感的、耗时、费力、看起来有风险或超级技术或学术的领域。\n3.  **学习具有隐藏好处的宝贵技能。** 人类有价值盲点。他们低估了具有抽象、长期回报的技能；微技能；其他学科的技能；和被遗忘的经典思想。\n4.  **重新定义价值比共识更好。** 在我们的职业生涯中，我们利用我们的技能为他人服务。可能是客户、老板、我们的粉丝、招聘人员或其他人。如果您能够比其他人更好地了解您所服务的人的未满足需求，您将能够更好地满足这些需求。\n\n或者，简而言之，请记住：\n\n-   成为第一\n-   面对困难\n-   寻找隐藏的好处\n-   重新定义价值\n\n以上是 Michael Simomons 的学习稀有技能的理论，其中总结的几点我们应该尝试的规则，在他的规则下，他指导有如下学习资源（随个人的情景可以有变种，这是当然的）\n![[2022-09-29#^cff4f1]] \n\n## 第一步，解释：我们现在从规则开始解释，从而找到我们自己情景下的学习\n1. 首先，所有的技能都很有价值，然后我们要去寻找稀缺性的技能。这个前提是我们先找到有价值的技能，通过一个指数增长的模型判断一个事物发展是否超出常规，这通常都代表这件事有价值。那我们以程序员的视角举例， Web3 正好是急需技能之一（因为需求而有价值），物联网，边缘方面的技能也并驾齐驱。\n2. 然后我们去寻找技能的**稀缺性**！（因为稀缺性决定价格）这也有一个模型来描述他，即被禁止的，未被大部分人发现的，可能枯燥的，耗时，费力的，或者看起来非常有风险的，以及非常学术的，非常技术专业的领域。（毕竟你不能指望一蹴而就的领域有多稀缺，信息差是很难一直保持的）\n3. 同时不要忽视长期的**被动技能**，复利主义，以及一些经典思想，他们由于长期存在于我们的身边，会被我们的价值评估器忽视。以抽象的，长期回报的技能为例：写笔记，坚持总结输出；微技能，一些工作上的小技巧，比如电脑使用领域上的一些工具快捷键；**其他领域的技能！** 往往会产生巨大的**化学反应**，比如生物对计算机领域的影响，当然我还是建议学数学，比如金融割韭菜融入区块链；以及一些经典思想：分治和中间层的思想贯彻整个计算机科学，经常发现计算机的一些新领域应用了老的经典的思想从而大放异彩，比如深度学习啦！\n4. 重新定义价值而非共识，这句类比对一个行业需求的深挖。技能可以为每一个使用者提供相同影响的结果，但精准的需求分析和挖掘，能让某项技能发掘出更多的价值，他提升了价值的同时也提升了稀缺性。\n\n当然，以上规则其实主要是个人视角出发，如何发挥个人最大的影响力的思考，从团队等方向还能有更多，比如规模效应，然而我们希望将讨论范围限制在个人视角上以期提供一些切实的指导。\n\n\t道理的扩散成本是非常低的，但是让一个人相信这个道理的成本是极高的。\n\n## 第二步 分析学习资源的新变化\n| 考虑稀缺性前                                 | 后                                                           |\n| -------------------------------------------- | ------------------------------------------------------------ |\n| 阅读最新的畅销书                             | 学术论文                                                     |\n| 时刻检查社交媒体查看最具影响力的人的发言     | 领域之外的学科（加密学、经济学）                             |\n| 时刻保持关注行业最新的新闻（所有人都在关注） | 获取一些专有数据的授权并研究                                 |\n|                                              | 和领域内部人士建立深厚的关系（并且他们很少公开分享自己的观点 |\n|                                              | 一些抽象的心理模型                                                             |\n\n可以看到其中明显的差距，同样的例子还有很多，就不一一列举了。笔者这里最想强调的还是稀缺代表远离共识，意味着你的选择几乎总是错的（即使你是个顶尖聪明的人），这是需要谦虚的态度，避免一次投入过多，需要有足够多的证据、验证。所以保持谦虚，离群才能尽量保证个人的稳定。\n\n以程序员的视角来讲，可以先有这几条经验：\n1. 不需要看书，书几乎总是过时的，不稀缺的，有需求的技能学习途径或者基础，尽量看学术论文。\n2. 避免关注颇具影响力的人的一言一行，避免一直关注，要么深挖，要么略过。\n3. 多接触一些其他领域的学识，当前行业的其他领域：安全、底层、抽象论证，其他行业的各个领域：哲学（游戏理论）、数学、经济学、等等。\n4. 专有数据并不一定需要授权，也需要自己收集，当你的一些独特视角需要验证时，可以轻松通过编程（最好合法）爬取需要的信息分析验证（如果有一些微技能比如数据分析，会让这个过程更顺理成章）\n5. 不用关注网红，而是转而建立深厚的个人关系，这其实在个人认知（资源等）不足的情况下的最好选择，请寻找自己身边的这种人，如果接触不到，或许你应该换一个环境。\n6. 保持写笔记的习惯，持续锻炼自己的心理模型，这个过于抽象，不过当你阅读这篇笔记的时候，其实就在做这样的事。延续它！\n\n## 下一步，选择\n\n现在，实践一下这个稀缺性的模型，找到适合你的技能，至少不盲目的学习了是吗？","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6":{"title":"微服务二次开发框架","content":"## 摘要\n核心是一套可定制的平台，以 [[Go]] Binary 程序和[[配置文件]]的方式安装基础设施。是二次开发平台，不是最终产品。\n功能核心是 [[SQL]] 扩容，[[文档型数据库]]描述模型和 [[MInio]] 扩容。\n\n## 问题、提示\n-  \n\n## 主要笔记\n-  \n\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98":{"title":"性能调优","content":"在非分布式时代，单机性能调优还有些作用。在云上时代，除非没钱需要[[降本增效]]，不然加机器加配置就行。相对于商业价值来说，一般的硬件成本占比较低。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E6%97%85%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8":{"title":"旅行的本质","content":"用异化思想去思考：\n旅行的本质是\n-   个人远离他们文化的真实道路\n-   深入未知的深处重塑自我\n-   然后将这种学习带回他们的文化，这样它就可以发展\n\n旅行就是人类的一种[[离群算法]]应用，另一种应用是学习具有稀缺性的技能 [[学习稀缺的技能]]\n\n为什么小说里的英雄冒险那么吸引人 -\u003e [[英雄之旅]] ","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E6%9C%AA%E6%9D%A5%E4%BA%BA%E6%89%8D%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87":{"title":"","content":"### Summary\n做 ==AI 的基础设施==的学习\n\n### Questions/Cues\n- 2022-10-12 [[学习稀缺的技能]]对[[程序员]]来说是哪些？ #query\n\n### Notes\n做 [[AI 的基础设施]]，再逐渐过渡到数据，后续还有数据交易这些。我的观点是不做ai，毕竟优势不在这。\n\n最优选择还是和金融相关、数据交易相关，但是这些目前都还是起步，风险大，慢慢看就行。\n\n主要是对于这个行业的理解，必须随着持续做下去，才能看到清晰的方向。这样有机会的话，可以带着技术换个更牛的企业，比如银行。\n\n做复合型人才，主要还是要做，做的过程中体会，不要着急，我觉得都没必要在三年内锁死自己发展方向。\n\n存储、网络如果发展的好，本地计算机意义都不大，问题是在什么时候才能达到预期，那么[[云操作系统]]，又应该是什么形态，这些作为主攻方向都是不错的。 ^eda002\n\n反正我觉得与其在性能上下功夫，不如在管理上，硬件成本又不是大问题，对不对。\n\n即使在云端考虑，也是[[管理高于性能]]的，做管控，不做技术细节的提升，本身优势就很大，单项技术深入研究难度极高，但是管理控制是综合技术应用，空间也大。\n\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E6%B8%85%E7%90%86-GIt-%E4%B8%AD%E7%9A%84%E5%8E%86%E5%8F%B2%E6%96%87%E4%BB%B6":{"title":"清理 GIt 中的历史文件","content":"```\ngit filter-branch --force --index-filter 'git rm --cached --ignore-unmatch path-to-your-remove-file' --prune-empty --tag-name-filter cat -- --all\n```\n\n其中, path-to-your-remove-file 就是你要删除的文件的相对路径(相对于git仓库的跟目录), 替换成你要删除的文件即可. 注意一点，这里的文件或文件夹，都不能以 '/' 开头，否则文件或文件夹会被认为是从 git 的安装目录开始。\n\n如果你要删除的目标不是文件，而是文件夹，那么请在 `git rm --cached` 命令后面添加 -r 命令，表示递归的删除（子）文件夹和文件夹下的文件，类似于 `rm -rf` 命令。\n\n更多请参考：[ https://help.github.com/articles/remove-sensitive-data](https://help.github.com/articles/remove-sensitive-data)","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E7%94%9F%E6%B4%BB%E6%B2%A1%E6%9C%89%E6%84%8F%E4%B9%89":{"title":"生活没有意义？","content":"\n参见我的 [[Focus and Different]] 原则","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E7%A7%81%E6%9C%89%E4%BA%91":{"title":"私有云","content":"## 摘要\n\n\n## 问题、提示\n-  \n\n## 主要笔记\n-  [[HCI|超融合]] 可以视为私有云的一种部署形态\n\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E7%A7%8B":{"title":"","content":"秋桜　aki sakura\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E9%97%AE%E9%A2%98":{"title":"端口占用问题","content":"## 摘要\n- `ps -ef | grep` then `netstat -anp | grep pid`\n- `lsof -i:port_num`\n\n## 主要笔记\n### netstat\nnetstat -tunlp 用于显示 tcp，udp 的端口和进程等相关情况。\nnetstat 查看端口占用语法格式：\nnetstat -tunlp | grep 端口号\n-   -t (tcp) 仅显示tcp相关选项\n-   -u (udp)仅显示udp相关选项\n-   -n 拒绝显示别名，能显示数字的全部转化为数字\n-   -l 仅列出在Listen(监听)的服务状态\n-   -p 显示建立相关链接的程序名","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E7%AE%A1%E7%90%86%E9%AB%98%E4%BA%8E%E6%80%A7%E8%83%BD":{"title":"","content":"### 摘要\n\n\n### 问题、提示\n- \n\n### 主要笔记\n-  [[分布式]] 和 [[云操作系统]] 带来的变化\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E8%87%AA%E5%8A%A8%E5%86%85%E5%A4%96%E7%BD%91%E8%B7%AF%E7%94%B1-bat-%E8%84%9A%E6%9C%AC":{"title":"自动内外网路由 bat 脚本","content":"```batch\n@:loop\n@time /T\n@REM REM 是注释. '@'是关闭回显.\n@REM 1.1.1.1 是网关, 对应修改.\n@REM For Chengdu\n@set gateway=1.1.1.1\n@REM For Shenzhen\n@REM @set gateway=1.1.1.1\n\n@REM 内网网络每间隔几秒钟会添加自己的网关为默认网关, 导致不能连通wifi外网\n@REM 删除内网网络添加的默认网关后, wifi网络才能连通.\n@route delete 0.0.0.0 %gateway% 2\u003enul\n\n@REM 根据自己的访问需求, 添加内网网段的白名单, 走内网路由.\n@route add 10.0.0.0  mask 255.0.0.0 %gateway% 2\u003enul\n@route add 200.200.0.0 mask 255.255.0.0 %gateway% 2\u003enul\n@route add 200.201.0.0 mask 255.255.0.0 %gateway% 2\u003enul\n@route add 192.200.0.0 mask 255.255.0.0 %gateway% 2\u003enul\n\n@timeout /T 1 /NOBREAK\n@goto :loop\n```\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E8%8B%B1%E9%9B%84%E4%B9%8B%E6%97%85":{"title":"英雄之旅","content":"\n英雄之旅就是人类社会中的最常见的原型神话。\n\n![[Pasted image 20220929203142.png]]![[Pasted image 20220929203529.png]]\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E8%A1%8C%E4%B8%9A%E7%9A%84%E7%BB%86%E5%88%86%E6%A0%87%E5%87%86-%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E5%B1%82":{"title":"","content":"","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E9%80%90%E6%AD%A5%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%AA%E6%96%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%BC%8F":{"title":"逐步描述一个新模型的方式","content":"通过描述一个东西，能提供什么，能做什么来感知他。\n\n不要急着下定义，这会让自己丧失求知欲。\n\n有点像编程领域的[[鸭子类型]]的延伸。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6":{"title":"配置文件","content":"## 摘要\n\n\n## 问题、提示\n-  \n\n## 主要笔记\n-  [[云原生的十二因素]]-可配置化\n\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E9%97%AA%E5%AD%98%E5%8C%96":{"title":"","content":"根据IDC 在2019年12月31日的报道：“2019前三个季度，中国企业级全闪存存储阵列市场同比增长超过60%。IDC预测， 2019年全闪存存储市场仍将达到近50%的增长”。\n\n据报道，[[闪存颗粒]]价格逐年下降（按照30%～40%的降幅）\n\n闪存化的过程中，必然催生[[全闪存阵列]]的发展，其中的相关技术[[NVMe Over Fabric]]（简称NVMe-oF）借助[[RDMA]]等超低延时的传输协议，可远程访问[[SSD]]，解决了下一代数据中心在横向扩展 (Scale-out) 时所遭遇的性能、功能、容量三者之间难以取舍的权衡问题","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E9%99%8D%E6%9C%AC%E5%A2%9E%E6%95%88":{"title":"降本增效","content":"新词，顾名思义\n\n和大环境也有关，[[diary/2022-10-12]] 记录：由于[[疫情]]原因，全球[[经济]]都有萎缩的倾向，大部分企业寻求降本增效来活下去。\n\n[[Kubernetes 集群的降本增效]]","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B":{"title":"鸭子类型","content":"同 Duck-Type\n\n原理是：只要一个东西会“嘎嘎”叫，就认为它是一只鸭子。\n\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/AI-%E7%9A%84%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD":{"title":"AI 的基础设施","content":"AI Infra","lastmodified":"2022-10-21T07:56:51.677474693Z","tags":null},"/CTO-%E7%A1%AE%E5%AE%9E%E5%90%8C%E6%97%B6%E9%9C%80%E8%A6%81%E6%89%AE%E6%BC%94%E4%BC%98%E7%A7%80%E7%9A%84-engineer":{"title":"CTO 确实同时需要扮演优秀的 engineer","content":"\n### CTO 确实同时需要扮演优秀的 engineer\n需要对该角色设置一些评判满足标准，保证 CTO 满足达到标准[https://blog.southparkcommons.com/your-cto-should-actually-be-technical/](https://blog.southparkcommons.com/your-cto-should-actually-be-technical/)。该文章也在 HackerNews 上引起热议。","lastmodified":"2022-10-21T07:56:51.677474693Z","tags":null},"/CUDA-Windows-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA":{"title":"CUDA Windows 环境搭建","content":"引用[[anaconda 安装]]\n\n## Windows 环境搭建\n\n### CUDA\n\n[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads?target_os=Windows\u0026target_arch=x86_64\u0026target_version=11\u0026target_type=exe_local)\n\n下载了 windows 下的 CUDA 11.7 的 local 版本，2.5g，下载安装即可\n\n安装完之后重启，可以运行`nvidia-smi.exe`查看一下自己的显卡。\n\n### Conda （python 环境）\n\n下载 miniconda\n\n[https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html) 安装\n\n通过 `code $PROFILE` 启动 vscode 添加该环境到 powershell 自动启动脚本中\n\n```powershell\n# 启动 miniconda 环境\nD:\\miniconda\\shell\\condabin\\conda-hook.ps1 ;conda activate 'D:\\miniconda'\n```\n\n  \n### Pytorch \u0026 Jupyter\n\n因为有梯子，速度挺快，就不用镜像源了\n\n[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) 找到对应 CUDA 的版本，用的 pip 下载，快一些比 conda。\n\n`pip3 install torch torchvision torchaudio --extra-index-url [https://download.pytorch.org/whl/cu116](https://download.pytorch.org/whl/cu116)`\n\n  \n\n`pip install matplotlib numpy jupyterlab`\n\n## oneflow\n\n[https://docs.oneflow.org/master/index.html](https://docs.oneflow.org/master/index.html)\n\n`python3 -m pip install -f https://release.oneflow.info oneflow==0.8.0+cu112`\n\n### 动手学深度学习\n\n可以从这里下载大量的 nodebook 供学习  \n\n[https://github.com/d2l-ai/d2l-zh](https://github.com/d2l-ai/d2l-zh)","lastmodified":"2022-10-21T07:56:51.677474693Z","tags":null},"/ConfigMap":{"title":"ConfigMap","content":"Kubernetes 中的一个对象。可以理解为一个 yaml 文件专门用作 APP 的配置文件，提供了相应的\n- 挂载\n- 监控更新\n等功能。\n\n限制：只有 1m 大小","lastmodified":"2022-10-21T07:56:51.677474693Z","tags":null},"/DeepSpeed":{"title":"DeepSpeed","content":"#github\nIntro::: DeepSpeed是Microsoft推出的深度学习库，用于训练Megatron-Turing NLG 530B和BLOOM等大型模型，在训练、推理和压缩三方面都有所创新。\n\nLink::: https://github.com/microsoft/DeepSpeed\n\nDeepSpeed具有如下优点：\n\n-   可进行十亿乃至万亿级参数的稀疏/密集模型的训练/推理\n-   可实现超高系统吞吐量，可高效扩展至数千个GPU\n-   可在资源有限的GPU系统上进行训练/推理\n-   可实现前所未有的低延时与高推理吞吐量\n-   可用低成本实现压缩，从而大幅降低推理延迟并缩减模型大小","lastmodified":"2022-10-21T07:56:51.677474693Z","tags":null},"/Dockerfile-%E5%A4%9A%E8%A1%8C%E8%84%9A%E6%9C%AC":{"title":"Dockerfile 多行脚本","content":"```\n# syntax = docker/dockerfile:1.4\nFROM debian \n-RUN apt-get \u0026\u0026 \\ \n- apt-get install -y vim \n +RUN \u003c\u003ceot bash \n+ apt-get update \n+ apt-get install -y vim \neot\n```","lastmodified":"2022-10-21T07:56:51.677474693Z","tags":null},"/FairScale":{"title":"FairScale","content":"#github\nIntro::: FairScale是由Facebook Research开发的PyTorch扩展库，具备高性能，可用于大型训练。\nLink::: https://github.com/facebookresearch/fairscale\n\n#### FairScale的愿景如下：\n-  易用性，开发简单易懂的FairScale API，使用户方便上手。\n-  模块化，实现模块化，使用户可将多个FairScale API无缝加入其训练循环。\n-  性能，实现 FairScale API的高可扩展性和高效率。\nFairScale支持完全分片数据并行（FullyShardedDataParallel，FSDP），FSDP是扩展大型神经网络训练的推荐方法。","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Focus-and-Different":{"title":"Focus and Different","content":"## 摘要\n最近总结的 #原则 和 #观念 ： 意义这种东西是通过聚集一个层面产生的。\n\n## 问题、提示\n-  为什么会觉得[[生活没有意义？]] #query \n\t- 其实更应该聚焦自己的当下生活，就会发现任何以前评估没有太大影响的事件，都对自己至关重要。 \n\n## 主要笔记\n-  \n\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Go":{"title":"Go","content":"[[Golang]] 的简写，一门计算机语言，[[我]]比较擅长。","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/GoJS":{"title":"GoJS","content":"#github\nLink:::https://github.com/NorthwoodsSoftware/GoJS\nIntro:::JavaScript diagramming library for interactive flowcharts, org charts, design tools, planning tools, visual languages.\n\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/IaaS":{"title":"IaaS","content":"## 摘要\n\n\n## 针对笔记问的问题\n-  \n\n## 主要笔记\n-  \n\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Kubernetes-%E7%9A%84%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88":{"title":"Kubernetes 的性能瓶颈","content":"[[Kubernetes]] 有很多[[性能调优]]的参数。\n\n2022年10月12日：前年看到的限制是 5000 pod，印象里这个值没有太大提升。\n\n单 Node Pod 承载数：一般在 100 以下，也有强力[[Kubernetes 集群的降本增效]]到 140 的","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Kubernetes-%E9%9B%86%E7%BE%A4%E7%9A%84%E9%99%8D%E6%9C%AC%E5%A2%9E%E6%95%88":{"title":"Kubernetes 集群的降本增效","content":"通过 [[HPA]] 和 [[ClusterAutoscaler]] [[降本增效]]吧\n\n[[离线混部]]也是业界主要探索的，有不少[[开源项目]]\n\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/LSMs":{"title":"LSMs","content":"基于日志结构的合并树\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/LVM-%E9%87%8D%E5%90%AF%E9%9C%80%E8%A6%81%E9%87%8D%E6%96%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E5%90%A6%E5%88%99%E4%B8%A2%E5%A4%B1-vg-%E8%AE%BE%E5%A4%87%E7%9A%84%E9%97%AE%E9%A2%98":{"title":"LVM 重启需要重新初始化否则丢失 vg 设备的问题","content":"https://www.aboutyun.com/thread-16065-1-1.html\n\n设置  开机挂载 ，ok了  \n  \ncat /etc/rc.d/rc.local |grep cinder-volumes || echo 'losetup -f /var/lib/cinder/cinder-volumes \u0026\u0026 vgchange -a y cinder-volumes ' \u003e\u003e /etc/rc.d/rc.local\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Megatron-LM":{"title":"Megatron-LM","content":"Intro::: Megatron是NVIDIA应用深度学习研究团队研发的大规模Transformer语言模型训练框架，支持模型并行（张量并行、序列并行与流水并行）与多节点预训练（multi-node pre-training），目前已支持BERT、GPT和T5模型。\n#github\nLink::: https://github.com/NVIDIA/Megatron-LM","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/NVMe":{"title":"","content":"Non-Volatile Memory Host Controller Interface Specification，非易失性[[存储]]主机控制器接口标准\n\n- 为什么 NVMe 更快 #query \n\t- [[PCIe]]：总线带来更大带宽和更低延迟\n\t- 并行：传统的[[SATA]]设备只能支持一个队列，一次只能接收32条数据；而NVMe存储则支持最多64000个队列，每个队列有64000个条目。\n\t- [[SATA]] 协议理论最大传输速度 6.0Gbps","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/OneFlow":{"title":"OneFlow","content":"#github\nIntro:::\n-   兼容PyTorch的API对模型进行编程\n-   运用全局视角（Global View）API将模型扩展至n维并行执行或分布式执行\n-   用静态图编译器（Static Graph Compiler）进行模型加速/部署\nLink::: https://github.com/Oneflow-Inc/oneflow （论文：\nhttps://arxiv.org/abs/2110.15032 ）","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/PriceOps":{"title":"PriceOps","content":"什么是 PriceOps ?\n解释网址： https://priceops.org/\n\n做一下笔记：\n\n\tPriceOps 部分定义\n\t\n\tPriceOps 是一种实现迭代和灵活性的方法。它描述了一组通过有效管理固有复杂性来促进定价模型探索的实现属性。将此视为一组架构蓝图和最佳实践，可以帮助您持续开发和完善您的定价基础架构。\n\t\n\tPriceOps_不是_关于任何特定产品应如何定价或如何确定此类价格的规定性指南。相反，它是如何实施定价模型以最大限度地提高灵活性和稳定性的指南。\n\n我理解如下：\n\n将定价模型通过代码描述出来，现在是 json，因为有 code 极强的表达能力，所以可能可以从中看出定价对于产品的影响有哪些。从而做出调整。\n\n列出了支撑模型的五个支柱（英文：[[pillar]])\n\n1. 定价模型的定义：例如版本化代表不会影响之前的用户\n2. 用户的时间表：这样一个定价计划就可以规定在哪些时间里面可以使用哪些功能，并能有多少使用量\n3. 计量系统：用于收集所有用户使用信息，一个数据中心中存储，来帮助定价模型的更新\n4. 权限检查：这样应用程序代码只需要提供功能，不需要知道功能在哪些计划中。\n5. PriceOps 工具：为以上行为提供操作的工具\n\n我很喜欢这种 [[逐步描述一个新模型的方式]]，它还能持续迭代一个东西定义，如果一开始就下结论，这个模型就老死了。","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Pricing-as-Code":{"title":"Pricing as Code","content":"一种很新的东西.\n概念模型: [[PriceOps]]\n示例产品：[[Tier]]  https://www.tier.run/","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Request-For-Comments":{"title":"Request For Comments","content":"## 摘要\nRFC 不能修改，有可能是错的，他从相当意义上构成了互联网的历史。标准并不总是对的。\n## 问题、提示\n-  \n\n## 主要笔记\n- RFC由一系列草案组成，起始于1969年（第一个RFC文档发布于1969年4月7日，参见“RFC30年”，RFC2555”）\n- “RFC编辑者”：约翰•普斯特尔（Jon Postel）（前 30 年）现在是一个小组\n- 从哪里开始阅读 RFC #query \n\t- 查找RFC的规范位置是[RFC编辑器网站](https://rfc-editor.org/)。但是，正如我们将在下面看到的那样，RFC编辑器缺少一些关键信息，因此大多数人都使用 [tools.ietf.org](https://tools.ietf.org/)\n- [IETF介绍及RFC Draft撰写 | Louie's Blog](http://ylong.net.cn/How_to_write_RFC_draft.html)","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/SaaS":{"title":"","content":"2018 年吧，我还记得当时讲解的现场，经过一系列的推演和被教学，当时决定 SaaS 作为未来 5 年的研究方向， 现在回首，确实通过 SaaS 进入了云计算的领域，做到了 [[IaaS]] 层去啦。也对基础设施有更深的领悟。对于 IT 和互联网企业和物联网企业，一个扎实的基础设施，灵活扩展的[[规范标准]]，行业的深耕领域知识的结合，这些基础设施的构建将会是未来一个阶段的主旋律。当[[行业的细分标准-基础设施层|行业细分标准]]制定完成后，类似 FireStore 的 SaaS 将会较容易的做出来了。说不定这时候，反而阴差阳错的实现了当初的目标。","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Sophisticated-zh":{"title":"","content":"\n# Sophisticated\n\n## 发音\n\n- səˈfɪstɪkeɪtɪd\n- səˈfɪstɪˌketɪd\n\n## 词义\n\n### 能被表示程度的副词或介词词组修饰的形容词\n\n(机器、装置等)高级的，精密的;(方法)复杂的\n\n\u003e 蜜蜂之间所用的交流方式是昆虫中最为复杂的方式之一。\n\n### 能被表示程度的副词或介词词组修饰的形容词\n\n善于社交的;高雅时髦的;见过世面的\n\n\u003e 克劳德是一个很有魅力、见多识广的伙伴。\n\n### 能被表示程度的副词或介词词组修饰的形容词\n\n精明老练的;老于世故的\n\n\u003e 这些人是观察外交政策领域动向的行家里手。\n\n\n\n## \n\n\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Terraform-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%A6%BB%E7%BA%BF%E6%BA%90":{"title":"","content":"\n### 1. 创建配置文件\n\n`.terraformrc`是 [[Terraform]] CLI的配置文件\n\n```\nplugin_cache_dir  = \"/root/.terraform.d/terraform-plugin-cache\" \ndisable_checkpoint = true\nprovider_installation {\n  filesystem_mirror {\n    path    = \"/root/.terraform.d/terraform-plugin-cache\"\n    include = [\"registry.terraform.io/*/*\"]\n  }\n}\n```\n\n-   plugin_cache_dir 是插件的缓存目录（此目录需要提前创建不然init报错）\n-   disable_checkpoint 禁用 需要连接HashiCorp 提供的网络服务的升级和安全公告检查\n\n\n### 2. 进行初始化\n\n插件下载方式有两种：\n\n1.  通过 `terraform init` 自动下载 provider 插件；\n2.  登入`registry.terraform.io`手动到 `GitHub`下载，并按照目录结构存放到`plugin_cache_dir`;\n\n```bash\n❯ tree /root/.terraform.d\n/root/.terraform.d\n├── checkpoint_signature\n└── terraform-plugin-cache\n    └── registry.terraform.io\n        ├── coder\n        │   └── coder\n        │       └── 0.5.0\n        │           └── linux_amd64\n        │               └── terraform-provider-coder_v0.5.0\n        └── kreuzwerker\n            └── docker\n                └── 2.20.2\n                    └── linux_amd64\n                        └── terraform-provider-docker_v2.20.2\n```\n\n然后运行 `terraform init` 即可\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/Tier":{"title":"Tier","content":"https://www.tier.run/\n理念： [[Pricing as Code]] 、[[PriceOps]] ","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/anaconda-%E5%AE%89%E8%A3%85":{"title":"anaconda 安装","content":"\n下载地址： [https://www.anaconda.com/products/distribution](https://www.anaconda.com/products/distribution)\n\n  \n\n以 linux 为例，下载的是一个 .sh 的文件，通过 bash 运行他即可\n\n`bash Anaconda3-2020.11-Linux-x86_64.sh`\n\n会运行一段交互式脚本，确认安装配置。\n\n### 内网环境配置\n\n`conda config --add channels [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)`\n\n查看添加的镜像：\n\n`conda config --get channels`\n\n推荐使用搜到的 `.condarc`直接复制粘贴\n\n```bash\nchannels:\n- defaults\nshow_channel_urls: true\nchannel_alias: http://xxx.com/nexus/repository/anaconda\ndefault_channels:\n- http://xxx.com/nexus/repository/anaconda/pkgs/main\n- http://xxx.com/nexus/repository/anaconda/pkgs/free\n- http://xxx.com/nexus/repository/anaconda/pkgs/r\n- http://xxx.com/nexus/repository/anaconda/pkgs/pro\n- http://xxx.com/nexus/repository/anaconda/pkgs/msys2\ncustom_channels:\nconda-forge: http://xxx.com/nexus/repository/anaconda\nmsys2: http://xxx.com/nexus/repository/anaconda\nbioconda: http://xxx.com/nexus/repository/anaconda\nmenpo: http://xxx.com/nexus/repository/anaconda\npytorch: http://xxx.com/nexus/repository/anaconda\nsimpleitk: http://xxx.com/nexus/repository/anaconda\nauto_activate_base: false #用于关闭自动启用 base 环境\n```\n\n  \n\n`pip config set global.index-url [https://pypi.tuna.tsinghua.edu.cn/simple](https://pypi.tuna.tsinghua.edu.cn/simple)`\n\n### 环境创建\n\nfrom： https://www.cnblogs.com/xiaojianliu/p/13466666.html\n\n在Anaconda中conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。所以对虚拟环境进行创建、删除等操作需要使用conda命令。\n```bash\nconda 本地环境常用操作\n#获取版本号\nconda --version 或 conda -V\n\n#检查更新当前conda\nconda update conda\n\n#查看当前存在哪些虚拟环境\nconda env list 或 conda info -e\n\n#查看--安装--更新--删除包\n\nconda list：\nconda search package_name# 查询包\nconda install package_name\nconda install package_name=1.5.0\nconda update package_name\nconda remove package_name\nconda创建虚拟环境\n#创建名为your_env_name的环境\nconda create --name your_env_name\n#创建制定python版本的环境\nconda create --name your_env_name python=2.7\nconda create --name your_env_name python=3.6\n#创建包含某些包（如numpy，scipy）的环境\nconda create --name your_env_name numpy scipy\n#创建指定python版本下包含某些包的环境\nconda create --name your_env_name python=3.6 numpy scipy\n激活虚拟环境\n#Linux\nsource activate your_env_name\n\n#Windows\nactivate your_env_name\n退出虚拟环境\n#Linux\nsource deactivate your_env_name\n\n#Windows\ndeactivate env_name\n删除虚拟环境\nconda remove -n your_env_name --all\nconda remove --name your_env_name --all\n复制某个环境\nconda create --name new_env_name --clone old_env_name\n在指定环境中管理包\nconda list -n your_env_name\nconda install --name myenv package_name \nconda remove --name myenv package_name\n使用国内 conda 软件源加速\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\nconda config --set show_channel_urls yes\n```\n\n`conda create --name projector python=3.9` 创建一个 3.9 的环境\n\n安装包：\n\n`conda install matplotlib numpy jupyterlab`\n\n启动 jupyter lab\n\n`jupyter lab --ip 0.0.0.0 --port 8888`\n\n### jupyter 代码提示\n\nlab 本来就带自动补全的。按 `tab`键就可以。\n\nlsp\n\n[https://github.com/jupyter-lsp/jupyterlab-lsp](https://github.com/jupyter-lsp/jupyterlab-lsp)\n\npip install 'jupyterlab\u003e=3.0.0,\u003c4.0.0a0' jupyterlab-lsp\npip install 'python-lsp-server[all]'\n\n注意在侧边栏 extension manager 中启用安装的 extension。也可以手动在侧边栏搜索 lsp 安装而不用 pip 安装然后重启 jupyter。\n\n## 离线安装\n\n有时候需要离线\n\n[Python pip离线安装package方法总结（以TensorFlow为例）](https://imshuai.com/python-pip-install-package-offline-tensorflow)\n\n1.  pip download tensorflow\n2.  将目录内容拷贝到目标offline机器（比如/offline_package_dir），并目标offline机器执行pip install --no-index --find-links=file:/offline_package_dir tensorflow","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/blogs/Configmap-Secret-Manager":{"title":"Configmap/Secret Manager","content":"## Configmap/Secret Manager\n\n\u003ca name=\"YjhpG\"\u003e\u003c/a\u003e\n## ReadLink\n\n- [configmap manager](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/configmap/configmap_manager.go)\n- [pkg/kubelet/secret/secret_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/secret/secret_manager.go)\n\u003ca name=\"GuGtv\"\u003e\u003c/a\u003e\n\n## Configmap Manager\n\n```go\n// Manager interface provides methods for Kubelet to manage ConfigMap.\ntype Manager interface {\n    // Get configmap by configmap namespace and name.\n    GetConfigMap(namespace, name string) (*v1.ConfigMap, error)\n    \n    // WARNING: Register/UnregisterPod functions should be efficient,\n    // i.e. should not block on network operations.\n    \n    // RegisterPod registers all configmaps from a given pod.\n    RegisterPod(pod *v1.Pod)\n    \n    // UnregisterPod unregisters configmaps from a given pod that are not\n    // used by any other registered pod.\n    UnregisterPod(pod *v1.Pod)\n}\n```\n\n接口非常简单。\n\n1. GetConfigMap ： 通过 [[namespace]] 和 name 获取对应 [[ConfigMap]] 对象。\n1. RegisterPod(pod *v1.Pod)：把指定 Pod 对象 yaml 指定的 configmap 注册到 Controller 中管理\n1. UnregisterPod(pod *v1.Pod)：把指定 Pod 对象 yaml 指定的 configmap 从 Controller 中注册管理中删除，注意 ConfigMap 需要没有任何其他已注册的 Pod 引用（即无被依赖项）才可以删除\n\n当前代码中有两种 manager 的实现\n\n-`NewCachingConfigMapManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager`：该实现有两点逻辑\n   - 当一个 Pod 创建或者更新时，所有的 configmap 缓存都失效。\n   -  GetObject() 调用首先从本地缓存获取，失败则访问 APISever 并刷新 configmap 的缓存。\n\n```go\n// NewCachingConfigMapManager creates a manager that keeps a cache of all configmaps\n// necessary for registered pods.\n// It implement the following logic:\n// - whenever a pod is create or updated, the cached versions of all configmaps\n//   are invalidated\n// - every GetObject() call tries to fetch the value from local cache; if it is\n//   not there, invalidated or too old, we fetch it from apiserver and refresh the\n//   value in cache; otherwise it is just fetched from cache\nfunc NewCachingConfigMapManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager {\n\tgetConfigMap := func(namespace, name string, opts metav1.GetOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).Get(context.TODO(), name, opts)\n\t}\n\tconfigMapStore := manager.NewObjectStore(getConfigMap, clock.RealClock{}, getTTL, defaultTTL)\n\treturn \u0026configMapManager{\n\t\tmanager: manager.NewCacheBasedManager(configMapStore, getConfigMapNames),\n\t}\n}\n```\n\n- `NewWatchingConfigMapManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager`：\n   - 当一个 Pod 创建或者更新时，会对指定该 Pod 引用的资源，并且该资源未被其他 Pod 引用进行独立的 watch。\n   - GetObject() 调用首先从本地缓存获取\n\n```go\n// NewWatchingConfigMapManager creates a manager that keeps a cache of all configmaps\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, we start individual watches for all\n//   referenced objects that aren't referenced from other registered pods\n// - every GetObject() returns a value from local cache propagated via watches\nfunc NewWatchingConfigMapManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager {\n\tlistConfigMap := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).List(context.TODO(), opts)\n\t}\n\twatchConfigMap := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) {\n\t\treturn kubeClient.CoreV1().ConfigMaps(namespace).Watch(context.TODO(), opts)\n\t}\n\tnewConfigMap := func() runtime.Object {\n\t\treturn \u0026v1.ConfigMap{}\n\t}\n\tisImmutable := func(object runtime.Object) bool {\n\t\tif configMap, ok := object.(*v1.ConfigMap); ok {\n\t\t\treturn configMap.Immutable != nil \u0026\u0026 *configMap.Immutable\n\t\t}\n\t\treturn false\n\t}\n\tgr := corev1.Resource(\"configmap\")\n\treturn \u0026configMapManager{\n\t\tmanager: manager.NewWatchBasedManager(listConfigMap, watchConfigMap, newConfigMap, isImmutable, gr, resyncInterval, getConfigMapNames),\n\t}\n}\n\n```\n\n\u003ca name=\"gEtqm\"\u003e\u003c/a\u003e\n## Secret Manager\n\nsecret manager 除了资源类型和 configmap 不一样，其他逻辑相同，所以仅列出两种 secret manager 的初始化函数。\u003cbr /\u003e[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[secret /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/secret)[secret_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/secret/secret_manager.go)\n\n```go\n// NewCachingSecretManager creates a manager that keeps a cache of all secrets\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, the cached versions of all secrets\n//   are invalidated\n// - every GetObject() call tries to fetch the value from local cache; if it is\n//   not there, invalidated or too old, we fetch it from apiserver and refresh the\n//   value in cache; otherwise it is just fetched from cache\nfunc NewCachingSecretManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager {\n\tgetSecret := func(namespace, name string, opts metav1.GetOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().Secrets(namespace).Get(context.TODO(), name, opts)\n\t}\n\tsecretStore := manager.NewObjectStore(getSecret, clock.RealClock{}, getTTL, defaultTTL)\n\treturn \u0026secretManager{\n\t\tmanager: manager.NewCacheBasedManager(secretStore, getSecretNames),\n\t}\n}\n\n// NewWatchingSecretManager creates a manager that keeps a cache of all secrets\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, we start individual watches for all\n//   referenced objects that aren't referenced from other registered pods\n// - every GetObject() returns a value from local cache propagated via watches\nfunc NewWatchingSecretManager(kubeClient clientset.Interface, resyncInterval time.Duration) Manager {\n\tlistSecret := func(namespace string, opts metav1.ListOptions) (runtime.Object, error) {\n\t\treturn kubeClient.CoreV1().Secrets(namespace).List(context.TODO(), opts)\n\t}\n\twatchSecret := func(namespace string, opts metav1.ListOptions) (watch.Interface, error) {\n\t\treturn kubeClient.CoreV1().Secrets(namespace).Watch(context.TODO(), opts)\n\t}\n\tnewSecret := func() runtime.Object {\n\t\treturn \u0026v1.Secret{}\n\t}\n\tisImmutable := func(object runtime.Object) bool {\n\t\tif secret, ok := object.(*v1.Secret); ok {\n\t\t\treturn secret.Immutable != nil \u0026\u0026 *secret.Immutable\n\t\t}\n\t\treturn false\n\t}\n\tgr := corev1.Resource(\"secret\")\n\treturn \u0026secretManager{\n\t\tmanager: manager.NewWatchBasedManager(listSecret, watchSecret, newSecret, isImmutable, gr, resyncInterval, getSecretNames),\n\t}\n}\n```\n\n\u003ca name=\"RX3SN\"\u003e\u003c/a\u003e\n## cache_based_manager\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[cache_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/cache_based_manager.go)\n\n```go\n// cacheBasedManager keeps a store with objects necessary\n// for registered pods. Different implementations of the store\n// may result in different semantics for freshness of objects\n// (e.g. ttl-based implementation vs watch-based implementation).\ntype cacheBasedManager struct {\n    objectStore          Store\n\tgetReferencedObjects func(*v1.Pod) sets.String\n\n\tlock           sync.Mutex\n\tregisteredPods map[objectKey]*v1.Pod\n}\n```\n\n该 manager 代码位于  [/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[cache_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/cache_based_manager.go)，属于通用的 Manager 结构体工具，用于保留注册的  Pod 所必要引用的 kubernetes 对象（objects）\u003cbr /\u003e如何做到的呢？\u003cbr /\u003e通过 getReferencedObjects 字段，一个可以传入的成员函数，自定义实现用于从 v1.Pod 对象中获取到对应对象（或一组对象）的 name。流程如下：\n\n```go\nfunc (c *cacheBasedManager) RegisterPod(pod *v1.Pod) {\n    // 1. 获取名字\n\tnames := c.getReferencedObjects(pod)\n\tc.lock.Lock()\n\tdefer c.lock.Unlock()\n    // 2. 给每一个名字和 pod 的命名空间一起添加到 c.objectStore 中存储\n\tfor name := range names {\n\t\tc.objectStore.AddReference(pod.Namespace, name)\n\t}\n    // 3. 检查是否之前已经注册了该 Pod\n\tvar prev *v1.Pod\n\tkey := objectKey{namespace: pod.Namespace, name: pod.Name, uid: pod.UID}\n\tprev = c.registeredPods[key]\n    // 4. 用新注册的 pod 替换之前存储的注册 Pod 的信息.\n\tc.registeredPods[key] = pod\n    // 5. 删除旧 Pod 在 c.objectStore 中的引用信息,这是因为在上面第二步 Add 到 c.objectStore\n    // 中时,这些资源的引用次数又新增了一次,但实际上只是同一个 Pod 的引用,自然需要删除,当然,也有\n    // 可能新 Pod 已经不再引用目标资源了, Delete 函数在下面也处理这个情况\n\tif prev != nil {\n\t\tfor name := range c.getReferencedObjects(prev) {\n\t\t\t// On an update, the .Add() call above will have re-incremented the\n\t\t\t// ref count of any existing object, so any objects that are in both\n\t\t\t// names and prev need to have their ref counts decremented. Any that\n\t\t\t// are only in prev need to be completely removed. This unconditional\n\t\t\t// call takes care of both cases.\n\t\t\tc.objectStore.DeleteReference(prev.Namespace, name)\n\t\t}\n\t}\n}\n```\n\n1. 获取名字\n1. 给每一个名字和 pod 的命名空间一起添加到 c.objectStore 中存储\n1. 检查是否之前已经注册了该 Pod\n1. 用新注册的 pod 替换之前存储的注册 Pod 的信息.\n5. 删除旧 Pod 在 c.objectStore 中的引用信息,这是因为在上面第二步 Add 到 c.objectStore 中时,这些资源的引用次数又新增了一次,但实际上只是同一个 Pod 的引用,自然需要删除,当然,也有可能新 Pod 已经不再引用目标资源了, Delete 函数在下面也处理这个情况\n\n\u003ca name=\"aQgQM\"\u003e\u003c/a\u003e\n### ttl ObjectStore\n\ncache_based 的 objectStore 通过 ttl 设置缓存有效期。\n\u003ca name=\"qzaxL\"\u003e\u003c/a\u003e\n\n## watch_based_manager\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet)[util /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util)[manager /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/pkg/kubelet/util/manager)[watch_based_manager.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/util/manager/watch_based_manager.go)\u003cbr /\u003e可以看到，watch_based_manager 最后使用了 NewCacheBasedManager ，所以 watch_based_manager  和 cache_based_manager 不同的是 ObjectStore 字段。\n\n\n```go\n// NewWatchBasedManager creates a manager that keeps a cache of all objects\n// necessary for registered pods.\n// It implements the following logic:\n// - whenever a pod is created or updated, we start individual watches for all\n//   referenced objects that aren't referenced from other registered pods\n// - every GetObject() returns a value from local cache propagated via watches\nfunc NewWatchBasedManager(\n\tlistObject listObjectFunc,\n\twatchObject watchObjectFunc,\n\tnewObject newObjectFunc,\n\tisImmutable isImmutableFunc,\n\tgroupResource schema.GroupResource,\n\tresyncInterval time.Duration,\n\tgetReferencedObjects func(*v1.Pod) sets.String) Manager {\n\n\t// If a configmap/secret is used as a volume, the volumeManager will visit the objectCacheItem every resyncInterval cycle,\n\t// We just want to stop the objectCacheItem referenced by environment variables,\n\t// So, maxIdleTime is set to an integer multiple of resyncInterval,\n\t// We currently set it to 5 times.\n\tmaxIdleTime := resyncInterval * 5\n\n\t// TODO propagate stopCh from the higher level.\n\tobjectStore := NewObjectCache(listObject, watchObject, newObject, isImmutable, groupResource, clock.RealClock{}, maxIdleTime, wait.NeverStop)\n\treturn NewCacheBasedManager(objectStore, getReferencedObjects)\n}\n```\n\nwatch_based_manager  通过 watch 而不是简单的 ttl 去确认或者刷新缓存。\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/blogs/How-Cloud-Develop-Kit-from-Google-designed-the-docstore-interface":{"title":"How Cloud Develop Kit from Google designed the docstore interface","content":"\n## How [[Cloud Develop Kit]] from Google designed the docstore interface\n\n\u003ca name=\"ljgKZ\"\u003e\u003c/a\u003e\n## Refer\n- [Docstore · Go CDK](https://gocloud.dev/howto/docstore/)\n- [urls.go - google/go-cloud - Sourcegraph](https://sourcegraph.com/github.com/google/go-cloud@master/-/blob/docstore/mongodocstore/urls.go)\n- [driver.go - Go](https://cs.opensource.google/go/go/+/refs/tags/go1.18.3:src/database/sql/driver/driver.go)\n\u003ca name=\"Eu7vN\"\u003e\u003c/a\u003e\n\n## Design objectives: \n\n**through the abstraction layer, we can mask differences, provide services in a standardized way, and configure business applications through description files.** \n\n**Provides design ideas and guidelines for applications that use document storage.** \n\n\u003ca name=\"yaqn6\"\u003e\u003c/a\u003e\n## Intro\ncommon in [[MongoDB]] [document Storage](https://en.wikipedia.org/wiki/Document-oriented_database) provides an abstraction layer. \n\nDocument Storage is a service that stores data in semi-structured JSON-like documents. These documents are grouped into collections. Like other NoSQL databases, document storage is modeless. \n\nThe design needs to support adding, retrieving, modifying, and deleting documents. \ndocstore Driver implementation of various services, including cloud and local solutions. You can develop applications locally and then reconfigure them to multiple cloud providers with minimal initialization. \n\u003ca name=\"FCVI2\"\u003e\u003c/a\u003e\n## 设计\n\u003ca name=\"fg6xL\"\u003e\u003c/a\u003e\n### Structuring Portable Code \nStructuring Portable Code the non-interface design imitates the database/SQL package of golang and wraps the existing common logic into the structure. The internal fields of the structure are driver interfaces. The method provided externally is the method corresponding to the structure rather than the implementation of the direction provided driver.\n\n\u003e The advantage of this design is that there is no need to implement general logic processing for each interface, and the code can be transplanted. In some cases, you only need to add and modify methods on the structure and do not need to destroy the method design in the interface. You can also mask some assertion logic. When switching different drivers, users do not need to determine the implementation of some optional interfaces.\n\n[Structuring Portable Code · Go CDK](https://gocloud.dev/concepts/structure/)\n\n[sql package - database/sql - Go Packages](https://pkg.go.dev/database/sql#DB)\n\n![[blogs/Pasted image 20221011180052.png]]\n\n\u003cbr /\u003eCode like below：\n```go\n// Define\ntype DB interface{\n    Exec(sql string)error\n}\n\n// Realize\ntype mysql struct {}\nfunc (m *mysql) Exec(sql string) error {return nil}\n\n// Execute\nsql.Database.Exec(\"\")\n```\n```go\n// package and structure\npackage sql\n\ntype DB struct {\n    driver driver.DB\n}\n\n// higher level logic\nfunc (db *DB) AnySignature(anyParams string) (anyReturn error) {\n    //... \n    db.driver.Exec(\"...\")\n    //...\n    return nil\n}\n// Define\npackage driver \n\ntype DB interface{\n    Exec(sql string)error\n}\n\n// Realize\ntype mysql struct {}\nfunc (m *mysql) Exec(sql string) error {return nil}\n\n// Execute\nsql.DB.AnySignature(\"\")\n```\n\u003ca name=\"xSKiY\"\u003e\u003c/a\u003e\n### Actions List\nFor MongoDB, batch processing can be carried out to improve efficiency. As the shielding layer of packaging, we hope to obtain this benefit according to the actual processing of driver. A queue or cache is required to submit a batch operation.\n\n[Batch write operations-MongoDB-CN-Manual](https://docs.mongoing.com/mongodb-crud-operations/bulk-write-operations)\n\n- [x] I think it is enough to undertake Google Go CDK design \n\n\u003ca name=\"g9Zj6\"\u003e\u003c/a\u003e\n### Driver Map \u0026 Opener\nInherited from the Mysql Driver registration method, through the golang standard import_\" github.com/xxx/driver\" different database drivers can be introduced. The principle is to use a global Map.\n\n[[Golang]] CDK has upgraded the Opener feature. The original custom URL Parsing method is \"mysql\", \"user:password@/dbname\" the features of the new version are blob+file:///dir even \u003capi\u003e+ \u003ctype\u003e+ prefix (e.g. blob+bucket+file:///dir) for Google Cloud SDK, the same URL can provide different functions. However, in our opinion, this function does not have much effect for the time being, so we will block their design. \n\n\u003ca name=\"MrRZi\"\u003e\u003c/a\u003e\n### Dependency Injection wire \nGo CDK use the wire project to inject dependencies to automatically switch the structure of different backend providers to the SDK. Different from the way Dapr accesses different services, Dapr uses the yaml description to determine the different plug-ins that are enabled. \n\nFor example, you need wire.Build() indicates the new function of the driver to be introduced. \n\nIt has little impact on this project and may not be added for the time being. \n\n\u003ca name=\"tUMOU\"\u003e\u003c/a\u003e\n\n### UUID usage\n\nmongoDB, each entry must have a Key, which can be passed through parameters. \n```go\ntype Player struct {\n    ID   interface{} `docstore:\"_id,omitempty\"`\n    Name string\n}\n```\n\nThe simplest is to indicate the_id field directly in the structure. \n\n```go\ndocstore.OpenCollection(context.Background(), \"mongo://my-db/my-coll?id_field=name\")\n```\nYou can also use the URL Parameter in the high-level abstraction. The following example specifies the_ID as the name field of the above-mentioned high-level abstraction, which is used by the underlying layer `mongodocstore.OpenCollection` mapping relationship, will automatically generate mongo official driver type `primitivie.ObjectID `\n\n```go\ncoll, err := mongodocstore.OpenCollection(mcoll, \"id\", nil)\n\ntype IDer struct {\n\tID primitive.ObjectID\n}\n```\n\nyou can also use `mongodocstore.OpenCollectionWithIDFunc` to specify how to generate an ID.\n\n```go\nnameFromDocument := func(doc docstore.Document) interface{} {\n    return primitive.NewObjectID()\n}\ncoll, err := mongodocstore.OpenCollectionWithIDFunc(mcoll, nameFromDocument, nil)\n```\n\n\u003ca name=\"F0eWR\"\u003e\u003c/a\u003e\n## Summary \nWe have completed the access design and understanding of Document Store and can perform basic operations on adding, deleting, modifying, and querying docstores. Next, we will build service applications based on this layer of abstraction. \n\nFor special functions of different docstores, you can add them to docstore to determine whether they are target-driven and change the method of external exposure.\n\n\u003ca name=\"E8DHr\"\u003e\u003c/a\u003e\n## function \n\nthe following shows the functions of the library. \n\n\u003ca name=\"hVYIf\"\u003e\u003c/a\u003e\n### Connect MongoDB\nThe default mongo driver uses MONGO_SERVER_URL link to the server, so you can use code to set it here or directly set it by using environment variables. \n\nthe following meaning is from mongodb://localhost:27017 the link on the server is called `my-db`in the database `my-coll` document. The unique field name of mongo is `name`. \n\n```go\nos.Setenv(\"MONGO_SERVER_URL\", \"mongodb://localhost:27017\")\n\ncoll, err := docstore.OpenCollection(context.Background(), \"mongo://my-db/my-coll?id_field=name\")\ndefer coll.Close()\n```\n\u003ca name=\"HM4A2\"\u003e\u003c/a\u003e\n### Corresponding display structure \n```go\ntype Player struct {\n\tName             string `docstore:\"name,omitempty\"`\n\tScore            int\n\tDocstoreRevision interface{}\n}\n```\n\u003ca name=\"YFPTq\"\u003e\u003c/a\u003e\n### Create \n```go\ncoll.Create(ctx, \u0026Player{Name: \"Pat\", Score: 7}); \n```\n\u003ca name=\"CuHHJ\"\u003e\u003c/a\u003e\n### Get \n```go\ncoll.Get(ctx, \u0026Player{Name: \"Pat\"});\n```\n\u003ca name=\"syvtX\"\u003e\u003c/a\u003e\n### Queries \nyou may need to manually create indexes to complete the query function. \n```go\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"gocloud.dev/docstore\"\n)\n\n// Ask for all players with scores at least 20.\niter := coll.Query().Where(\"Score\", \"\u003e=\", 20).OrderBy(\"Score\", docstore.Descending).Get(ctx)\ndefer iter.Stop()\n\n// Query.Get returns an iterator. Call Next on it until io.EOF.\nfor {\n\tvar p Player\n\terr := iter.Next(ctx, \u0026p)\n\tif err == io.EOF {\n\t\tbreak\n\t} else if err != nil {\n\t\treturn err\n\t} else {\n\t\tfmt.Printf(\"%s: %d\\n\", p.Name, p.Score)\n\t}\n}\n```\n\u003ca name=\"zif5K\"\u003e\u003c/a\u003e\n### Update a single field of an Update entry\n```go\npat2 := \u0026Player{Name: \"Pat\"}\nerr := coll.Actions().Update(pat, docstore.Mods{\"Score\": 15}).Get(pat2).Do(ctx)\n```\n\u003ca name=\"VJYOy\"\u003e\u003c/a\u003e\n### Replace \ncompletely replace the entire entry \n```go\ncoll.Replace(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"QjVog\"\u003e\u003c/a\u003e\n### Put \nthe Put function is equivalent to CreateOrUpdate\n```go\ncoll.Put(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"wzyMJ\"\u003e\u003c/a\u003e\n### Delete \n```go\ncoll.Delete(ctx, \u0026Player{Name: \"Pat\", Score: 15})\n```\n\u003ca name=\"wer1V\"\u003e\u003c/a\u003e\n### More examples \n\n- [CLI Sample](https://github.com/google/go-cloud/tree/master/samples/gocdk-docstore)\n- [Order Processor sample](https://gocloud.dev/tutorials/order/)\n- [docstore package examples](https://godoc.org/gocloud.dev/docstore#pkg-examples)\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/blogs/Node-Status-Manager":{"title":"Node Status Manager","content":"## Node Status Manager\n\n\u003ca name=\"eQr2o\"\u003e\u003c/a\u003e\n## ReadLink\n- [pkg/kubelet/nodestatus/setters.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/pkg/kubelet/nodestatus/setters.go)\n- [/](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg/kubelet)[kubelet_node_status.go](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go)\n\n\u003ca name=\"HvtiD\"\u003e\u003c/a\u003e\n## Directory Layout\n```go\npkg/kubelet/nodestatus\n |- setters.go\n |- setters_test.go\n```\n\u003ca name=\"ABxjo\"\u003e\u003c/a\u003e\n## Setter\n```go\n// Setter modifies the node in-place, and returns an error if the modification failed.\n// Setters may partially mutate the node before returning an error.\ntype Setter func(node *v1.Node) error\n```\nthe Setter function defines a function that performs operations on the v1.Node object. If an error is returned, the Node object may also be changed. \nFrom the function definition, you can see its usage: use functions to generate different setters for a class of modification of Node objects. In this way, you can modify the state of a Node. \nUse the simplest func GoRuntime() Setter example: \n```go\n// GoRuntime returns a Setter that sets GOOS and GOARCH on the node.\nfunc GoRuntime() Setter {\n\treturn func(node *v1.Node) error {\n\t\tnode.Status.NodeInfo.OperatingSystem = goruntime.GOOS\n\t\tnode.Status.NodeInfo.Architecture = goruntime.GOARCH\n\t\treturn nil\n\t}\n}\n```\nthis mode belongs to the [[middleware]] operation mode. You can contact middleware for understanding. \n\u003ca name=\"E3eSp\"\u003e\u003c/a\u003e\n## Setter List\nwe learned the setter mode changed by Node status. Currently, the code contains the following 12 setters:\n- **NodeAddress** returns a Setter that updates address-related information on the node.：updates address-related fields, such as IP address and hostname (typically the hostname variable in kubelet). \n- **MachineInfo**returns a Setter that updates machine-related information on the node.：updates fields related to host information, such as the maximum number of pods, the number of pods allocated to each core, and the number of resources. \n- **VersionInfo**returns a Setter that updates version-related information on the node.：containerRuntime version, cadvisor version\n- **DaemonEndpoints**returns a Setter that updates the daemon endpoints on the node.\n- **Images**returns a Setter that updates the images on the node.：updates image information. \n- **GoRuntime**returns a Setter that sets GOOS and GOARCH on the node.：GOOS GOARCH information \n- **ReadyCondition** returns a Setter that updates the v1.NodeReady condition on the node.：determines whether the node is in the Ready state from Kubelet fields such as the error return function in the runtimeState. \n- **MemoryPressureCondition**returns a Setter that updates the v1.NodeMemoryPressure condition on the node.\n- **PIDPressureCondition**returns a Setter that updates the v1.NodePIDPressure condition on the node.\n- **DiskPressureCondition**returns a Setter that updates the v1.NodeDiskPressure condition on the node.\n- **VolumesInUse**returns a Setter that updates the volumes in use on the node.\n- **VolumeLimits**returns a Setter that updates the volume limits on the node.\n\n\nSetter 的入参通常是 Kubelet 中的字段，自然使用是通过 [[Kubelet]] 去初始化使用。\n\u003ca name=\"uRi9a\"\u003e\u003c/a\u003e\n## Kubelet Node Status\n[/](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e)[pkg /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg)[kubelet /](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/tree/pkg/kubelet)[kubelet_node_status.go](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go)\n\u003ca name=\"LGGvP\"\u003e\u003c/a\u003e\n###  Setter 使用处\nafter all setters are initialized in the defaultNodeStatusFuncs function, the function returns a Setter array. \n\n[kubelet_node_status.go? L613](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L613)\n\n```go\n// defaultNodeStatusFuncs is a factory that generates the default set of\n// setNodeStatus funcs\nfunc (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error {\n\t// if cloud is not nil, we expect the cloud resource sync manager to exist\n\tvar nodeAddressesFunc func() ([]v1.NodeAddress, error)\n\tif kl.cloud != nil {\n\t\tnodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses\n\t}\n\tvar validateHostFunc func() error\n\tif kl.appArmorValidator != nil {\n\t\tvalidateHostFunc = kl.appArmorValidator.ValidateHost\n\t}\n\tvar setters []func(n *v1.Node) error\n\tsetters = append(setters,\n\t\tnodestatus.NodeAddress(kl.nodeIPs, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),\n\t\tnodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,\n\t\t\tkl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent),\n\t\tnodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),\n\t\tnodestatus.DaemonEndpoints(kl.daemonEndpoints),\n\t\tnodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),\n\t\tnodestatus.GoRuntime(),\n\t)\n\t// Volume limits\n\tsetters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))\n\n\tsetters = append(setters,\n\t\tnodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),\n\t\tnodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),\n\t\tnodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),\n\t\tnodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager.Status, kl.shutdownManager.ShutdownStatus, kl.recordNodeStatusEvent),\n\t\tnodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),\n\t\t// TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event\n\t\t// and record state back to the Kubelet runtime object. In the future, I'd like to isolate\n\t\t// these side-effects by decoupling the decisions to send events and partial status recording\n\t\t// from the Node setters.\n\t\tkl.recordNodeSchedulableEvent,\n\t)\n\treturn setters\n}\n```\nThe array is assigned to the setNodeStatusFuncs of kubelet.\n```go\n\t// Generating the status funcs should be the last thing we do,\n\t// since this relies on the rest of the Kubelet having been constructed.\n\tklet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()\n```\n\u003ca name=\"n0m8P\"\u003e\u003c/a\u003e\n## SyncNodeStatus Procedure\nhow do Kubelet use these Kubelet? The core is syncNodeStatus functions.\n\n[kubelet_node_status](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L435)\n```go\n// syncNodeStatus should be called periodically from a goroutine.\n// It synchronizes node status to master if there is any change or enough time\n// passed from the last sync, registering the kubelet first if necessary.\nfunc (kl *Kubelet) syncNodeStatus() {\n\tkl.syncNodeStatusMux.Lock()\n\tdefer kl.syncNodeStatusMux.Unlock()\n\n\tif kl.kubeClient == nil || kl.heartbeatClient == nil {\n\t\treturn\n\t}\n\tif kl.registerNode {\n\t\t// This will exit immediately if it doesn't need to do anything.\n\t\tkl.registerWithAPIServer()\n\t}\n\tif err := kl.updateNodeStatus(); err != nil {\n\t\tklog.ErrorS(err, \"Unable to update node status\")\n\t}\n}\n```\nsyncNodeStatus the function is called periodically in goroutine to synchronize the node status to the master.\n\u003ca name=\"bdMgb\"\u003e\u003c/a\u003e\n### 入口 Entry\ncurrently, it is called in three places: \n\n1. [kubelet.go? L1428:26](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet.go?L1428:26)\nin the Run function of the Kubelet, start goroutine for periodic synchronization. \n```go\ngo wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop)\n```\n\n2. [kubelet.go? L2433:7](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet.go?L2433:7)\n: performs one-time synchronization in the fastStatusUpdateOnce function.\n```go\nfunc (kl *Kubelet) fastStatusUpdateOnce() {\n\tfor {\n\t\t...\n        kl.syncNodeStatus()\n        return\n\t}\n}\n```\n\n3. [nodeshutdown_manager_linux.go? L283:11](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go?L283:11)\n: it is called in the start() function of nodeshutdownmanager, which is actually a goroutine and is triggered only after the shutdown event is received from the channel.\n```go\nif isShuttingDown {\n    // Update node status and ready condition\n    go m.syncNodeStatus()\n\n    m.processShutdownEvent()\n} \n```\n\n\u003ca name=\"FKaYq\"\u003e\u003c/a\u003e\n### 注册 RegisterWithAPIserver\nif kubelet needs to be registered, a for loop is executed to wait for registration to the APIServer.\n```go\nfor {\n    time.Sleep(step)\n    step = step * 2\n    if step \u003e= 7*time.Second {\n        step = 7 * time.Second\n    }\n\n    // 1. 获取 node 对象及其信息\n    node, err := kl.initialNode(context.TODO())\n    if err != nil {\n        klog.ErrorS(err, \"Unable to construct v1.Node object for kubelet\")\n        continue\n    }\n\n    klog.InfoS(\"Attempting to register node\", \"node\", klog.KObj(node))\n    // 2. 注册到 APIServer 中去\n    registered := kl.tryRegisterWithAPIServer(node)\n    if registered {\n        klog.InfoS(\"Successfully registered node\", \"node\", klog.KObj(node))\n        kl.registrationCompleted = true\n        return\n    }\n}\n```\n\n1. node, err := kl.initialNode(context.TODO()) : obtains the node object and its information. \n2. registered := kl.tryRegisterWithAPIServer(node) : Register to APIServer \n\n\u003ca name=\"bj9z5\"\u003e\u003c/a\u003e\n### Use Setter\n[Function tryUpdateNodeStatus (kubelet_node_status.go? L470:20)](https://sourcegraph.com/github.com/kubernetes/kubernetes@d2c5779dadc9ed7a462c36bc280b2f9a200c571e/-/blob/pkg/kubelet/kubelet_node_status.go?L470:20)\n: the processing part of the volumeManager is omitted.\n```go\n// tryUpdateNodeStatus tries to update node status to master if there is any\n// change or enough time passed from the last sync.\nfunc (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error {\n    originalNode := node.DeepCopy()\n    ...\n\tkl.setNodeStatus(node)\n    ...\n\t// Patch the current status on the API server\n\tupdatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node)\n    ...\n\treturn nil\n}\n```\nkl.setNodeStatus just traverses all the Setter functions we mentioned just now.\n\n```go\nfunc (kl *Kubelet) setNodeStatus(node *v1.Node) {\n\tfor i, f := range kl.setNodeStatusFuncs {\n\t\tklog.V(5).InfoS(\"Setting node status condition code\", \"position\", i, \"node\", klog.KObj(node))\n\t\tif err := f(node); err != nil {\n\t\t\tklog.ErrorS(err, \"Failed to set some node status fields\", \"node\", klog.KObj(node))\n\t\t}\n\t}\n}\n```\n\u003ca name=\"GWa5D\"\u003e\u003c/a\u003e\n## Conclusion\nwe have learned: \n1. what are the change functions of the Node Status and what rules are followed to sign the function.\n2. how to register a Setter function to a Kubelet. \n3. Kubelet when these setters are called to change the status of a Node. \n\nNext: \n\n1. you can try to add a custom setter function. \n2. Kubernetes code is not as neat as the design. Some todo can be changed after reading this article and code. Try to decouple the code. (You can also find it by searching todo in the code.)\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/blogs/Open-Source-History-of-Dapr-project":{"title":"Open Source History of Dapr project","content":"[[## Open Source]] History of [[Dapr]] project\n\nAt the beginning of this open-source column, I wrote this article to describe the birth and development of open-source projects, express my views on the open-source community and ecology, and share it with you. \n\nSome opinions are out of personal perspective, and there are inevitably some mistakes and mistakes. Please forgive me and correct me. \n\n## Background\nbefore the birth of the Dapr project, I would like to explain the current situation of the Dapr project for readers to understand the project itself. \n\nDapr is a [[CNCF]] community-driven open source project with Microsoft as its contributor. Microsoft, according to the author, the first author should be [[Bai Haishi]] and [[Yaron]] (he is also the author of the Dapr Learning Manual, who proposed [[OAM]] and Dapr). \n\nThe work objectives of the Dapr project are described as follows: \n\nDapr is a portable, event-driven runtime that enables any developer to quickly build flexible, stateless, and stateful applications that can run on cloud platforms or edge computing. \nSome community students think Dapr is the next form of the service mesh, and some people also call this runtime software of the new era [[mecha]] (mecha), mecha provides distributed capabilities for business applications, just like the operator wearing a mecha, to do what he could not have done.\n\nThe following figure shows Bilgin Ibryam. Multi-Runtime Microservices Architecture \n\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658032639778/CRnDlZMU-.png align=\"left\")\n\ntherefore, the Dapr project is open-source software that provides distributed capabilities for modern distributed applications. Currently, it is open-source on GitHub and has gained 18.6K stars, which is very popular. \n\n## Birth \nTracing back to open source on GitHub, the first submission was on June 21, 2019, \nthe birth of open-source projects is usually accompanied by the discovery of practical problems. \n\nBefore saying the problems solved by the Dapr project, there is another project that has to be mentioned, Microsoft's OAM(Open Application Model). \n\nThe two projects have been known to the public for 19 years. I remember that the co-sponsor of OAM is Ali. At that time, Kubernetes was very popular, and the problems on the computing scheduling platform were Kubernetes solved by Golang's killer application. \n\nHowever, using Kubernetes puts forward more and higher requirements for Developers, especially its new concept, which covers different APIs and unique working methods. \n\nHow to solve this problem? \n\nAny problem in the field of computer science can be solved by adding an indirect intermediate layer.\n\nIt is believed that smart readers, based on their current knowledge, have already thought that if a new design language can be used as the middle layer to block the similarities and differences of infrastructure developers do not need to pay attention to and focus on business coding, can it be solved? \n\nIn this way, OAM is naturally ready to come out. (If you are concerned about OAM, you can learn about the implementation of this project standard in Alibaba, namely Kubevela project, this project has great potential)\n\nDapr came up with an idea when Bai Haishi and his Israeli colleagues discussed OAM Yaron Schneider. It designed a new programming mode to encapsulate the common functions of the distributed system into Sidecar(Kubernetes concept, description, and business application in the same Pod container) and expose them to developers through HTTP or gRPC (two common transmission modes, which are compatible with most applications). \n\nThe idea is named Distributed Application Runtime, or Dapr for short. [This paragraph is taken from an interview with Bai Haishi, the founder of OAM and DAPR: a simple idea of a 33-year senior programmer -Zhang Shanyou]] \n\nDapr provides several new features to help solve the problems: \n\nthe first is to provide services in the form of Sidecar. In the container orchestration platform, Sidecar provides services in a non-intrusive way. \n\nFor example, Envoy Sidecar acts as a proxy for routing and forwarding. It is independent of major applications and therefore has cross-language features. Users can reuse logic without binding to a programming language, which is especially useful in the microservice era. \n\nThe second is the concept of Building Block, which allows Dapr users to customize different Building blocks, instead of forcing users to use distributed functions provided by Dapr for all functions.\n\n## Open source \n\nAfter talking for so long, I finally talked about the open-source features of the Dapr project. \nThe benefits of open source can be seen in the summary of my other article. This article will not go into detail, but mainly explore the reasons why Dapr needs to open source and provide material examples for everyone to understand the open source operation mode. \n\nDapr can be analyzed from the positioning of its general distributed runtime software, and the standard is its core! \n\nStandards cannot be achieved by one person or a company. It is necessary to strengthen Dapr's influence and promote its designation of standards that are uniformly recognized by everyone. It is the only choice to establish a community of common contributions through open source.\n\nIt is not only a matter of standards. Dapr, as an application in the new era, naturally has many new ideas, which need to be verified. A large number of engineers need to be invested in the verification of the programming mode. \n\nThis part of manpower expenditure and verification cost is extremely large. The continuous development of the project can only be supported by the rapid discussion of design, implementation, and community verification in the form of Community co-construction. \n\nTherefore, human resources are also considered in most open-source projects. \n\nFinally, reach users. \n\nWhen Dapr is a user-oriented project, there are developers who are more enthusiastic than open-source communities. Open-source is the best choice to make Dapr's development closer to users and the wide application of cloud developers that it wants to achieve. \n\nWe recommend two projects to observe the popularity and activity distribution of open-source projects ( [[Star-History]] and [[OSSInsight]] ), which are [[Bytebase]] and [[PingCap]] open-source tools. One picture wins thousands of words, and two pictures are attached to show its function. \n\nFigure 1: Star harvest trend of open source Dapr project\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658032918444/FsLL7iu_d.png align=\"left\")\n\n## Development\nSince the Dapr project is to serve developers, it is natural to investigate the main functions that developers use in the programming and provide them to users as different building blocks. \n\nCurrently, the capabilities it provides include state management, service development, message sending and receiving, publishing and subscription, security information management, Actor mode (originated from the Orleans project in Microsoft's dotnet ecosystem), etc. In the initial form of state management development, concurrent control, version management, and other capabilities are also added. \n\nNow, building blocks such as distributed locks and workflows are gradually added. These new functions and new building blocks are all built by community users' needs.\n\nIt can be seen from this that Dapr's open source strategy has achieved remarkable results. \n\nThe emergence of Dapr also coincides with the wave of XaaS. It reduces the occupation of the edge environment (more than 50 M binary, only 4 M memory is needed during operation), provides edge devices and applications with low capability, flexibly switches between edge environment and cloud, and supports multiple operating environments, which are its excellent sources of competitiveness. \n\nThe development evaluation of an open source project must pay attention to its related ecology. Dapr, as a similar infrastructure project, will discuss two ecosystems. \n\nOne is the ecosystem that supports the Dapr project operation. That is, driven by various Building blocks, their ecology determines which infrastructure Dapr users can apply.\n\nTake [[PubSub]] as an example. Common message queue drivers such as Kafka, Redis, NatsStreaming, and Pulsar provide the runtime capability in the publish/subscribe mode. \n\nThe ecosystem in this area is rich and colorful. The core problem is that drivers are contributed to the community by themselves. The code quality and the functions provided during application runtime are uneven. It can be seen that the idea of standardization cannot be easily achieved in the real world. \n\nOne is the Dapr-based project built on it. This ecology can also be reflected in the cases where most companies use Dapr. \n\nThe main users of Dapr started from the founders Microsoft and Ali, and now companies such as [[Qingyun]] have participated in the co-construction and produced many projects and practical cases. \n\nTaking Microsoft as an example, users who serve it can easily and painlessly switch the underlying dependencies on the cloud (for example, switching message queues from Rabbitmq to kafka). \nFor example, Alibaba provides a large number of distributed capabilities for function applications in its functional computing platform. \nFor example, Ant Financial has developed a layotto project based on its excellent ServiceMesh development experience, IT has implemented the distributed runtime concept that conforms to its own IT infrastructure (and is open-source).\nFor example, Qingyun's Openfunction is also built using Dapr in the function computing platform.\n\nEven Microsoft has launched a commercial product container app based on Dapr, which allows users to write function-level services. The infrastructure is provided by context. \nDapr provides these services with the choice of only focusing on business code logic. \n\nThe developer ecosystem of open-source projects is an important criterion. The number of issues created, the speed of response, the richness of proposal submission, the degree of the active contributor, the entry and loss of new contributors and core contributors, and other indicators are all important bases for us to evaluate the developer ecosystem. This section can praise [[OSSInsight]] project, which provides you with query services through the website and provides us with powerful data for evaluating open-source projects. \n\nFigure 2: analysis of open source activities of the Dapr project\n\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1658033060286/sUkAuPA7c.png align=\"left\")\n\n## Trend \ncompetition trend of domain software: Hot open-source projects actually symbolize the main competition fields in the current industry. PaaS, which took Kubernetes as the core in the past few years, IaaS, which was recently represented by Infra as Code, DevOps and Security, and SaaS and FaaS, which will further compete fiercely in the future, provide better value-added services. Different fields have their own solutions. We can see how to provide more valuable services from the open-source ecosystem. \n\nThe development trend of programmers: modern developers are generally faced with anxiety problems. As programmers, some of our work contents are boring, but with the passion for programming and the pursuit of a career, we can develop various innovative achievements in our daily work, which may not only achieve ourselves but also benefit the world. At the industry level and even at the national level, open source is embraced. Under such a development trend, open source will integrate young programmers as one of the popular cultures. \nMy personal advice is to understand the open source as soon as possible, embrace him, and become a compound talent. The next step for programmers is to explore the open source field. \n\nAnd this article has roughly described the context of the Dapr project. Only from the project ecology of Dapr, we can see the fierce competition in the development of cloud computing. We don't know how many projects are floating and disappearing in the tide, or they never appear in our eyes after a wave of waves. I hope readers can have a deeper understanding and ideas about the software life cycle, especially open-source software.\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/blogs/React-Hooks-State-Persistence":{"title":"React Hooks State Persistence","content":"\n\n本文讲述如何分析设计 通过 [[React Hooks]] 进行 State 持久化管理\n\n  \n\n## 分析\n\n正常前端，组件为类文件，自己维持状态，不易复用。\n\n  \n\n首先把组件中的 [[UI]] 和 状态分开，用 Action 连接，如下图。\n\n![[blogs/Pasted image 20221011190847.png]]\n\nAction 是[[算子]]\n\n  \n\n### Function\n\n则可成为以下函数\n\n-   UI = `f(S)`\n\n-   状态驱动组件重新渲染 UI\n\n-   Scu =`f(Sc, ∆)`\n\n-   组件会用到的 Scu 和 更改 Sc 的 ∆ 方法决定。\n\n  \n\n#### S\n\n每一个组件有他自己的状态集 s。\n\n  \n\n##### scu\n\n即，component use ：组件用到的状态，比如计数器中的数字\n\n所有组件的使用到 scu 共同组成一个状态 Scu--渲染一个 UI。\n\n  \n\n##### sc\n\n即，收到组件影响的状态，如登录组件可能每登录一次就会增加计数器，但是对于登录组件并不会用到这个状态，虽然它会更改它。\n\n  \n\n### 入参\n\n#### ∆\n\n设计 State 框架时，让每一个组件声明 sa 状态时，提供一个更改自己的函数 ma ，在 Action 事件时调用用于更改 State，而多个 ma 的集合为 ∆。\n\n  \n\n#### S\n\nSa =`f(S, ∆)` 中的 S 作为 f 的参数传入，因为并不知道 Action 会更改哪些 State 【甚至不知道有哪些】，故把所有 State 都作为入参。\n\n  \n\n### 局部渲染\n\n更改的状态驱动 UI 渲染，如果相同可以不改变。\n\n  \n\n如上所说，UI 由于入参为 S ，会接收所有的 State，组件自己根据自己需要的 sa 变动渲染，而不是 UI 根据 S 改动分发事件。\n\n  \n\n观察 Hooks 可知，`useState()` 方法使用`[Object.is](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is#Description)` [比较算法](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is#Description) 来比较 state。\n\n而 `useEffect()`则提供选择让它 [在只有某些值改变的时候](https://zh-hans.reactjs.org/docs/hooks-reference.html#conditionally-firing-an-effect) 才执行的参数。\n\n  \n\n## 设计\n\n### 实虚部数学模型\n\n实数并不完备，引入虚部。\n\n  \n\n虚数，只需要去掉虚部就可以表示实数。\n\n  \n\nCurry Func 也如此。\n\n  \n\n以上同理：`f(S,∆)`中`f(S)` 代表实数，不完备，加入 ∆ ，可以表达所有情况。\n\n  \n\n更改的维度从一维的 线 成为了 二维的平面。\n\n  \n\n另：框架使用的`f(S, ∆)`还是一维的线，但其实是该平面 任意一条线 ，因为 f(S,∆) 已经中的 ∆ 和 S 已经经由使用者确定，即在多维度选择了一个平面降维实现在代码中了。\n\n  \n\n### Persistence \n\n需要一个地方存储数据，local，session，remote 等.\n\n  \n\n### Connector\n\n组件如何把触发的事件分发给 State 处理？需要通信。\n\n  \n\n由于 js 单线程模型，选择共享内存设计新增一个 Connector 用于通信。\n\n  \n\n组件 Component 如何通知 State 改动。共享内存，采用 Connector 中间层。\n\n  \n\n### Action by CurryFunc\n\nState 如何知道框架使用者定义的 Action 改动了哪些 State ？即不知道 ∆ 的具体值。采用 Curry Func 满足延迟求值的需求。\n\n使用 `fg(S){return f(∆)`} 代替 `f(S,∆)`\n\nState 框架使用者自己使用 `f(∆)` 注册自己的状态更改算子 ∆。\n\n  \n\nState 框架开发者使用 `fg(S)` ，只管自己传入所有的 State 即可。\n\n  \n\n由于 React Hooks 的存在，state 自带使用 `f(S,∆)` 进行更新的功能。故框架留出 useState() 接口，返回 `f(∆)` ，供使用者进行状态管理。\n\n  \n\n#### Redux\n\nRedux 也是基于此函数模型，而在 Hooks 中官方已经使用 `useReducer(reducer, initialState)` 实现了它。其中 reducer 是设定好的 `f(S，∆)` ，而它返回 state 和 dispatch，其中 state 就是 Sa 而 dispatch 就是 `f(∆)`。\n\n```js\nfunction useReducer(reducer, initialState) {\n  const [state, setState] = useState(initialState);\n  function dispatch(action) {\n    const nextState = reducer(state, action);\n    setState(nextState);\n  }\n  return [state, dispatch];\n}\n```\n\n在我们看来，它也内部实现了 Connector 的作用。\n\n  \n\n## 实现\n\n### Persistence\n\n首先是通过 Hooks 实现存储, 使用 Local Store\n\n```js\nfunction useLocalJSONStore(key, defaultValue) {\n    const [state, setState] = useState(\n      () =\u003e JSON.parse(localStorage.getItem(key)) || defaultValue\n    );\n    useEffect(() =\u003e {\n      localStorage.setItem(key, JSON.stringify(state));\n    }, [key, state]);\n    return [state, setState];\n}\n```\n\n  \n\n#### 存储位置\n\n解决了持久化存储，提供外在的状态管理支持。考虑到我们会使用 Go 来做前端：\n\n1.  使用 Hooks 加 sqlite3 库本地存储\n2.  使用 Hooks 和 [[Go]] 通信完成\n\n  \n\n### Connector\n\n为了使用 Hooks 实现全局的状态通知。\n\n首先明白 `useState()` 获取到的 `setState()` 会触发当前组件的渲染：[https://zh-hans.reactjs.org/docs/hooks-state.html](https://zh-hans.reactjs.org/docs/hooks-state.html)\n\nConnector 让使用全局状态的组件订阅来连接上全局的状态更新，将自己的 `setState()` 传入更新队列，当其中任何一个组件使用 `dispatch()` 更改状态时会触发这个命名空间下的全部状态更新，从而达到刷新所有状态组件的目的。\n\n```js\nimport { useEffect } from \"react\"\n\nconst Connector = {}\n\nconst Broadcast = (name, state) =\u003e {\n    if (!Connector[name]) return;\n    Connector[name].forEach(setter =\u003e setter(state))\n}\n\nconst Subscribe = (name, setter) =\u003e {\n    if (!Connector[name]) Connector[name] =[];\n    Connector[name].push(setter)\n}\n\nconst UnSubscribe = (name, setter) =\u003e {\n    if (!Connector[name]) return\n    const index = Connector[name].indexOf(setter)\n    if (index !== -1) Connector[name].splice(index, 1)\n}\n\nconst connect = (name,setState) =\u003e {\n    console.log('connect')\n    useEffect(() =\u003e{\n        Subscribe(name, setState)\n        console.log('subscirbe',name)\n        return () =\u003e {\n            UnSubscribe(name,setState)\n            console.log('unsubscribe',name)\n        }\n    },[])\n}\n```\n  \n\n### useStore\n\n使用者使用 `useStore()` 来获取全局状态和 `dispatch()` 函数。内部实现就是 State Hook ，并拿到 `setState()`注册到订阅列表中。\n\n```js\nimport {Broadcast,connect} from './Connector'\nimport {useState} from 'react'\n\nexport function useStore(key,value) {\n    const [state,setState] = useState(value)\n    connect(key,setState)\n\n    return [state, (key,value) =\u003e {\n        Broadcast(key,value)\n    }]\n}\n```\n\n  \n\n### 目前状况\n\n  \n\n![](https://cdn.nlark.com/yuque/0/2019/svg/176280/1574064167135-22f14865-31ba-4b6c-a144-0d1315954ec1.svg)\n\n  \n\n## 使用\n\n使用 `useStore(key, value)` 即可。\n\n```js\nimport {useStore} from './useStore'\n\nexport function Counter({key,initialCount}) {\n    // const [count, setCount] = useLocalJSONStore(keyname, initialCount);\n    const [state, dispatch] = useStore(key,initialCount)\n    return (\n      \u003c\u003e\n        Count: {state}\n        \u003cbutton onClick={() =\u003e dispatch(keyname,initialCount)}\u003eReset\u003c/button\u003e\n        \u003cbutton onClick={() =\u003e dispatch(keyname,state-1)}\u003e-\u003c/button\u003e\n        \u003cbutton onClick={() =\u003e dispatch(keyname,state+1)}\u003e+\u003c/button\u003e\n      \u003c/\u003e\n    );\n  }\n```\n\n  \n\n![](https://cdn.nlark.com/yuque/0/2019/png/176280/1574063711150-a567cd8c-2117-47ea-8446-02da34624b22.png)\n\n## 进阶\n\n-   异步状态\n-   [[装饰器]]","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/blogs/Role-of-microservice-framework":{"title":"Role of microservice framework","content":"## Role of [[microservice framework]]\n\n## HTTP Channel and [[GRPC]] Channel \n\nbefore we begin, let's explore the differences between HTTP and RPC. The reason why gRPC is discussed here is that no one uses common RPC. \n\nHTTP is a common communication method used for business coding, and its popularity is needless to say. As a Web programmer, HTTP Server programming is its core skill. RPC is also indispensable in microservices. \n\nCan programmers who are familiar with one of the encoding quickly get started with the business coding of another transmission method? \n\nAfter all, the business logic is consistent, which seems to be only different in network transmission. \n\nColleagues who have done this know that the differences in business coding are not small. Although the differences are constrained to the transport layer through the abstraction layer during design, there is no framework to block the differences in implementation. Therefore, coding students need to go deep into it and handle it by themselves. \n\nFor example, you need to learn more about envoy and proto files, how to encode requests and return values, and how to use specific [[protobuf]] to parse message packets in your business. \n\nThe differences can be shielded at the abstraction level. We still need to write detailed differences in implementation. These are the operations that some programmers can replace with frameworks. \n\n## Top programmers and beginners, beginners, and idiots \n\nthe role of the framework is to make correct coding behavior without thinking.\n\nThere are enough ecological libraries for the current language to help compile various coding types. When lacking, some ecosystems can be transplanted by referring to other languages to reduce the workload. However, not every programmer can do such behavior at any time. \n\nGoogle is a friend. Business Code they often get into trouble because of something they don't know so that no matter how their skills or intelligence are, they can't solve the problem.\n\nIn the business, some coding work will be compared to moving bricks. Programmers are described as manual work to move code from here to there. However, when someone participates in the process, the error probability will also be related to the state of a person.\n\nThrough Murphy's law, we can recognize that errors must occur in these processes. How do reduce personal decisions to ensure high quality and high output of assembly line coding manpower?\n\nIf you want to treat yourself as an idiot coder and leave the error-prone parts to tools, the framework will generate great benefits. \n\n\n\u003e Nothing is built on Stone; Everything is built on sand, but we must build sand as stone.\n\u003e                                 -Jorge Louis Borges \n\n\nthe following are some examples: \n\n- code review: \n\narchitects not only need to formulate process standards, but also need to supervise the implementation. Code review is the major part of the workload. However, there are thousands of people, and code writers have their ideas. There may even be a design-based cohesion function, which is scattered at all levels in implementation, and the review process is even more inefficient. \n\nConstraints can be carried out through the framework, which is also the wisdom of software engineering. By increasing restrictions, standards can be formulated to provide efficiency.\n\n- Best Practices: \n\nbusiness code usually uses simple addition, deletion, modification, query data, and target resources. At the same time, there are some common functional requirements, such as [[JWT]]. \n\nThe framework can shield these differences. For example, JWT only has different types of tokens carried by HTTP, and ORM shields the actual data storage software interfaces in the background for addition, deletion, query, modification, and modification. \n\nThis is another wisdom of computer science, solving problems by adding a middle layer. Framework users can switch to different implementations without thinking.\n\nIf the best practices provided by the framework cannot meet the requirements, it is time for the document to show its role. Technical personnel-oriented documentation is useful only when problems occur.\n\n## The dilemma of microservices caused by abstract hierarchy and abstract leakage \n\n\u003e Google software engineering mentions three key differences between programming and software engineering: **time**, **scope**, and **trade-offs**. \n\nHowever, the idea of the framework is beautiful enough, but the realization, in reality, is full of trade-offs and the pursuit of perfection. \n\nEven if the strange requirements of a specific time limit on the business side are excluded. The design cannot be accomplished overnight and a perfect abstract design can be completed.\n\nAbstract leakage refers to the abstraction of implementation details that should be hidden during software development, which inevitably exposes the underlying details and limitations.\n\nNot to mention that a complete system has more than one or two levels. How to make reasonable abstraction and promote it as a standard is a long-term practice and change in many microservice frameworks and coding fields. \n\nAbstraction means unification, while behind the abstraction level, it usually means the actual services with different characteristics. Do you use the union or intersection of these services for abstraction? Whether to consider extended compatibility or functionality.\n\n\u003e For more information, see another article. [Mongo Doc access design](), is practical experience. \n\n\u003e Also The API of Dapr. Many Interfaces of Golang (IO, SQL, and Net) can see abstract practical practices.\n\nFor example, designers will struggle with whether to provide a certain function to the outside, so they have done a lot of work to provide it. However, in terms of function usage, it may be a pseudo requirement or a simple shielding. However, in actual scenarios, it is necessary to have a lower layer of functions, and the abstraction level is still broken down. \n\nAt this point, everyone understands that it falls into specific scenarios and analyzes specific problems. Therefore, a microservice framework that has passed the postgraduate entrance examination for a long time must have solved many problems in the target scenario. \n\n\u003e This reminds me that programmers always pursue new technologies. New microservice frameworks usually have high expectations, hoping that they can completely solve the problems encountered in practice that the old frameworks cannot solve. Finally, expectations often fail. Why can we expect a new untested framework to meet the needs of the technical framework that has been designed and modified many times in practice in specific fields?\n\nBack to our question at the beginning, is there a framework that unifies the HTTP Channel and gRPC Channel, and only needs to write the handler's internal code without paying attention to other work?\n\n In the modern framework, Dapr did accomplish this. \n\nWhat about the abstract cost? \n\nThe field type in the [[Protobuf]] is lost, and it is considered a payload. The handler has different self-processing types, which is consistent with HTTP abstraction. \n\nIs it true that such an abstraction layer has just come up with now? If you have a deeper understanding of computer science, you will find that some past ideas shine brilliantly in new scenarios. \n\nTime is the most significant variable (for example, previous programmers needed to deduct bytes. Now, do you still need to care about insufficient memory for personal PC and cloud coding?). \n\n## Summary\nThe above describes the problems related to the microservice framework considered in the experience. Just raising questions is a hooligan. My opinions and suggestions are mentioned a lot in the article. \n\nTo make a summary, it is: \n\nproviding a fool-like automated microservice framework enables programmers to make fewer decisions and make better decisions. \n\nOnly by using the time saved to innovate business links and business models, and not being involved in non-creative work such as environment building, can workers feel the value of innovation and self-achievement.","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/blogs/Technician-and-experiencer":{"title":"Technician and experiencer","content":"## Technician and experiencer\n\nWe believe that the experience of the experience is linked to the actual things and actual behaviors, while the technology of the technician symbolizes more general knowledge. \n\nFirst of all, in practical operation, it can be seen that skilled people are inferior to experienced people in all aspects, but they usually do better than inexperienced people. \nBy many views gained from the experience of the universal Judgment (Generic), we believe that technology was born. \n\nGenerally, we think that people with technology have a deeper understanding of this kind of thing, (in short, smarter) because their behaviors will be guided, and their starting point is reason rather than inertia. We also believe that people with technology master the ability to impart theories, but experienced people cannot teach others. \n\nDo you want to judge whether you are good at a certain technology and a master in this field? \n\nwhen you think that you can do something if others ask you but you can only tell the specific situation and specific practices, you are not a wise professor. If the universal judgment you say is not considered dialectically by yourself, you are not a Master of this event. \n\nAccording to this statement, the concept of a Technologist is close to a wise man, and wisdom is also explained as knowledge about the origin and principle of things. Readers can now think about what characteristics people with high IQ have in the concept we are talking about? \n \nfrom the above ideas, it is not surprising that mathematicians call them philosophers or wise men, because this discipline is based on the basic principles (Root) rather than a series of secondary disciplines. \n\nTherefore, the more common the principle is, the more it is regarded as truth. People who think they have mastered the truth tend to be frustrated in the field of new knowledge. \n\nSince the opinions that can be collected correspond to the infinite things, in reality, I am inconvenient to think that the universal principle of a kind of things also lacks the existence of truth because there are infinite kinds. \n\nSo when experts claim to be masters in other fields, I don't fear to think that they are not very good in this field.\n\nThe above metaphysical discussion is not to explore whether the truth exists in a pessimistic way. Instead, I want to express my praise for practical operation from the perspective of reality and the method of getting the so-called twice the result with half the effort by deeply learning the principles of things. \n\nOn the other hand, as programmers, top programmers, and technicians' technologies, they can explain the measurement of the difference in value they create. \n\nIt is not difficult to become an expert in a certain field through practice but based on the viewpoint of experience summary, it will be our goal to put forward universal fragments (.e. creation, design, and the invention of new technologies). ","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/blogs/TokenBucket":{"title":"TokenBucket","content":"## TokenBucket\n\n\n## Overview\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1657442131915/dUeLDBuYM.png align=\"left\")\n\n\n\n- available per second Limit put tokens into the bucket, or, every time 1/Limit add a token to the second bucket \n- maximum storage in buckets burst tokens. If the bucket is full, the new token will be discarded. \n- when an N is consumed when the data packet of the unit arrives N tokens, and then send the packet \n- if the available token in the bucket is less than N, the packet will be cached or discarded \n\n## token bucket algorithm \n\nthe token bucket algorithm is the most commonly used algorithm in network Traffic Shaping (Traffic Shaping) and Rate Limiting (Rate Limiting). \n\nTypically, the token bucket algorithm is used to control the number of data sent to the network and allow the sending of burst data. \n\n### overview \n\nthis package is based on the Token Bucket algorithm (Token Bucket) to implement throttling, which is very easy to use. RateLimiter is often used to limit the access rate to some physical or logical resources. It supports three methods, \n\n- AllowN() If you can't get it, return it immediately.\n- WaitN() It is temporarily lined up. When the token is sufficient, it may be returned to the position because of the Cancel of Context.\n- ReserveN() Started directly, but the predecessors dug the pit and filled it. The next request will pay the price for this, and wait until the tokens will make up for the air. There is enough token in the barrel.\n\n### Working instance \nassume that one is working RateLimiter \n\n#### allow and wait \nFor a Ratelimiter that generates a token per second, every second without a token, we will add a token 1.\n\n If the Ratelimiter does not use it in 10 seconds, then tokens become 10.0. At this time, a request arrives and requests three tokens, we will serve it from the token in Ratelimiter, tokens to 7.0. After this request, another request comes and requests 10 tokens.\n\n We will from the remaining 7 token cards from RatelimiterFor this request, there are three tokens left, we will get them from the new token produced by Ratelimiter. \n\nWe already know that the Ratelimiter produces 1 new token per second, which means that the above request still requires the three commands required for above request. The card requires it to wait for 3 seconds.\n\n#### reserve\nImagine a Ratelimiter generated a token per second, and now it is not used (in the initial state). If an expensive request requires 100 token cards. If we choose to let this request wait for 100 seconds before allowing it to execute, this is obviously ridiculous. \n\nWhy do we do nothing but just wait for 100 seconds? A better approach is to allow this request to execute immediately (no different from all), and then postpone the subsequent request to the right time point. \n\nWe allow this expensive task to perform immediately and delay the subsequent request for 100 seconds. This strategy is to let the task execute and wait at the same time.\n\n#### About timetoact\nAn important conclusion: Ratelimit does not remember the last request, but the next request allows the time to execute. This can also tell us very straightforwardly that the time interval of reaching the next scheduling time point. \n\nThe Ratelimiter is also very simple: the next scheduling time has passed. The difference between this time and the current time is how long the Ratelimiter has not been used. We will translate this time into tokens.Limit == 1), and just one request per second, then tokens will not grow.\n\n#### burst\nRatelimiter has a barrel capacity that is directly discarded when the request is greater than the capacity of this barrel.\n\nhttps://github.com/golang/time/blob/master/rate/rate.go\n\n","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/chrome-%E7%9A%84-lighthouse-%E6%B5%8B%E8%AF%95":{"title":"","content":"[google 官方介绍文档](https://developer.chrome.com/docs/lighthouse/overview/)\n\n一个 [[chrome]] 内置功能，评判网页性能打开速度等工作。评判很准所以传播广泛","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/diary/2022-08-09":{"title":"","content":"观察到的好产品经理的共有特质是[[洞察(insight) ]]而在公司里有话语权的产品经理共有特质则是[[雄辩(eloquence)]]这两个特质许多时候是不兼容的，因为说的多了，看和听的时间就少了。 所以大公司产品到后来都做烂，除非顶层负责人还能保持产品初心。 来自 [@天舟](https://www.yuque.com/tianzhou-7jgjb)的分享","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/diary/2022-08-29":{"title":"2022-08-29","content":"[[未来人才的学习目标]]","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/diary/2022-09-27":{"title":"2022-09-27","content":"- [[不要片面的根据一部分想法下决定]]\n\n\n[[做总结和感想的时候需要具体的事例]]\n  \n\n[[CTO 确实同时需要扮演优秀的 engineer]]","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/diary/2022-09-28":{"title":"2022-09-28","content":"\n - #todo \n\t* \n\t* \n- #journal \n\t- 17:50 离群的特异学习会远离共识，共识通常是正确的，因此稀有的技能选择存在巨大的风险，为了降低风险，我们必须保持谦虚。\n\t- 18:10 看到一篇很好的关于选择学习什么技能的文章： https://medium.com/accelerated-intelligence/while-most-people-fight-to-learn-in-demand-skills-smart-people-are-secretly-learning-rare-skills-f9b26856c9d6 学习笔记：[[学习稀缺的技能]]","lastmodified":"2022-10-21T07:56:51.681474842Z","tags":null},"/diary/2022-09-29":{"title":"2022-09-29","content":"\n - #todo \n\t* \n\t* \n- #journal\n- 15:34 添加测试\u003cbr\u003e\u003cbr\u003e\n- 20:23 memory 真好用，可以用来吐槽，就是没有发送快捷键\n\n- 20:25 最近需要思考学点什么东西。把昨天的博客看完吧\n- 20:30 学术文章\u003cbr\u003e我领域之外的学科，其他人甚至都不知道\u003cbr\u003e许可专有数据\u003cbr\u003e与可能不会公开分享某些见解的领域内部人士建立深厚的关系\u003cbr\u003e心智模型（难以评估的抽象值）\n  这些才是应该在我们的日常学习中去学习的东西，总结的面很到位。因为稀缺性，才有放大的价格。 ^cff4f1","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/diary/2022-09-30":{"title":"2022-09-30","content":"\n - #todo \n\t* golang devcloud 使用 minio\n\t* 学习稀有技能的进一步研究，找一个目标 [[学习稀缺的技能]]\n- #journal ^e61eca","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/diary/2022-10-08":{"title":"2022-10-08","content":"\n- [[buildkit]] \n\t- [高策写过 buildkit 的使用体验](http://gaocegege.com/Blog/kubernetes/buildkit)，在他的 [[envd]] 的工作中 ^42517f\n\t- 新 [[Dockerfile]] 语法特性：buildkit 可以通过 docker buildx build 解析新的语法特性。from [[2022-10-08#^42517f]] \n\t\t- 多次 build 之间的缓存 `RUN --mount=type=cache,target=/root/.cache/pip pip install ...`\n\t\t- 多体系架构的支持 `docker buildx build --platform linux/amd64,linux/arm64 .`\n\t\t- 多行脚本 [[Dockerfile 多行脚本]] 只增加一个构建层\n- [[rocksdb]] 是单节点 KV 数据库, 设计基于 [[LSMs]] .[[rocksdb]] 是早期 [[Google]] 项目[[LevelDB]] 的一个分支。from  [[为什么我们在RocksDB上创建CockroachDB项目？]]\n- [demo 网址](https://postgres-wasm.netlify.app/) [[postgreSQL]] 跑在浏览器里，通过 [[wasm]]\n- [[novelai]] 使用 [[stable diffusion]] 生成了大量的二次元图片。repo 库： [github.com/NovelAI/stable-diffusion](https://github.com/NovelAI/stable-diffusion) 用 [[jupyter nodebook]] 写的\n- 试用一下 [[maigret]] 一个通过用户 id 收集全网账户报告的工具","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/diary/2022-10-09":{"title":"2022-10-09","content":"尝试使用软件进行思路的梳理，现在是个人时间看板的梳理。[[MnicTime]] 这个软件是可以记录所有的软件的时间的。手机 iPhone 也可以记录软件使用时间。\n不能像上面流水线记录，不然跟不上思路。总结然后记录：\n\n目的：需要找到哪些是可以优化的时间习惯。 通过目前对自我的观察：\n1. 各种群聊的聊天，然后发消息，消耗了部分时间\n2. 消磨时间的操作，这部分时间应该属于可以优化的。\n3. 最后是因为好奇心去看文章的时间\n\n三类大时间中，首先优化聊天时间，去掉大部分要进入查看的群聊即可，感觉很简单嘛。\n\n[[使用 quartz 托管 obsidian 到网络上]] \n\n[[清理 GIt 中的历史文件]]\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/diary/2022-10-10":{"title":"2022-10-10","content":"[[Pricing as Code]] 是[[Tier]]这个产品使用的理念。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/diary/2022-10-12":{"title":"2022-10-12","content":"今天研究了一下 [[etcd 的性能瓶颈]]。[[etcd]] 的 event 事件太多，影响 etcd 性能，有很多大公司有过分离存储 event 的实践。\n通过参数`--etcd-servers-overrides stringSlice`可以将event存储到单独的集群。\n\u003e Etcd is used for two distinct cluster purposes - cluster state and event processing. These have different i/o characteristics. It is important to scalability testing efforts that the iops provided by the servers to etcd be consistent and protected. These leads to two requirements:\n\u003e  - Split etcd: Two different etcd clusters for events and cluster state (note: this is currently the GKE production default as well).\n\u003e  - Dedicated separate IOPS for the etcd clusters on each control plane node. On a bare metal install this could look like a dedicated SSD. This requires a more specific configuration per provider. Config table below\n\n今天翻以前的[[语雀]]笔记，找到了之前想了解的[[开源项目]]，他们都有巨大的改变或者进步。比如 [[VictoriaMetrics]]已经成为大家都愿意无缝替代 Prometheus 的新软件了。另外是 Code Server 领域中的  sail，之前试用的时候还有 chrome 插件，用起来由于天朝网络问题并不好用。现在也进步成为新项目了。这个项目停止开发了，然而 Coder 项目从中新生，结合 Terraform 描述基础架构的能力，[Coder](https://coder.com) 不仅可以构建一致共享的开发环境，还能通过 code 声明式利用云上和自己的基础架构设施（比如在 Google Cloud 申请一个 ec2 用于开发环境的运行）。当然，基础的能力应该还是 Code Server 提供的。\nCoder 首页一个问题我很喜欢，正好问到了我的关注点：当我可以用 Terraform 来配置开发环境的时候为什么还需要 Coder？：Coder 支持开/关调度以降低成本，让您控制您的团队可以配置哪些基础设施值（例如，我只想让我的团队控制 CPU），将您的云开发环境网络连接到您的本地机器等等。\n这个笔记是 #DevelopmentEnvironment 主题下的，当时的想法其实是构建学习环境，降低新人培养成本。\n\n[[slipbox]] \n\n[[Terraform 配置本地离线源]]\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/etcd":{"title":"etcd","content":"[[分布式]][[键值数据库]]","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/etcd-%E7%9A%84%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88":{"title":"etcd 的性能瓶颈","content":"[[etcd]] 通常和 [[API Server]] 一起作为整个 [[Kubernetes 的性能瓶颈]]\n\n\n## Refer Link\n[蚂蚁集团万级规模 K8s 集群 etcd 高可用建设之路 · SOFAStack](https://www.sofastack.tech/blog/ant-groups-10000-scale-k8s-cluster-etcd-high-availability-construction-road/) 以下摘抄的经验数据\n\n\t当 K8s 集群规模扩大时，etcd 承载着 KV 数据剧增、event 消息暴增以及消息写放大的三种压力。 为了证明所言非虚，特引用部分数据为佐证：\n\t1.  etcd KV 数据量级在 100 万以上；\n\t2.  etcd event 数据量在 10 万以上；\n\t3.  etcd 读流量压力峰值在 30 万 pqm 以上，其中读 event 在 10k qpm 以上；\n\t4.  etcd 写流量压力峰值在 20 万 pqm 以上，其中写 event 在 15k qpm 以上；\n\t5.  etcd CPU 经常性飙升到 900% 以上；\n\t6.  etcd 内存 RSS 在 60 GiB 以上；\n\t7.  etcd 磁盘使用量可达 100 GiB 以上；\n\t8.  etcd 自身的 goroutine 数量 9k 以上；\n\t9.  etcd 使用的用户态线程达 1.6k 以上；\n\t10.  etcd gc 单次耗时常态下可达 15ms。\n\t\n\t使用 Go 语言实现的 etcd 管理这些数据非常吃力，无论是 CPU、内存、gc、goroutine 数量还是线程使用量，基本上都接近 go runtime 管理能力极限：经常在 CPU profile 中观测到 go runtime 和 gc 占用资源超过 50% 以上。\n\n\t蚂蚁的 K8s 集群在经历高可用项目维护之前，当集群规模突破 7 千节点规模时，曾出现如下性能瓶颈问题：\n\t\n\t1.  etcd 出现大量的读写延迟，延迟甚至可达分钟级；\n\t2.  kube-apiserver 查询 pods / nodes / configmap / crd 延时很高，导致 etcd oom；\n\t3.  etcd list-all pods 时长可达 30 分钟以上；\n\t4.  2020 年 etcd 集群曾因 list-all 压力被打垮导致的事故就达好几起；\n\t5.  控制器无法及时感知数据变化，如出现 watch 数据延迟可达 30s 以上。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/golang-timer-%E8%A7%A3%E6%9E%90":{"title":"","content":"网址： https://tonybai.com/2016/12/21/how-to-use-timer-reset-in-golang-correctly/\n\n近期[gopheracademy blog](https://blog.gopheracademy.com/)发布了一篇 《[How Do They Do It: Timers in Go](https://blog.gopheracademy.com/advent-2016/go-timers)》，通过对timer源码的分析，讲述了timer的原理，大家可以看看。\n\ngo runtime 实际上仅仅是启动了一个单独的 [[goroutine]]，运行 timerproc函数，维护了一个”[[最小堆]]”，定期wake up后，读取堆顶的timer，执行timer对应的f函数，并移除该timer element。创建一个Timer实则就是在这个最小堆中添加一个element，Stop一个timer，则是从堆中删除对应的element。","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/hugo-extended":{"title":"hugo-extended","content":"\nhugo 安装网址：[https://gohugo.io/getting-started/installing/](https://gohugo.io/getting-started/installing/)\n\nwindows 我直接使用了安装\n```bash\nscoop install hugo-extended\n```","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/hugo-obsidian":{"title":"hugo-obsidian","content":"使用 [[Golang]] install 功能安装\n\n```\n# Install and link `hugo-obsidian` locally\ngo install github.com/jackyzha0/hugo-obsidian@latest\n```","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/kube-vip-%E7%9A%84%E4%B8%93%E9%97%A8%E7%9A%84%E6%84%9F%E8%B0%A2-issue":{"title":"kube-vip 的专门的感谢 issue","content":"#开源趣闻\nhttps://github.com/kube-vip/kube-vip/issues/239\n\n# Not an issue, just a thank you #239 \n\n [Closed](https://github.com/kube-vip/kube-vip/issues/239#event-7371032974)\n\nopened this issue 4 comments \n\n## Comments\n\n[![@kylos101](https://avatars.githubusercontent.com/u/1272076?s=80\u0026u=3a1662f1bacd974dedcf0117fd84193b6d0feb37\u0026v=4)](https://github.com/kylos101)\n\n### **[kylos101](https://github.com/kylos101)** [on 11 Jul 2021](https://github.com/kube-vip/kube-vip/issues/239#issue-941363311)\n\nThis is amazing, I just got it setup, and cannot believe it was that easy to get working!\n\nI setup my Traefik ingress to use the kube-vip IP address as its external IP, on my bare-metal cluster at home.\n\nThank you, `kube-vip` team. Y'all rock!","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/kubeadm-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B":{"title":"kubeadm 初始化流程","content":"## 摘要\n\n\n## 问题、提示\n-  \n\n## 主要笔记\n- 代码位置： 在 Kubernetes 源码包里`kubernetes/cmd/kubeadm/app`，kubelet 这个 repo 只是用来做聚合 issues 用的。\n- [sourcegraph 代码位置](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kubeadm/app/cmd/init.go)\n- kubeadm init 由一系列的 workflow 脚本组成，[截取如下](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kubeadm/app/cmd/init.go?L141)：\n\t ```go\n\t// initialize the workflow runner with the list of phases\n\tinitRunner.AppendPhase(phases.NewPreflightPhase())\n\tinitRunner.AppendPhase(phases.NewCertsPhase())\n\tinitRunner.AppendPhase(phases.NewKubeConfigPhase())\n\t```\n\t- 每一个 Phase 内部都是一系列的 Golang 脚本，通过面向对象的设计，用包的调用操作函数。\n\t\t- 当然，底层的脚本调用通过[接口实现](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kubeadm/app/util/initsystem/initsystem.go?L17)的方式，兼容了不同系统的调用启动，比如 [[Unix]] 系统上有 [[systemctl]] 就用，或者用 [[openrc]]。\n\t\t- kubeadm 自己写了一个遍历添加脚本的代码用作工作流调度：[/](https://sourcegraph.com/github.com/kubernetes/kubernetes)[cmd /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd)[kubeadm /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm)[app /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm/app)[cmd /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm/app/cmd)[phases /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm/app/cmd/phases)[workflow /](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/tree/cmd/kubeadm/app/cmd/phases/workflow)[runner.go](https://sourcegraph.com/github.com/kubernetes/kubernetes/-/blob/cmd/kubeadm/app/cmd/phases/workflow/runner.go)\n- [官方文档对于实现细节的展示](https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/implementation-details/#core-design-principles)","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/maigret":{"title":"maigret","content":"https://github.com/soxoj/maigret","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/namespace":{"title":"namespace","content":"Kubernetes 中的命名空间概念，可参考 [[Linux Namespace]]","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/nginx-%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6":{"title":"","content":"```nginx\nserver {\n  listen 80;\n  # gzip config\n  gzip on;\n  gzip_min_length 1k;\n  gzip_comp_level 9;\n  gzip_types text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml;\n  gzip_vary on;\n  gzip_disable \"MSIE [1-6]\\.\";\n\n  root /usr/share/nginx/html;\n\n  server_name  {{domain}};\n\n  location / {\n    try_files $uri $uri/ /index.html;\n  }\n  location /api {\n    proxy_pass http://{{ip:port}}/;\n    proxy_set_header   X-Forwarded-Proto $scheme;\n    proxy_set_header   Host              $http_host;\n    proxy_set_header   X-Real-IP         $remote_addr;\n  }\n}\n# server {\n#   # 如果有资源，建议使用 https + http2，配合按需加载可以获得更好的体验\n#   listen 443 ssl http2 default_server;\n#   # \u0008证书的公私钥\n#   ssl_certificate /path/to/public.crt;\n#   ssl_certificate_key /path/to/private.key;\n#   location / {\n#         # 用于配合 browserHistory使用\n#         try_files $uri $uri/ /index.html;\n#   }\n#   location /api {\n#       proxy_pass https://preview.pro.ant.design;\n#       proxy_set_header   X-Forwarded-Proto $scheme;\n#       proxy_set_header   Host              $http_host;\n#       proxy_set_header   X-Real-IP         $remote_addr;\n#   }\n# }\n```","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/slipbox":{"title":"","content":"原来就是卢曼笔记啊","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/terraform-minio":{"title":"","content":"结合 devcloud 和之前的 minio 中间件思考一下问题：如何集成使用？\n\n\n首先描述 [Minio Provier](https://registry.terraform.io/providers/refaktory/minio/latest/docs)， [[MInio]] 是需要自己部署的，[[Terraform]] minio 这个 [[terraform provider|provicer]]只是提供了配置选项。不过我们使用基础设施其实也就是配置了。没有启动功能用什么启动呢？\n1. 自己本地启动\n2. 通过 terraform 的其他云 provider 在云上申请一个这样的资源，比如 [AWS Terraform Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)\n\n当然，由于 [[terraform]] 的 [[terraform provider]] 编写比较简单，其实也可以自己编辑一个以 [[docker]] 作为 [[infrastructure]] 的包含 minio 启动流程的 provider 啦。比如组合 docker provider 声明一个  minio 镜像的 [[container]] 就好啦。\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/web-%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B":{"title":"web 技术发展历程","content":"## 摘要\n\n## 问题、提示\n-  \n\n## 主要笔记\n- https://www.epicweb.dev/the-webs-next-transition ：具有详尽的关于 Web 技术发展的经验和论证优缺点。以及整个 web 历史的架构，都画了图说明。 能写出来需要优秀的经验和总结能力，这篇文章写的很好。毕竟 [[Focus and Different]]\n\t- 其中提到 [[SPA]] 的[[状态管理]]是一个大问题，按我的经验来说也是，[[Redux]] 等状态管理的框架，[[UI]] 的[[单向渲染逻辑]]，都是状态管理的大问题。当然核心问题就是[[缓存失效]]！毕竟缓[[存失效是软件中最困难的问题之一]]。\n\t- 提到了框架 [[Remix]]，找机会学学了解下 #todo  \n\t\t- https://twitter.com/shamwhoah/status/1575619809714503681 : 还能快速在 [[chrome 的 lighthouse 测试]]中获得很高分！\n\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/windows-%E5%BC%80%E5%90%AF%E8%87%AA%E5%8A%A8%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC":{"title":"windows 开启自动执行脚本","content":"## 摘要\n前者是针对某个账户的开机启动，账户名没改默认是Administrator，后者是针对所有账户的启动：\n放在指定路径即可：\n- `C:\\Users\\你的账户名\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup`\n- `C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\StartUp`","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null},"/windows-%E8%87%AA%E5%8A%A8%E4%BB%A5%E7%AE%A1%E7%90%86%E5%91%98%E6%9D%83%E9%99%90%E8%BF%90%E8%A1%8C-bat-%E8%84%9A%E6%9C%AC":{"title":"windows 自动以管理员权限运行 bat 脚本","content":"```bat\n@echo off\n\u003enul 2\u003e\u00261 \"%SYSTEMROOT%\\system32\\cacls.exe\" \"%SYSTEMROOT%\\system32\\config\\system\"\nif '%errorlevel%' NEQ '0' (\ngoto UACPrompt\n) else ( goto gotAdmin )\n:UACPrompt\necho Set UAC = CreateObject^(\"Shell.Application\"^) \u003e \"%temp%\\getadmin.vbs\"\necho UAC.ShellExecute \"%~s0\", \"\", \"\", \"runas\", 1 \u003e\u003e \"%temp%\\getadmin.vbs\"\n\"%temp%\\getadmin.vbs\"\nexit /B\n:gotAdmin\nif exist \"%temp%\\getadmin.vbs\" ( del \"%temp%\\getadmin.vbs\" )\npushd \"%CD%\"\nCD /D \"%~dp0\"\n```\n\n使用方法：最下面接写的批处理即可，以下以[[自动内外网路由 bat 脚本]]举例：\n","lastmodified":"2022-10-21T07:56:51.685474991Z","tags":null}}